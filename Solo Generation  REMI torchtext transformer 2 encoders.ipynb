{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022889aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator,ReversibleField\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import remi_utils as utils\n",
    "import twoencodertransformer as kk\n",
    "import pickle\n",
    "source_folder = \"solo_generation_dataset_augmented_presplit\"\n",
    "folder = \"dynamic_augmented_models/2enc_3rd\"\n",
    "destination_folder = folder + \"/solo_generation_weights\"\n",
    "generated_outputs = folder +  \"/generated_samples\"\n",
    "dissimilar_interpolation = folder + \"/interpolation\"\n",
    "vocab = folder + \"/vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0bfcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = random.getstate()\n",
    "# pickle.dump(state, open('./state.pkl', 'wb'))\n",
    "\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "state = pickle.load(open('./state.pkl', 'rb'))\n",
    "random.setstate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088348f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(destination_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs).mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation).mkdir(parents=True, exist_ok=True)\n",
    "Path(vocab).mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/intro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/outro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/solo\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/predict\").mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation+\"/intro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation+\"/outro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation+\"/predict\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "event2word, word2event = pickle.load(open('dictionary_augmented.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193a1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:1\" \n",
    "else:  \n",
    "    dev = \"cpu\" \n",
    "print(dev)\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc28e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields\n",
    "\n",
    "intro_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "intro_piano_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "outro_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "outro_piano_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "solo_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "solo_piano_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "fields = [('intro', intro_field), ('intro_piano', intro_piano_field), \\\n",
    "          ('outro', outro_field), ('outro_piano', outro_piano_field), \\\n",
    "          ('solo', solo_field), ('solo_piano', solo_piano_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='train_torchtext.csv', validation='val_torchtext.csv', test='test_torchtext.csv',\n",
    "                                           format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "# Iterators\n",
    "BATCH_SIZE = 8\n",
    "train_iter = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.intro),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.intro),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "test_iter = BucketIterator(test, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.intro),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "\n",
    "# Vocabulary\n",
    "\n",
    "intro_field.build_vocab(train, min_freq=1)\n",
    "intro_piano_field.build_vocab(train, min_freq=3)\n",
    "outro_field.build_vocab(train, min_freq=1)\n",
    "outro_piano_field.build_vocab(train, min_freq=3)\n",
    "solo_field.build_vocab(train, min_freq=1)\n",
    "solo_piano_field.build_vocab(train, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300385ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n",
      "353\n",
      "509\n",
      "335\n",
      "326\n",
      "279\n",
      "253\n",
      "522\n",
      "281\n",
      "444\n",
      "619\n",
      "325\n",
      "319\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "     (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "     (solo, solo_len),(solo_piano, solo_piano_len)), _ in (test_iter):\n",
    "    #print(intro.transpose(1,0).size(0))\n",
    "    print(solo.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eec1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.enabled=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2794d9da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/seq2seq_transformer/seq2seq_transformer.py\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        src2_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.src2_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src2_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = kk.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, src2, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        src2_seq_length, N = src2.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        \n",
    "        src2_positions = (\n",
    "            torch.arange(0, src2_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src2_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
    "        ).to(self.device)\n",
    "        embed_src2 = self.dropout(\n",
    "            (self.src2_word_embedding(src2) + self.src2_position_embedding(src2_positions))\n",
    "        ).to(self.device)\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
    "        ).to(self.device)\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        src2_padding_mask = self.make_src_mask(src2)\n",
    "        #print(src_padding_mask.size())\n",
    "        #print(src2_padding_mask.size())\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_src2,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            src2_key_padding_mask=src2_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e4ad3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(intro_field.vocab)\n",
    "src2_vocab_size = len(outro_field.vocab)\n",
    "trg_vocab_size = len(solo_field.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 1200\n",
    "forward_expansion = 4\n",
    "src_pad_idx = 1 #english.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    src2_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3910013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,997,275 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4) #non augmented 3e-4\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "\n",
    "def save_best_checkpoint(state, nth,filename=\"_checkpoint.pt\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "#     torch.save(state, destination_folder + str(nth)+filename)\n",
    "    torch.save(state, destination_folder + '/metrics.pt')\n",
    "\n",
    "def save_final_checkpoint(state, nth,filename=\"_checkpoint.pt\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, destination_folder + \"/\" + str(nth)+filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43049c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoi input str get int\n",
    "# intro_field.vocab.stoi\n",
    "# itos input into get token/str\n",
    "# intro_field.vocab.itos[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99045b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b08d03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #for _, (src, _,trg,_) in enumerate(iterator):\n",
    "    for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "     (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "     (solo, solo_len),(solo_piano, solo_piano_len)), _ in (iterator):\n",
    "        src, src2, trg = intro.transpose(1,0), outro.transpose(1,0), solo.transpose(1,0)\n",
    "        src, src2, trg = src.to(device), src2.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src ,src2, trg[:-1, :])\n",
    "        \n",
    "#         print(output.size())\n",
    "#         print(trg.size())\n",
    "        \n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "#         print(torch.isfinite(trg).all().cpu().item())\n",
    "#         print(torch.isfinite(output).all().cpu().item())\n",
    "#         print(torch.isfinite(loss).all().cpu().item())\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.cpu().detach().item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #for _, (src, _,trg,_) in enumerate(iterator):\n",
    "        for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "         (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "         (solo, solo_len),(solo_piano, solo_piano_len)), _ in (iterator):\n",
    "            src, src2, trg = intro.transpose(1,0), outro.transpose(1,0), solo.transpose(1,0)\n",
    "            src, src2, trg = src.to(device), src2.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, src2, trg[:-1, :]) #turn off teacher forcing\n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.cpu().detach().item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71aa22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, sentence2, intro, outro, solo, device, max_length=1200):\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    # print(tokens)\n",
    "    tokens2 = [token.lower() for token in sentence2.split(' ')]\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, intro.init_token)\n",
    "    tokens.append(intro.eos_token)\n",
    "\n",
    "    tokens2.insert(0, outro.init_token)\n",
    "    tokens2.append(outro.eos_token)\n",
    "    \n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [intro.vocab.stoi[token] for token in tokens]\n",
    "    text_to_indices2 = [outro.vocab.stoi[token] for token in tokens2]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "    sentence2_tensor = torch.LongTensor(text_to_indices2).unsqueeze(1).to(device)\n",
    "    \n",
    "    outputs = [solo.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, sentence2_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if best_guess == solo.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    # print(outputs)\n",
    "    translated_sentence = [solo.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86887ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "df_intro = pd.read_csv(source_folder + '/val_torchtext.csv')\n",
    "val_intro = df_intro['intro'].values\n",
    "val_solo = df_intro['solo'].values\n",
    "val_outro = df_intro['outro'].values\n",
    "val_data=[]\n",
    "for i in range(len(val_intro)):\n",
    "    temp_dict = {}\n",
    "    temp_dict['intro'] = val_intro[i]\n",
    "    temp_dict['solo'] = val_solo[i]\n",
    "    temp_dict['outro'] = val_outro[i]\n",
    "    val_data.append(temp_dict)\n",
    "print(len(val_intro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ead0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mode_collapse(model):\n",
    "    count = 0\n",
    "    translations = []\n",
    "    for i in range(3):\n",
    "        if len(val_intro) > 1200:\n",
    "            continue\n",
    "        intro = val_intro[i]\n",
    "        solo = val_solo[i]\n",
    "        outro = val_outro[i]\n",
    "        #print(intro)\n",
    "        list_intro = [int(x) for x in intro.split(' ')]\n",
    "        list_solo = [int(x) for x in solo.split(' ')]\n",
    "        list_outro = [int(x) for x in outro.split(' ')]\n",
    "        translated_sentence = translate_sentence(model, intro, outro, intro_field, outro_field, solo_field, device, max_length=1200)\n",
    "        \n",
    "        translated_sentence = [int(x) for x in translated_sentence if x != '<pad>' and x != '<sos>' and x != '<eos>' and x != '<unk>']\n",
    "        print(translated_sentence)\n",
    "        translations.append(translated_sentence)\n",
    "        if i > 0:\n",
    "            if translations[i-1] == translations[i]:\n",
    "                count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894016c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 1s\n",
      "\tTrain Loss: 4.291 | Train PPL:  73.070\n",
      "\t Val. Loss: 3.421 |  Val. PPL:  30.606\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch: 02 | Time: 1m 1s\n",
      "\tTrain Loss: 3.022 | Train PPL:  20.531\n",
      "\t Val. Loss: 2.845 |  Val. PPL:  17.201\n",
      "=> Saving checkpoint\n",
      "Epoch: 03 | Time: 1m 2s\n",
      "\tTrain Loss: 2.697 | Train PPL:  14.837\n",
      "\t Val. Loss: 2.637 |  Val. PPL:  13.970\n",
      "=> Saving checkpoint\n",
      "Epoch: 04 | Time: 1m 2s\n",
      "\tTrain Loss: 2.523 | Train PPL:  12.462\n",
      "\t Val. Loss: 2.480 |  Val. PPL:  11.935\n",
      "=> Saving checkpoint\n",
      "Epoch: 05 | Time: 1m 2s\n",
      "\tTrain Loss: 2.357 | Train PPL:  10.554\n",
      "\t Val. Loss: 2.340 |  Val. PPL:  10.378\n",
      "=> Saving checkpoint\n",
      "Epoch: 06 | Time: 1m 2s\n",
      "\tTrain Loss: 2.200 | Train PPL:   9.024\n",
      "\t Val. Loss: 2.156 |  Val. PPL:   8.640\n",
      "=> Saving checkpoint\n",
      "Epoch: 07 | Time: 1m 3s\n",
      "\tTrain Loss: 2.041 | Train PPL:   7.702\n",
      "\t Val. Loss: 2.058 |  Val. PPL:   7.829\n",
      "=> Saving checkpoint\n",
      "Epoch: 08 | Time: 1m 2s\n",
      "\tTrain Loss: 1.947 | Train PPL:   7.005\n",
      "\t Val. Loss: 2.030 |  Val. PPL:   7.615\n",
      "=> Saving checkpoint\n",
      "Epoch: 09 | Time: 1m 2s\n",
      "\tTrain Loss: 1.878 | Train PPL:   6.543\n",
      "\t Val. Loss: 2.010 |  Val. PPL:   7.465\n",
      "=> Saving checkpoint\n",
      "Epoch: 10 | Time: 1m 2s\n",
      "\tTrain Loss: 1.829 | Train PPL:   6.228\n",
      "\t Val. Loss: 2.006 |  Val. PPL:   7.435\n",
      "=> Saving checkpoint\n",
      "Epoch: 11 | Time: 1m 3s\n",
      "\tTrain Loss: 1.781 | Train PPL:   5.936\n",
      "\t Val. Loss: 2.024 |  Val. PPL:   7.567\n",
      "Epoch: 12 | Time: 1m 3s\n",
      "\tTrain Loss: 1.741 | Train PPL:   5.703\n",
      "\t Val. Loss: 2.030 |  Val. PPL:   7.613\n",
      "Epoch: 13 | Time: 1m 3s\n",
      "\tTrain Loss: 1.702 | Train PPL:   5.487\n",
      "\t Val. Loss: 2.040 |  Val. PPL:   7.690\n",
      "Epoch: 14 | Time: 1m 3s\n",
      "\tTrain Loss: 1.668 | Train PPL:   5.303\n",
      "\t Val. Loss: 2.069 |  Val. PPL:   7.918\n",
      "Epoch: 15 | Time: 1m 3s\n",
      "\tTrain Loss: 1.633 | Train PPL:   5.117\n",
      "\t Val. Loss: 2.077 |  Val. PPL:   7.982\n",
      "Epoch: 16 | Time: 1m 3s\n",
      "\tTrain Loss: 1.598 | Train PPL:   4.941\n",
      "\t Val. Loss: 2.084 |  Val. PPL:   8.035\n",
      "Epoch: 17 | Time: 1m 3s\n",
      "\tTrain Loss: 1.569 | Train PPL:   4.800\n",
      "\t Val. Loss: 2.115 |  Val. PPL:   8.293\n",
      "Epoch: 18 | Time: 1m 2s\n",
      "\tTrain Loss: 1.539 | Train PPL:   4.660\n",
      "\t Val. Loss: 2.134 |  Val. PPL:   8.450\n",
      "Epoch: 19 | Time: 1m 2s\n",
      "\tTrain Loss: 1.511 | Train PPL:   4.531\n",
      "\t Val. Loss: 2.166 |  Val. PPL:   8.725\n",
      "Epoch: 20 | Time: 1m 3s\n",
      "\tTrain Loss: 1.482 | Train PPL:   4.402\n",
      "\t Val. Loss: 2.197 |  Val. PPL:   8.996\n",
      "=> Saving checkpoint\n",
      "Epoch: 21 | Time: 1m 2s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.309\n",
      "\t Val. Loss: 2.233 |  Val. PPL:   9.330\n",
      "=> Saving checkpoint\n",
      "Epoch: 22 | Time: 1m 3s\n",
      "\tTrain Loss: 1.432 | Train PPL:   4.185\n",
      "\t Val. Loss: 2.258 |  Val. PPL:   9.561\n",
      "Epoch: 23 | Time: 1m 3s\n",
      "\tTrain Loss: 1.408 | Train PPL:   4.088\n",
      "\t Val. Loss: 2.305 |  Val. PPL:  10.023\n",
      "Epoch: 24 | Time: 1m 3s\n",
      "\tTrain Loss: 1.385 | Train PPL:   3.996\n",
      "\t Val. Loss: 2.319 |  Val. PPL:  10.168\n",
      "Epoch: 25 | Time: 1m 3s\n",
      "\tTrain Loss: 1.362 | Train PPL:   3.905\n",
      "\t Val. Loss: 2.361 |  Val. PPL:  10.601\n",
      "[0, 1, 2, 158, 70, 49, 48, 22, 16, 49, 48, 22, 78, 51, 48, 22, 17, 64, 48, 22, 90, 53, 48, 22, 27, 49, 48, 22, 74, 64, 48, 22, 0, 1, 54, 48, 22, 67, 32, 48, 22, 4, 54, 48, 22, 23, 64, 48, 22, 8, 51, 48, 22, 8, 51, 48, 22, 72, 32, 48, 22, 10, 64, 48, 22, 91, 32, 48, 22, 13, 32, 48, 22, 70, 32, 48, 22, 16, 32, 48, 22, 78, 32, 48, 22, 17, 54, 48, 22, 90, 32, 48, 22, 27, 54, 48, 22, 74, 64, 48, 22, 0, 1, 32, 48, 22, 67, 32, 48, 22, 4, 32, 48, 22, 23, 32, 48, 22, 8, 32, 48, 22, 72, 32, 48, 22, 10, 32, 48, 22, 91, 32, 48, 22, 13, 32, 48, 22, 70, 32, 48, 22, 16, 32, 48, 22, 78, 32, 48, 22, 78, 32, 48, 22, 17, 32, 48, 22, 90, 32, 48, 22, 17, 32, 48, 22, 90, 32, 48, 22, 27, 64, 48, 22, 74, 32, 48, 22, 74, 32, 48, 22, 0, 1, 32, 48, 22, 67, 32, 48, 22, 4, 32, 48, 22, 23, 54, 48, 22, 23, 32, 48, 22, 8, 32, 48, 22, 72, 51, 55, 22, 72, 32, 48, 22]\n",
      "[0, 1, 2, 89, 67, 51, 62, 7, 23, 54, 62, 7, 72, 51, 62, 7, 91, 51, 62, 7, 70, 51, 62, 7, 78, 51, 62, 7, 90, 51, 62, 7, 74, 54, 62, 7, 0, 67, 51, 62, 7, 23, 51, 62, 7, 72, 51, 62, 7, 91, 51, 62, 7, 70, 51, 62, 7, 78, 51, 62, 7, 90, 51, 62, 7, 74, 51, 62, 7, 0, 67, 51, 62, 7, 23, 51, 62, 7, 72, 51, 62, 7, 91, 51, 62, 7, 70, 51, 62, 7, 78, 51, 62, 7, 90, 51, 62, 7, 74, 51, 62, 7, 0, 67, 51, 62, 7, 23, 51, 62, 7, 72, 51, 62, 7, 91, 51, 62, 7, 70, 51, 62, 7, 70, 51, 62, 41, 90, 51, 62, 7, 74, 51, 62, 7, 0, 67, 51, 62, 7, 23, 51, 62, 7, 72, 51, 62, 7, 72, 51, 62, 7, 91, 51, 62, 7, 70, 51, 62, 7, 70, 51, 62, 7, 78, 51, 62, 7, 90, 51, 62, 7, 74, 51, 62, 7, 0, 67, 51, 62, 7, 67, 51, 62, 7, 23, 51, 62, 7, 72, 51, 62, 7, 91, 51, 62, 7, 70, 51, 62, 41, 78, 51, 62, 7, 90, 51, 62, 7, 90, 51, 62, 7, 74, 51, 62, 7, 74, 51, 62, 7, 0, 67, 51, 62, 7, 23, 51, 62, 7, 72, 51, 62, 7, 91, 51, 62, 7, 70, 51, 62, 7, 70, 51, 62, 7, 70, 51, 62, 7, 70, 51, 62, 41, 90, 51, 62, 7]\n",
      "[0, 1, 2, 89, 67, 51, 6, 7, 23, 51, 6, 7, 72, 51, 6, 7, 91, 51, 6, 7, 70, 51, 6, 7, 78, 51, 6, 7, 90, 51, 6, 7, 74, 51, 6, 7, 0, 67, 51, 6, 7, 23, 51, 6, 7, 72, 51, 6, 7, 91, 51, 6, 7, 70, 51, 6, 7, 78, 51, 6, 7, 90, 51, 6, 7, 90, 51, 6, 7, 74, 51, 6, 7, 0, 67, 51, 6, 7, 23, 51, 6, 7, 72, 51, 6, 7, 91, 51, 6, 7, 70, 51, 6, 7, 70, 51, 6, 7, 78, 51, 6, 7, 90, 51, 6, 7, 74, 51, 6, 7, 0, 67, 51, 6, 7, 23, 51, 6, 7, 72, 51, 6, 7, 91, 51, 6, 41, 70, 51, 6, 7, 70, 51, 6, 7, 78, 51, 6, 7, 90, 51, 6, 7, 74, 51, 6, 7, 0, 67, 51, 6, 7, 23, 51, 6, 7, 72, 51, 6, 7, 91, 51, 6, 7, 70, 51, 6, 7, 70, 51, 6, 7, 78, 51, 6, 7, 78, 51, 6, 7, 90, 51, 6, 7, 74, 51, 6, 7, 0, 67, 51, 6, 7, 23, 51, 6, 7, 23, 51, 6, 41, 72, 51, 6, 7, 72, 51, 6, 7, 91, 51, 6, 7, 70, 51, 6, 41, 78, 51, 6, 7, 90, 51, 6, 7, 74, 51, 6, 7, 0, 67, 51, 6, 7, 23, 51, 6, 7, 72, 51, 6, 7]\n",
      "Epoch: 26 | Time: 1m 3s\n",
      "\tTrain Loss: 1.335 | Train PPL:   3.802\n",
      "\t Val. Loss: 2.420 |  Val. PPL:  11.249\n",
      "Epoch: 27 | Time: 1m 2s\n",
      "\tTrain Loss: 1.318 | Train PPL:   3.737\n",
      "\t Val. Loss: 2.423 |  Val. PPL:  11.282\n",
      "Epoch: 28 | Time: 1m 3s\n",
      "\tTrain Loss: 1.294 | Train PPL:   3.648\n",
      "\t Val. Loss: 2.454 |  Val. PPL:  11.640\n",
      "Epoch: 29 | Time: 1m 2s\n",
      "\tTrain Loss: 1.275 | Train PPL:   3.580\n",
      "\t Val. Loss: 2.445 |  Val. PPL:  11.526\n",
      "Epoch: 30 | Time: 1m 3s\n",
      "\tTrain Loss: 1.255 | Train PPL:   3.507\n",
      "\t Val. Loss: 2.567 |  Val. PPL:  13.033\n",
      "Epoch: 31 | Time: 1m 2s\n",
      "\tTrain Loss: 1.237 | Train PPL:   3.444\n",
      "\t Val. Loss: 2.573 |  Val. PPL:  13.109\n",
      "Epoch: 32 | Time: 1m 3s\n",
      "\tTrain Loss: 1.212 | Train PPL:   3.362\n",
      "\t Val. Loss: 2.574 |  Val. PPL:  13.119\n",
      "Epoch: 33 | Time: 1m 3s\n",
      "\tTrain Loss: 1.196 | Train PPL:   3.305\n",
      "\t Val. Loss: 2.612 |  Val. PPL:  13.627\n",
      "Epoch: 34 | Time: 1m 3s\n",
      "\tTrain Loss: 1.178 | Train PPL:   3.248\n",
      "\t Val. Loss: 2.634 |  Val. PPL:  13.928\n",
      "Epoch: 35 | Time: 1m 3s\n",
      "\tTrain Loss: 1.158 | Train PPL:   3.183\n",
      "\t Val. Loss: 2.732 |  Val. PPL:  15.360\n",
      "Epoch: 36 | Time: 1m 2s\n",
      "\tTrain Loss: 1.143 | Train PPL:   3.135\n",
      "\t Val. Loss: 2.728 |  Val. PPL:  15.298\n",
      "Epoch: 37 | Time: 1m 3s\n",
      "\tTrain Loss: 1.122 | Train PPL:   3.070\n",
      "\t Val. Loss: 2.786 |  Val. PPL:  16.223\n",
      "Epoch: 38 | Time: 1m 3s\n",
      "\tTrain Loss: 1.105 | Train PPL:   3.018\n",
      "\t Val. Loss: 2.798 |  Val. PPL:  16.418\n",
      "Epoch: 39 | Time: 1m 3s\n",
      "\tTrain Loss: 1.089 | Train PPL:   2.973\n",
      "\t Val. Loss: 2.811 |  Val. PPL:  16.631\n",
      "Epoch: 40 | Time: 1m 3s\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.924\n",
      "\t Val. Loss: 2.855 |  Val. PPL:  17.377\n",
      "=> Saving checkpoint\n",
      "Epoch: 41 | Time: 1m 3s\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.879\n",
      "\t Val. Loss: 2.973 |  Val. PPL:  19.544\n",
      "=> Saving checkpoint\n",
      "Epoch: 42 | Time: 1m 3s\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.827\n",
      "\t Val. Loss: 2.922 |  Val. PPL:  18.587\n",
      "Epoch: 43 | Time: 1m 3s\n",
      "\tTrain Loss: 1.029 | Train PPL:   2.799\n",
      "\t Val. Loss: 2.981 |  Val. PPL:  19.706\n",
      "Epoch: 44 | Time: 1m 3s\n",
      "\tTrain Loss: 1.010 | Train PPL:   2.747\n",
      "\t Val. Loss: 3.028 |  Val. PPL:  20.650\n",
      "Epoch: 45 | Time: 1m 2s\n",
      "\tTrain Loss: 0.998 | Train PPL:   2.713\n",
      "\t Val. Loss: 3.059 |  Val. PPL:  21.300\n",
      "Epoch: 46 | Time: 1m 2s\n",
      "\tTrain Loss: 0.986 | Train PPL:   2.680\n",
      "\t Val. Loss: 3.063 |  Val. PPL:  21.399\n",
      "Epoch: 47 | Time: 1m 3s\n",
      "\tTrain Loss: 0.969 | Train PPL:   2.636\n",
      "\t Val. Loss: 3.098 |  Val. PPL:  22.160\n",
      "Epoch: 48 | Time: 1m 3s\n",
      "\tTrain Loss: 0.955 | Train PPL:   2.600\n",
      "\t Val. Loss: 3.141 |  Val. PPL:  23.136\n",
      "Epoch: 49 | Time: 1m 3s\n",
      "\tTrain Loss: 0.944 | Train PPL:   2.571\n",
      "\t Val. Loss: 3.136 |  Val. PPL:  23.020\n",
      "Epoch: 50 | Time: 1m 3s\n",
      "\tTrain Loss: 0.932 | Train PPL:   2.539\n",
      "\t Val. Loss: 3.195 |  Val. PPL:  24.419\n",
      "[0, 1, 2, 175, 1, 53, 58, 15, 67, 32, 58, 22, 4, 51, 58, 52, 72, 32, 58, 80, 74, 51, 58, 22, 0, 1, 45, 58, 22, 67, 11, 58, 22, 4, 64, 58, 52, 72, 32, 58, 66, 13, 51, 58, 52, 90, 51, 58, 22, 27, 11, 58, 52, 0, 1, 51, 58, 52, 4, 51, 58, 52, 72, 51, 58, 47, 10, 51, 58, 22, 13, 51, 58, 52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 43, 114, 1, 53, 58, 31, 4, 53, 58, 31, 8, 53, 58, 31, 10, 53, 58, 34, 13, 53, 58, 34, 17, 53, 58, 34, 0, 1, 54, 58, 42, 10, 54, 58, 34, 16, 32, 58, 31, 17, 35, 58, 34, 0, 1, 51, 58, 28, 10, 54, 58, 41, 13, 54, 58, 41, 17, 35, 58, 34, 0, 1, 35, 58, 135, 0, 1, 35, 58, 31, 4, 54, 58, 37, 17, 51, 58, 34, 0, 1, 51, 58, 108, 0, 1, 53, 58, 108, 8, 35, 58, 31, 10, 51, 58, 31, 13, 54, 61, 34, 17, 51, 58, 34, 0, 1, 51, 58, 19]\n",
      "[0, 1, 2, 175, 1, 53, 9, 15, 67, 32, 9, 22, 4, 51, 9, 52, 72, 32, 9, 80, 74, 51, 9, 22, 0, 1, 45, 9, 22, 67, 11, 9, 22, 4, 64, 9, 52, 72, 32, 9, 80, 74, 32, 9, 22, 0, 1, 51, 9, 22, 67, 32, 9, 22, 4, 32, 9, 52, 72, 51, 9, 66, 27, 11, 40, 52, 0, 67, 32, 40, 22, 4, 54, 40, 112]\n",
      "Epoch: 51 | Time: 1m 3s\n",
      "\tTrain Loss: 0.921 | Train PPL:   2.511\n",
      "\t Val. Loss: 3.271 |  Val. PPL:  26.325\n",
      "Epoch: 52 | Time: 1m 3s\n",
      "\tTrain Loss: 0.905 | Train PPL:   2.472\n",
      "\t Val. Loss: 3.290 |  Val. PPL:  26.841\n",
      "Epoch: 53 | Time: 1m 3s\n",
      "\tTrain Loss: 0.900 | Train PPL:   2.459\n",
      "\t Val. Loss: 3.308 |  Val. PPL:  27.337\n",
      "Epoch: 54 | Time: 1m 3s\n",
      "\tTrain Loss: 0.887 | Train PPL:   2.427\n",
      "\t Val. Loss: 3.325 |  Val. PPL:  27.793\n",
      "Epoch: 55 | Time: 1m 3s\n",
      "\tTrain Loss: 0.876 | Train PPL:   2.402\n",
      "\t Val. Loss: 3.359 |  Val. PPL:  28.768\n",
      "Epoch: 56 | Time: 1m 2s\n",
      "\tTrain Loss: 0.865 | Train PPL:   2.376\n",
      "\t Val. Loss: 3.432 |  Val. PPL:  30.948\n",
      "Epoch: 57 | Time: 1m 2s\n",
      "\tTrain Loss: 0.854 | Train PPL:   2.349\n",
      "\t Val. Loss: 3.428 |  Val. PPL:  30.818\n",
      "Epoch: 58 | Time: 1m 3s\n",
      "\tTrain Loss: 0.846 | Train PPL:   2.331\n",
      "\t Val. Loss: 3.475 |  Val. PPL:  32.293\n",
      "Epoch: 59 | Time: 1m 3s\n",
      "\tTrain Loss: 0.834 | Train PPL:   2.302\n",
      "\t Val. Loss: 3.559 |  Val. PPL:  35.114\n",
      "Epoch: 60 | Time: 1m 3s\n",
      "\tTrain Loss: 0.823 | Train PPL:   2.277\n",
      "\t Val. Loss: 3.532 |  Val. PPL:  34.190\n",
      "=> Saving checkpoint\n",
      "Epoch: 61 | Time: 1m 2s\n",
      "\tTrain Loss: 0.818 | Train PPL:   2.266\n",
      "\t Val. Loss: 3.613 |  Val. PPL:  37.074\n",
      "=> Saving checkpoint\n",
      "Epoch: 62 | Time: 1m 3s\n",
      "\tTrain Loss: 0.807 | Train PPL:   2.240\n",
      "\t Val. Loss: 3.612 |  Val. PPL:  37.030\n",
      "Epoch: 63 | Time: 1m 3s\n",
      "\tTrain Loss: 0.800 | Train PPL:   2.225\n",
      "\t Val. Loss: 3.620 |  Val. PPL:  37.345\n",
      "Epoch: 64 | Time: 1m 2s\n",
      "\tTrain Loss: 0.792 | Train PPL:   2.209\n",
      "\t Val. Loss: 3.652 |  Val. PPL:  38.555\n",
      "Epoch: 65 | Time: 1m 2s\n",
      "\tTrain Loss: 0.783 | Train PPL:   2.188\n",
      "\t Val. Loss: 3.644 |  Val. PPL:  38.226\n",
      "Epoch: 66 | Time: 1m 3s\n",
      "\tTrain Loss: 0.773 | Train PPL:   2.167\n",
      "\t Val. Loss: 3.722 |  Val. PPL:  41.329\n",
      "Epoch: 67 | Time: 1m 2s\n",
      "\tTrain Loss: 0.766 | Train PPL:   2.151\n",
      "\t Val. Loss: 3.703 |  Val. PPL:  40.560\n",
      "Epoch: 68 | Time: 1m 3s\n",
      "\tTrain Loss: 0.758 | Train PPL:   2.134\n",
      "\t Val. Loss: 3.736 |  Val. PPL:  41.928\n",
      "Epoch: 69 | Time: 1m 2s\n",
      "\tTrain Loss: 0.748 | Train PPL:   2.113\n",
      "\t Val. Loss: 3.719 |  Val. PPL:  41.240\n",
      "Epoch: 70 | Time: 1m 3s\n",
      "\tTrain Loss: 0.739 | Train PPL:   2.095\n",
      "\t Val. Loss: 3.778 |  Val. PPL:  43.744\n",
      "Epoch: 71 | Time: 1m 3s\n",
      "\tTrain Loss: 0.735 | Train PPL:   2.086\n",
      "\t Val. Loss: 3.830 |  Val. PPL:  46.067\n",
      "Epoch: 72 | Time: 1m 3s\n",
      "\tTrain Loss: 0.726 | Train PPL:   2.066\n",
      "\t Val. Loss: 3.866 |  Val. PPL:  47.739\n",
      "Epoch: 73 | Time: 1m 3s\n",
      "\tTrain Loss: 0.722 | Train PPL:   2.058\n",
      "\t Val. Loss: 3.767 |  Val. PPL:  43.232\n",
      "Epoch: 74 | Time: 1m 3s\n",
      "\tTrain Loss: 0.712 | Train PPL:   2.039\n",
      "\t Val. Loss: 3.837 |  Val. PPL:  46.407\n",
      "Epoch: 75 | Time: 1m 3s\n",
      "\tTrain Loss: 0.705 | Train PPL:   2.024\n",
      "\t Val. Loss: 3.868 |  Val. PPL:  47.829\n",
      "[0, 1, 2, 175, 10, 25, 106, 15, 91, 11, 106, 22, 13, 5, 106, 15, 70, 5, 106, 15, 16, 25, 106, 19, 0, 1, 32, 62, 31, 4, 11, 75, 31, 8, 32, 46, 31, 72, 64, 106, 22, 10, 32, 76, 22, 10, 64, 75, 52, 10, 11, 76, 15, 91, 32, 76, 15, 13, 24, 107, 15, 70, 11, 106, 15, 16, 11, 106, 7, 0, 1, 11, 46, 31, 4, 64, 46, 31, 8, 32, 75, 31, 10, 64, 104, 66, 10, 5, 62, 15, 91, 38, 76, 15, 13, 11, 76, 15, 70, 24, 76, 15, 16, 5, 76, 15, 0, 1, 38, 62, 52, 4, 64, 75, 31, 8, 32, 46, 15, 72, 64, 46, 22, 10, 51, 46, 22, 10, 64, 46, 15, 10, 64, 46, 19, 91, 51, 46, 15, 13, 51, 46, 15, 70, 64, 62, 15, 16, 54, 46, 7]\n",
      "[0, 1, 43, 114, 13, 38, 84, 28, 13, 38, 144, 28, 27, 38, 84, 28, 27, 38, 144, 28, 0, 8, 38, 61, 87, 8, 38, 106, 87, 0, 1, 11, 84, 28, 1, 11, 106, 28, 10, 38, 76, 28, 10, 38, 106, 28, 17, 11, 61, 85, 17, 11, 107, 85, 0, 8, 38, 76, 34, 8, 38, 106, 34, 13, 49, 61, 87, 13, 49, 107, 87, 0, 8, 38, 61, 28, 8, 38, 107, 28, 16, 49, 61, 31, 16, 49, 107, 31, 17, 64, 61, 31, 17, 64, 107, 31, 27, 51, 61, 31, 27, 51, 107, 31, 0, 1, 32, 61, 85, 1, 32, 107, 85]\n",
      "[0, 1, 2, 162, 1, 64, 9, 15, 67, 35, 9, 31, 67, 51, 9, 7, 23, 64, 9, 31, 72, 51, 9, 15, 91, 35, 26, 36, 91, 35, 26, 36, 0, 67, 54, 26, 52, 67, 54, 26, 52, 8, 35, 9, 15, 72, 64, 9, 15, 91, 35, 9, 22, 91, 54, 20, 22, 13, 35, 9, 47, 13, 64, 20, 47, 90, 51, 40, 15, 90, 51, 20, 22, 27, 51, 40, 15, 74, 54, 20, 22, 0, 1, 54, 40, 15, 67, 54, 20, 7, 23, 51, 20, 15, 8, 51, 20, 15, 72, 64, 20, 47, 13, 51, 20, 15, 70, 51, 20, 22, 16, 54, 20, 7, 78, 54, 20, 7, 90, 64, 20, 15, 27, 51, 20, 7, 74, 54, 20, 15, 0, 1, 51, 20, 15, 67, 54, 20, 41, 23, 54, 20, 15, 8, 51, 20, 15, 72, 51, 20, 7, 91, 64, 20, 7, 91, 54, 20, 7, 70, 51, 20, 15, 16, 54, 20, 22, 78, 54, 20, 31, 78, 51, 20, 15, 17, 51, 20, 31, 90, 51, 20, 31, 74, 51, 20, 15, 74, 54, 20, 15, 0, 67, 54, 20, 22, 67, 54, 20, 22, 4, 51, 20, 22, 23, 54, 20, 22, 8, 54, 20, 22, 72, 54, 20, 22, 10, 54, 20, 22, 91, 54, 20, 22, 91, 54, 20, 22, 70, 54, 20, 22, 16, 54, 20, 22, 78, 54, 20, 22, 78, 54, 20, 22, 17, 54, 20, 22, 90, 54, 20, 22, 17, 51, 20, 22, 90, 51, 20, 22, 90, 51, 20, 47]\n",
      "Epoch: 76 | Time: 1m 3s\n",
      "\tTrain Loss: 0.701 | Train PPL:   2.016\n",
      "\t Val. Loss: 3.919 |  Val. PPL:  50.329\n",
      "Epoch: 77 | Time: 1m 3s\n",
      "\tTrain Loss: 0.696 | Train PPL:   2.006\n",
      "\t Val. Loss: 3.961 |  Val. PPL:  52.532\n",
      "Epoch: 78 | Time: 1m 3s\n",
      "\tTrain Loss: 0.689 | Train PPL:   1.992\n",
      "\t Val. Loss: 3.976 |  Val. PPL:  53.288\n",
      "Epoch: 79 | Time: 1m 3s\n",
      "\tTrain Loss: 0.680 | Train PPL:   1.974\n",
      "\t Val. Loss: 3.989 |  Val. PPL:  53.983\n",
      "Epoch: 80 | Time: 1m 3s\n",
      "\tTrain Loss: 0.677 | Train PPL:   1.968\n",
      "\t Val. Loss: 3.989 |  Val. PPL:  53.982\n",
      "=> Saving checkpoint\n",
      "Epoch: 81 | Time: 1m 2s\n",
      "\tTrain Loss: 0.673 | Train PPL:   1.960\n",
      "\t Val. Loss: 4.071 |  Val. PPL:  58.612\n",
      "=> Saving checkpoint\n",
      "Epoch: 82 | Time: 1m 3s\n",
      "\tTrain Loss: 0.668 | Train PPL:   1.950\n",
      "\t Val. Loss: 4.020 |  Val. PPL:  55.701\n",
      "Epoch: 83 | Time: 1m 3s\n",
      "\tTrain Loss: 0.659 | Train PPL:   1.933\n",
      "\t Val. Loss: 4.066 |  Val. PPL:  58.301\n",
      "Epoch: 84 | Time: 1m 3s\n",
      "\tTrain Loss: 0.654 | Train PPL:   1.923\n",
      "\t Val. Loss: 4.099 |  Val. PPL:  60.287\n",
      "Epoch: 85 | Time: 1m 3s\n",
      "\tTrain Loss: 0.648 | Train PPL:   1.912\n",
      "\t Val. Loss: 4.143 |  Val. PPL:  62.987\n",
      "Epoch: 86 | Time: 1m 2s\n",
      "\tTrain Loss: 0.646 | Train PPL:   1.908\n",
      "\t Val. Loss: 4.121 |  Val. PPL:  61.633\n",
      "Epoch: 87 | Time: 1m 2s\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 4.135 |  Val. PPL:  62.509\n",
      "Epoch: 88 | Time: 1m 3s\n",
      "\tTrain Loss: 0.637 | Train PPL:   1.891\n",
      "\t Val. Loss: 4.182 |  Val. PPL:  65.508\n",
      "Epoch: 89 | Time: 1m 3s\n",
      "\tTrain Loss: 0.630 | Train PPL:   1.878\n",
      "\t Val. Loss: 4.211 |  Val. PPL:  67.426\n",
      "Epoch: 90 | Time: 1m 3s\n",
      "\tTrain Loss: 0.627 | Train PPL:   1.872\n",
      "\t Val. Loss: 4.176 |  Val. PPL:  65.092\n",
      "Epoch: 91 | Time: 1m 3s\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.873\n",
      "\t Val. Loss: 4.161 |  Val. PPL:  64.162\n",
      "Epoch: 92 | Time: 1m 3s\n",
      "\tTrain Loss: 0.618 | Train PPL:   1.855\n",
      "\t Val. Loss: 4.260 |  Val. PPL:  70.804\n",
      "Epoch: 93 | Time: 1m 3s\n",
      "\tTrain Loss: 0.614 | Train PPL:   1.848\n",
      "\t Val. Loss: 4.193 |  Val. PPL:  66.227\n",
      "Epoch: 94 | Time: 1m 3s\n",
      "\tTrain Loss: 0.609 | Train PPL:   1.839\n",
      "\t Val. Loss: 4.278 |  Val. PPL:  72.121\n",
      "Epoch: 95 | Time: 1m 3s\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.833\n",
      "\t Val. Loss: 4.325 |  Val. PPL:  75.537\n",
      "Epoch: 96 | Time: 1m 3s\n",
      "\tTrain Loss: 0.601 | Train PPL:   1.824\n",
      "\t Val. Loss: 4.355 |  Val. PPL:  77.884\n",
      "Epoch: 97 | Time: 1m 3s\n",
      "\tTrain Loss: 0.592 | Train PPL:   1.807\n",
      "\t Val. Loss: 4.324 |  Val. PPL:  75.493\n",
      "Epoch: 98 | Time: 1m 3s\n",
      "\tTrain Loss: 0.592 | Train PPL:   1.808\n",
      "\t Val. Loss: 4.334 |  Val. PPL:  76.230\n",
      "Epoch: 99 | Time: 1m 3s\n",
      "\tTrain Loss: 0.589 | Train PPL:   1.803\n",
      "\t Val. Loss: 4.317 |  Val. PPL:  74.930\n",
      "Epoch: 100 | Time: 1m 3s\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.794\n",
      "\t Val. Loss: 4.397 |  Val. PPL:  81.229\n",
      "=> Saving checkpoint\n",
      "[0, 1, 2, 154, 8, 2, 154, 13, 2, 154, 78, 53, 62, 15, 17, 5, 76, 47, 0, 23, 51, 62, 15, 8, 25, 61, 85, 78, 35, 46, 22, 78, 64, 46, 22, 17, 53, 46, 22, 17, 64, 46, 22, 90, 64, 46, 15, 90, 11, 62, 7, 27, 51, 46, 22, 27, 25, 62, 22, 74, 38, 46, 22, 74, 25, 62, 15, 0, 1, 11, 46, 15, 1, 24, 62, 7, 4, 32, 46, 31, 4, 25, 62, 52, 78, 32, 62, 15, 17, 38, 76, 41, 0, 1, 51, 62, 22, 1, 51, 62, 22, 67, 79, 62, 15, 4, 5, 62, 22, 8, 32, 62, 15, 72, 25, 62, 31, 10, 24, 46, 22, 91, 38, 104, 15, 13, 32, 104, 15, 70, 11, 104, 7, 78, 32, 46, 7, 17, 64, 104, 41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 43, 117, 4, 123, 73, 34, 4, 123, 97, 34, 10, 45, 73, 31, 10, 45, 97, 31, 13, 45, 62, 31, 13, 45, 76, 31, 16, 63, 73, 31, 16, 63, 97, 31, 17, 71, 76, 31, 17, 71, 97, 31, 27, 49, 76, 34, 27, 49, 120, 34, 0, 4, 45, 73, 34, 4, 45, 97, 34, 10, 53, 73, 31, 10, 53, 97, 31, 13, 63, 62, 31, 13, 63, 76, 31, 16, 71, 73, 31, 16, 71, 97, 31, 17, 45, 76, 31, 17, 45, 120, 31, 27, 45, 73, 34, 27, 45, 97, 34, 0, 4, 63, 73, 34, 4, 63, 97, 34, 10, 45, 73, 31, 10, 45, 97, 31, 13, 63, 62, 31, 13, 63, 76, 31, 16, 63, 73, 31, 16, 63, 97, 31, 17, 71, 73, 31, 17, 71, 97, 31, 27, 53, 76, 34, 27, 53, 120, 34, 0, 4, 71, 73, 34, 4, 71, 97, 34, 10, 45, 73, 31, 10, 45, 97, 31, 13, 63, 62, 31, 13, 63, 76, 31, 16, 123, 73, 31, 16, 123, 97, 31, 17, 63, 20, 31, 17, 63, 120, 31, 90, 49, 73, 34, 90, 49, 97, 34]\n",
      "[0, 1, 2, 89, 67, 51, 21, 66, 91, 53, 9, 15, 13, 51, 73, 22, 70, 51, 21, 66, 74, 51, 73, 22, 0, 1, 53, 9, 22, 67, 64, 9, 59, 74, 35, 9, 15, 0, 1, 54, 73, 15, 67, 35, 9, 22, 4, 53, 21, 77, 74, 53, 9, 15, 0, 1, 63, 9, 15, 67, 14, 9, 103, 70, 54, 9, 137, 74, 35, 21, 15, 0, 1, 11, 20, 68, 74, 45, 9, 22, 0, 67, 32, 21, 15, 23, 54, 9, 7, 72, 32, 21, 47, 74, 35, 9, 15, 0, 1, 54, 21, 7, 8, 54, 20, 15, 72, 53, 21, 7, 91, 35, 21, 34, 74, 32, 21, 15, 0, 1, 64, 9, 34, 23, 54, 9, 41, 8, 54, 9, 15, 72, 54, 9, 68]\n",
      "Epoch: 101 | Time: 1m 3s\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.787\n",
      "\t Val. Loss: 4.366 |  Val. PPL:  78.724\n",
      "=> Saving checkpoint\n",
      "Epoch: 102 | Time: 1m 3s\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.778\n",
      "\t Val. Loss: 4.374 |  Val. PPL:  79.382\n",
      "Epoch: 103 | Time: 1m 3s\n",
      "\tTrain Loss: 0.575 | Train PPL:   1.778\n",
      "\t Val. Loss: 4.507 |  Val. PPL:  90.620\n",
      "Epoch: 104 | Time: 1m 3s\n",
      "\tTrain Loss: 0.571 | Train PPL:   1.770\n",
      "\t Val. Loss: 4.366 |  Val. PPL:  78.748\n",
      "Epoch: 105 | Time: 1m 3s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 4.448 |  Val. PPL:  85.493\n",
      "Epoch: 106 | Time: 1m 3s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 4.412 |  Val. PPL:  82.456\n",
      "Epoch: 107 | Time: 1m 3s\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 4.463 |  Val. PPL:  86.723\n",
      "Epoch: 108 | Time: 1m 3s\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.741\n",
      "\t Val. Loss: 4.513 |  Val. PPL:  91.239\n",
      "Epoch: 109 | Time: 1m 3s\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 4.433 |  Val. PPL:  84.148\n",
      "Epoch: 110 | Time: 1m 4s\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 4.558 |  Val. PPL:  95.404\n",
      "Epoch: 111 | Time: 1m 3s\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 4.581 |  Val. PPL:  97.576\n",
      "Epoch: 112 | Time: 1m 3s\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.722\n",
      "\t Val. Loss: 4.556 |  Val. PPL:  95.237\n",
      "Epoch: 113 | Time: 1m 3s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 4.550 |  Val. PPL:  94.651\n",
      "Epoch: 114 | Time: 1m 3s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.715\n",
      "\t Val. Loss: 4.542 |  Val. PPL:  93.868\n",
      "Epoch: 115 | Time: 1m 3s\n",
      "\tTrain Loss: 0.534 | Train PPL:   1.706\n",
      "\t Val. Loss: 4.557 |  Val. PPL:  95.335\n",
      "Epoch: 116 | Time: 1m 3s\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.700\n",
      "\t Val. Loss: 4.598 |  Val. PPL:  99.276\n",
      "Epoch: 117 | Time: 1m 3s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 4.593 |  Val. PPL:  98.790\n",
      "Epoch: 118 | Time: 1m 3s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.691\n",
      "\t Val. Loss: 4.631 |  Val. PPL: 102.590\n",
      "Epoch: 119 | Time: 1m 3s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 4.630 |  Val. PPL: 102.472\n",
      "Epoch: 120 | Time: 1m 3s\n",
      "\tTrain Loss: 0.521 | Train PPL:   1.684\n",
      "\t Val. Loss: 4.677 |  Val. PPL: 107.496\n",
      "=> Saving checkpoint\n",
      "Epoch: 121 | Time: 1m 3s\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.676\n",
      "\t Val. Loss: 4.683 |  Val. PPL: 108.046\n",
      "=> Saving checkpoint\n",
      "Epoch: 122 | Time: 1m 3s\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.670\n",
      "\t Val. Loss: 4.674 |  Val. PPL: 107.140\n",
      "Epoch: 123 | Time: 1m 2s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.668\n",
      "\t Val. Loss: 4.758 |  Val. PPL: 116.540\n",
      "Epoch: 124 | Time: 1m 3s\n",
      "\tTrain Loss: 0.510 | Train PPL:   1.666\n",
      "\t Val. Loss: 4.735 |  Val. PPL: 113.911\n",
      "Epoch: 125 | Time: 1m 3s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.661\n",
      "\t Val. Loss: 4.734 |  Val. PPL: 113.767\n",
      "[0, 1, 2, 134, 23, 53, 86, 41, 91, 53, 98, 52, 78, 53, 98, 19, 74, 53, 98, 68, 0, 91, 53, 96, 108, 0, 91, 53, 86, 34, 78, 53, 96, 52, 74, 53, 96, 85, 0, 91, 53, 57, 87, 74, 53, 96, 77, 0, 91, 53, 98, 42, 74, 53, 96, 15, 0, 67, 53, 96, 7]\n",
      "[0, 1, 43, 139, 78, 35, 62, 22, 17, 51, 58, 22, 90, 49, 62, 22, 27, 32, 61, 15, 74, 51, 73, 15, 0, 67, 53, 73, 31, 23, 51, 73, 125, 0, 67, 54, 61, 15, 23, 51, 73, 15, 8, 51, 73, 22, 72, 51, 73, 22, 10, 54, 73, 15, 91, 51, 73, 15, 70, 51, 73, 15, 16, 53, 73, 52, 17, 53, 73, 15, 90, 51, 73, 22, 74, 54, 73, 28, 0, 23, 35, 58, 15, 72, 49, 73, 15, 10, 54, 73, 52, 78, 54, 73, 15, 17, 35, 73, 15, 90, 54, 73, 15, 74, 54, 73, 7, 0, 1, 35, 73, 15, 67, 51, 73, 15, 23, 51, 73, 15, 8, 51, 73, 15, 8, 35, 73, 31, 72, 51, 73, 15, 91, 53, 73, 15, 13, 51, 73, 22, 70, 51, 73, 22, 78, 54, 73, 15, 17, 51, 73, 15, 90, 51, 73, 47, 0, 1, 53, 73, 7, 90, 51, 76, 15, 27, 51, 20, 52, 23, 51, 76, 15, 8, 35, 20, 52, 91, 54, 20, 52, 78, 54, 76, 31, 17, 51, 76, 15, 90, 54, 20, 15, 74, 35, 20, 15, 74, 51, 20, 15, 0, 1, 51, 20, 15, 67, 51, 20, 15, 23, 54, 20, 15, 8, 51, 76, 22, 8, 51, 76, 15, 72, 35, 20, 22, 13, 51, 76, 22, 70, 54, 20, 15, 90, 51, 20, 22, 27, 51, 20, 15]\n",
      "[0, 1, 2, 162, 1, 49, 6, 80, 16, 53, 57, 15, 78, 45, 12, 15, 17, 54, 21, 7, 27, 49, 12, 31, 0, 1, 38, 96, 66, 10, 53, 21, 15, 91, 63, 12, 15, 13, 53, 57, 34, 90, 63, 12, 15, 27, 45, 21, 15, 74, 54, 21, 22, 0, 1, 32, 96, 7, 4, 49, 12, 36, 74, 45, 82, 22, 74, 5, 96, 22, 74, 51, 12, 22, 74, 53, 12, 22, 74, 51, 57, 22, 0, 1, 35, 6, 22, 1, 64, 50, 31, 4, 64, 6, 31, 8, 54, 12, 31, 72, 53, 12, 102]\n",
      "Epoch: 126 | Time: 1m 3s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.656\n",
      "\t Val. Loss: 4.711 |  Val. PPL: 111.127\n",
      "Epoch: 127 | Time: 1m 3s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.656\n",
      "\t Val. Loss: 4.751 |  Val. PPL: 115.753\n",
      "Epoch: 128 | Time: 1m 2s\n",
      "\tTrain Loss: 0.496 | Train PPL:   1.642\n",
      "\t Val. Loss: 4.769 |  Val. PPL: 117.842\n",
      "Epoch: 129 | Time: 1m 3s\n",
      "\tTrain Loss: 0.495 | Train PPL:   1.641\n",
      "\t Val. Loss: 4.725 |  Val. PPL: 112.742\n",
      "Epoch: 130 | Time: 1m 3s\n",
      "\tTrain Loss: 0.495 | Train PPL:   1.640\n",
      "\t Val. Loss: 4.814 |  Val. PPL: 123.201\n",
      "Epoch: 131 | Time: 1m 3s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
      "\t Val. Loss: 4.889 |  Val. PPL: 132.780\n",
      "Epoch: 132 | Time: 1m 3s\n",
      "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
      "\t Val. Loss: 4.779 |  Val. PPL: 118.934\n",
      "Epoch: 133 | Time: 1m 3s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.622\n",
      "\t Val. Loss: 4.863 |  Val. PPL: 129.468\n",
      "Epoch: 134 | Time: 1m 3s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.620\n",
      "\t Val. Loss: 4.876 |  Val. PPL: 131.116\n",
      "Epoch: 135 | Time: 1m 3s\n",
      "\tTrain Loss: 0.484 | Train PPL:   1.622\n",
      "\t Val. Loss: 4.832 |  Val. PPL: 125.424\n",
      "Epoch: 136 | Time: 1m 3s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 4.874 |  Val. PPL: 130.859\n",
      "Epoch: 137 | Time: 1m 2s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 4.829 |  Val. PPL: 125.121\n",
      "Epoch: 138 | Time: 1m 3s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 4.867 |  Val. PPL: 129.891\n",
      "Epoch: 139 | Time: 1m 3s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 4.872 |  Val. PPL: 130.584\n",
      "Epoch: 140 | Time: 1m 3s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.601\n",
      "\t Val. Loss: 4.905 |  Val. PPL: 134.934\n",
      "=> Saving checkpoint\n",
      "Epoch: 141 | Time: 1m 3s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 4.924 |  Val. PPL: 137.503\n",
      "=> Saving checkpoint\n",
      "Epoch: 142 | Time: 1m 3s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 4.923 |  Val. PPL: 137.398\n",
      "Epoch: 143 | Time: 1m 3s\n",
      "\tTrain Loss: 0.466 | Train PPL:   1.593\n",
      "\t Val. Loss: 4.895 |  Val. PPL: 133.561\n",
      "Epoch: 144 | Time: 1m 3s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 4.947 |  Val. PPL: 140.770\n",
      "Epoch: 145 | Time: 1m 3s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.582\n",
      "\t Val. Loss: 5.020 |  Val. PPL: 151.479\n",
      "Epoch: 146 | Time: 1m 3s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 4.975 |  Val. PPL: 144.798\n",
      "Epoch: 147 | Time: 1m 3s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 4.977 |  Val. PPL: 144.974\n",
      "Epoch: 148 | Time: 1m 3s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 4.972 |  Val. PPL: 144.386\n",
      "Epoch: 149 | Time: 1m 3s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 4.978 |  Val. PPL: 145.218\n",
      "Epoch: 150 | Time: 1m 3s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 4.982 |  Val. PPL: 145.767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 95, 4, 11, 48, 31, 8, 11, 58, 31, 10, 38, 57, 31, 13, 38, 58, 31, 16, 5, 48, 34, 27, 25, 46, 52, 0, 1, 54, 48, 7, 4, 38, 100, 80, 16, 24, 128, 37, 0, 4, 38, 100, 34, 10, 32, 48, 34, 16, 32, 75, 36, 0, 4, 5, 46, 85, 16, 32, 75, 34, 27, 32, 46, 34, 0, 67, 11, 48, 83]\n",
      "[0, 1, 43, 176, 70, 51, 107, 102, 78, 35, 62, 15, 78, 51, 58, 15, 78, 35, 61, 15, 90, 53, 62, 15, 90, 51, 73, 15, 90, 35, 61, 15, 74, 49, 62, 22, 74, 49, 58, 22, 74, 35, 73, 15, 0, 67, 53, 62, 15, 67, 53, 58, 22, 67, 53, 61, 15, 23, 53, 58, 66, 23, 53, 61, 66, 23, 35, 76, 66, 72, 51, 84, 52, 70, 51, 97, 108, 78, 123, 62, 15, 78, 53, 58, 15, 78, 35, 73, 15, 90, 53, 62, 15, 90, 35, 58, 15, 90, 54, 73, 22, 74, 53, 26, 22, 74, 53, 62, 15, 74, 53, 73, 15, 0, 67, 49, 62, 22, 67, 35, 58, 22, 67, 35, 73, 15, 23, 54, 9, 34, 23, 54, 73, 34, 23, 35, 61, 34, 70, 51, 97, 108, 78, 51, 58, 15, 78, 51, 73, 15, 90, 54, 58, 22, 90, 54, 73, 22, 74, 63, 62, 22, 74, 49, 58, 15, 0, 67, 71, 58, 22, 67, 53, 73, 15, 23, 35, 58, 34, 23, 54, 73, 34, 23, 54, 61, 34, 91, 51, 58, 22, 70, 51, 73, 15, 70, 51, 97, 108, 78, 53, 46, 15, 78, 54, 73, 15, 90, 53, 26, 15, 90, 54, 73, 15, 74, 51, 73, 47, 0, 23, 35, 62, 15, 23, 51, 73, 15, 72, 51, 73, 15, 10, 54, 73, 7, 91, 53, 62, 7, 91, 54, 73, 7, 70, 51, 107, 87, 78, 54, 73, 15, 78, 54, 61, 15, 90, 54, 73, 15, 90, 51, 61, 15, 74, 35, 58, 22, 74, 35, 73, 15, 0, 67, 54, 73, 15, 67, 35, 61, 15, 23, 51, 61, 41, 23, 51, 76, 19, 72, 51, 106, 34, 91, 51, 76, 15, 91, 51, 97, 15, 70, 35, 20, 15, 70, 54, 76, 15, 70, 51, 97, 34, 78, 54, 73, 31, 78, 54, 20, 31, 74, 51, 58, 19, 74, 64, 73, 41, 0, 23, 64, 73, 7, 72, 54, 9, 7, 91, 51, 73, 7, 70, 51, 97, 87, 74, 51, 58, 19, 74, 54, 20, 19, 0, 23, 54, 58, 52, 23, 54, 20, 19, 72, 51, 120, 34, 91, 64, 20, 15, 13, 64, 76, 15, 70, 51, 73, 22, 70, 54, 73, 15, 70, 53, 62, 15, 70, 35, 73, 7, 70, 51, 107, 87, 78, 49, 62, 34, 74, 54, 62, 19]\n",
      "[0, 1, 2, 162, 1, 49, 9, 80, 16, 53, 9, 15, 78, 45, 73, 15, 17, 54, 21, 7, 27, 49, 73, 31, 0, 1, 38, 20, 66, 10, 53, 21, 15, 91, 63, 73, 15, 13, 53, 9, 34, 90, 63, 73, 15, 27, 45, 21, 15, 74, 54, 20, 22, 0, 1, 32, 20, 7, 4, 49, 73, 36, 74, 45, 20, 22, 74, 5, 20, 22, 74, 51, 21, 22, 74, 53, 73, 22, 74, 51, 9, 22, 0, 1, 35, 62, 22, 1, 64, 26, 31, 4, 64, 62, 31, 8, 54, 73, 31, 72, 53, 73, 102]\n",
      "Epoch: 151 | Time: 1m 4s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 4.938 |  Val. PPL: 139.453\n",
      "Epoch: 152 | Time: 1m 3s\n",
      "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
      "\t Val. Loss: 5.080 |  Val. PPL: 160.723\n",
      "Epoch: 153 | Time: 1m 3s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.552\n",
      "\t Val. Loss: 5.066 |  Val. PPL: 158.605\n",
      "Epoch: 154 | Time: 1m 3s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 4.994 |  Val. PPL: 147.596\n",
      "Epoch: 155 | Time: 1m 3s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
      "\t Val. Loss: 5.010 |  Val. PPL: 149.832\n",
      "Epoch: 156 | Time: 1m 3s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 5.112 |  Val. PPL: 165.942\n",
      "Epoch: 157 | Time: 1m 3s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 4.984 |  Val. PPL: 146.049\n",
      "Epoch: 158 | Time: 1m 3s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 5.101 |  Val. PPL: 164.142\n",
      "Epoch: 159 | Time: 1m 3s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.533\n",
      "\t Val. Loss: 5.089 |  Val. PPL: 162.189\n",
      "Epoch: 160 | Time: 1m 3s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 5.159 |  Val. PPL: 173.954\n",
      "=> Saving checkpoint\n",
      "Epoch: 161 | Time: 1m 3s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 5.014 |  Val. PPL: 150.521\n",
      "=> Saving checkpoint\n",
      "Epoch: 162 | Time: 1m 3s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 5.119 |  Val. PPL: 167.136\n",
      "Epoch: 163 | Time: 1m 3s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 5.121 |  Val. PPL: 167.560\n",
      "Epoch: 164 | Time: 1m 3s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.520\n",
      "\t Val. Loss: 5.138 |  Val. PPL: 170.446\n",
      "Epoch: 165 | Time: 1m 3s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 5.204 |  Val. PPL: 181.990\n",
      "Epoch: 166 | Time: 1m 3s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 5.112 |  Val. PPL: 165.941\n",
      "Epoch: 167 | Time: 1m 3s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 5.188 |  Val. PPL: 179.026\n",
      "Epoch: 168 | Time: 1m 3s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 5.222 |  Val. PPL: 185.221\n",
      "Epoch: 169 | Time: 1m 3s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 5.166 |  Val. PPL: 175.249\n",
      "Epoch: 170 | Time: 1m 3s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 5.181 |  Val. PPL: 177.902\n",
      "Epoch: 171 | Time: 1m 3s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 5.189 |  Val. PPL: 179.250\n",
      "Epoch: 172 | Time: 1m 3s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.494\n",
      "\t Val. Loss: 5.291 |  Val. PPL: 198.536\n",
      "Epoch: 173 | Time: 1m 3s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 5.226 |  Val. PPL: 186.128\n",
      "Epoch: 174 | Time: 1m 3s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.491\n",
      "\t Val. Loss: 5.315 |  Val. PPL: 203.465\n",
      "Epoch: 175 | Time: 1m 3s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 5.245 |  Val. PPL: 189.603\n",
      "[0, 1, 2, 203, 1, 38, 46, 7, 4, 38, 58, 15, 8, 25, 58, 66, 16, 18, 58, 15, 78, 38, 73, 7, 17, 79, 58, 85, 0, 1, 11, 48, 7, 4, 25, 58, 22, 8, 79, 58, 108, 0, 13, 11, 62, 47, 17, 25, 58, 77, 0, 8, 79, 58, 184]\n",
      "[0, 1, 43, 176, 4, 38, 86, 31, 8, 64, 96, 28, 16, 51, 86, 31, 17, 51, 86, 28, 0, 4, 51, 12, 31, 8, 32, 57, 31, 10, 51, 12, 31, 13, 51, 57, 31, 16, 51, 50, 31, 17, 51, 58, 34, 0, 1, 51, 48, 31, 4, 54, 50, 31, 8, 54, 6, 28, 16, 51, 50, 31, 17, 53, 48, 31, 27, 53, 50, 31, 0, 1, 54, 48, 31, 4, 54, 48, 31, 8, 35, 50, 108]\n",
      "[0, 1, 2, 159, 1, 49, 50, 19, 8, 2, 159, 8, 51, 50, 41, 13, 2, 159, 13, 2, 159, 16, 51, 9, 31, 17, 49, 6, 37, 0, 1, 2, 159, 1, 64, 6, 31, 4, 54, 6, 15, 23, 51, 6, 15, 8, 2, 159, 8, 51, 6, 31, 10, 35, 50, 15, 13, 2, 159, 16, 64, 6, 7, 17, 2, 159, 27, 54, 50, 15, 74, 51, 50, 15, 0, 1, 2, 159, 1, 2, 159, 1, 32, 6, 15, 67, 54, 6, 15, 4, 51, 6, 31, 8, 2, 159, 8, 51, 6, 31, 10, 54, 6, 15, 13, 2, 159, 13, 51, 6, 19, 17, 2, 159, 27, 35, 50, 15, 74, 51, 6, 15, 0, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 64, 6, 31, 4, 64, 6, 31, 8, 2, 159, 8, 2, 159, 8, 64, 6, 31, 10, 64, 6, 15, 91, 54, 6, 15, 13, 2, 159, 13, 51, 6, 15, 70, 35, 50, 15, 16, 35, 6, 7, 78, 35, 6, 15, 17, 2, 159, 27, 49, 6, 15, 74, 49, 50, 22, 0, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 51, 6, 31, 4, 54, 6, 15, 23, 51, 6, 15, 8, 2, 159, 8, 2, 159, 8, 54, 6, 15, 72, 54, 6, 15, 10, 54, 50, 15, 13, 2, 159, 13, 51, 6, 47, 17, 2, 159, 17, 64, 6, 7, 27, 35, 48, 15, 74, 54, 6, 15, 74, 35, 30, 15, 0, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 11, 6, 15, 4, 51, 6, 31, 4, 53, 6, 15, 23, 64, 6, 31, 8, 2, 159, 72, 54, 48, 15, 91, 35, 48, 15, 13, 2, 159, 13, 64, 48, 15, 13, 2, 159, 13, 64, 6, 15, 16, 51, 50, 41, 17, 2, 159, 17, 2, 159, 17, 64, 30, 15, 90, 64, 33, 15, 27, 45, 30, 77, 0, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 2, 159, 1, 2, 159, 67, 53, 33, 31, 8, 2, 159, 1, 2, 159, 67, 64, 33, 31, 8, 2, 159, 67, 64, 50, 31, 8, 2, 159, 8, 2, 159, 8, 32, 6, 22, 8, 54, 6, 22, 72, 54, 50, 15, 10, 64, 6, 15, 91, 54, 6, 15, 13, 2, 159, 13, 2, 159, 13, 2, 159, 16, 64, 48, 15, 78, 64, 6, 15, 17, 54, 50, 15, 90, 54, 50, 15, 90, 54, 6, 34, 17, 2, 159, 27, 32, 6, 31, 74, 49, 6, 41, 0, 1, 2, 159, 1, 54, 6, 31]\n",
      "Epoch: 176 | Time: 1m 3s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 5.174 |  Val. PPL: 176.539\n",
      "Epoch: 177 | Time: 1m 3s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
      "\t Val. Loss: 5.254 |  Val. PPL: 191.311\n",
      "Epoch: 178 | Time: 1m 3s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 5.185 |  Val. PPL: 178.561\n",
      "Epoch: 179 | Time: 1m 3s\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 5.210 |  Val. PPL: 183.174\n",
      "Epoch: 180 | Time: 1m 3s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 5.315 |  Val. PPL: 203.465\n",
      "=> Saving checkpoint\n",
      "Epoch: 181 | Time: 1m 4s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 5.316 |  Val. PPL: 203.489\n",
      "=> Saving checkpoint\n",
      "Epoch: 182 | Time: 1m 3s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
      "\t Val. Loss: 5.277 |  Val. PPL: 195.711\n",
      "Epoch: 183 | Time: 1m 3s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.460\n",
      "\t Val. Loss: 5.337 |  Val. PPL: 207.942\n",
      "Epoch: 184 | Time: 1m 3s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 5.234 |  Val. PPL: 187.584\n",
      "Epoch: 185 | Time: 1m 4s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 5.268 |  Val. PPL: 194.109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186 | Time: 1m 3s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 5.337 |  Val. PPL: 207.955\n",
      "Epoch: 187 | Time: 1m 3s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 5.288 |  Val. PPL: 197.956\n",
      "Epoch: 188 | Time: 1m 3s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 5.354 |  Val. PPL: 211.426\n",
      "Epoch: 189 | Time: 1m 3s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 5.384 |  Val. PPL: 217.843\n",
      "Epoch: 190 | Time: 1m 3s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 5.368 |  Val. PPL: 214.468\n",
      "Epoch: 191 | Time: 1m 3s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 5.492 |  Val. PPL: 242.841\n",
      "Epoch: 192 | Time: 1m 3s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 5.370 |  Val. PPL: 214.939\n",
      "Epoch: 193 | Time: 1m 3s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 5.442 |  Val. PPL: 230.902\n",
      "Epoch: 194 | Time: 1m 3s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 5.429 |  Val. PPL: 227.913\n",
      "Epoch: 195 | Time: 1m 3s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 5.423 |  Val. PPL: 226.671\n",
      "Epoch: 196 | Time: 1m 4s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 5.434 |  Val. PPL: 229.049\n",
      "Epoch: 197 | Time: 1m 3s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 5.448 |  Val. PPL: 232.288\n",
      "Epoch: 198 | Time: 1m 3s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 5.413 |  Val. PPL: 224.292\n",
      "Epoch: 199 | Time: 1m 3s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 5.492 |  Val. PPL: 242.732\n",
      "Epoch: 200 | Time: 1m 3s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 5.575 |  Val. PPL: 263.643\n",
      "=> Saving checkpoint\n",
      "[0, 1, 2, 203, 1, 38, 46, 7, 4, 38, 62, 15, 8, 25, 62, 66, 16, 18, 62, 15, 78, 38, 58, 7, 17, 79, 62, 85, 0, 1, 11, 46, 7, 4, 25, 62, 22, 8, 79, 62, 108, 0, 13, 11, 62, 47, 17, 25, 50, 77, 0, 8, 79, 62, 184]\n",
      "[0, 1, 43, 176, 70, 51, 39, 102, 78, 35, 50, 15, 78, 51, 58, 15, 78, 35, 12, 15, 90, 53, 50, 15, 90, 51, 58, 15, 90, 35, 12, 15, 74, 49, 48, 22, 74, 49, 6, 22, 74, 35, 57, 15, 0, 67, 53, 50, 15, 67, 53, 58, 22, 67, 53, 57, 15, 23, 53, 58, 66, 23, 53, 57, 66, 23, 35, 96, 66, 72, 51, 96, 52, 70, 51, 39, 108, 78, 123, 48, 15, 78, 53, 62, 15, 78, 35, 6, 15, 90, 53, 48, 15, 90, 35, 50, 15, 90, 54, 57, 22, 74, 53, 46, 22, 74, 53, 50, 15, 74, 53, 58, 15, 0, 67, 49, 48, 22, 67, 35, 6, 22, 67, 35, 57, 15, 23, 54, 6, 34, 23, 54, 57, 34, 23, 35, 57, 34, 70, 51, 98, 108, 78, 51, 6, 15, 78, 51, 57, 15, 90, 54, 6, 22, 90, 54, 57, 22, 74, 63, 50, 22, 74, 49, 58, 15, 0, 67, 71, 6, 22, 67, 53, 57, 15, 23, 35, 6, 34, 23, 54, 57, 34, 23, 54, 86, 34, 91, 51, 6, 22, 70, 51, 58, 15, 70, 51, 84, 108, 78, 53, 46, 15, 78, 54, 57, 15, 90, 53, 48, 15, 90, 54, 12, 15, 74, 51, 58, 47, 0, 23, 35, 50, 15, 23, 51, 58, 15, 72, 51, 58, 15, 10, 54, 57, 7, 91, 53, 50, 7, 91, 54, 58, 7, 70, 51, 39, 87, 78, 54, 58, 15, 78, 54, 12, 15, 90, 54, 58, 15, 90, 51, 12, 15, 74, 35, 6, 22, 74, 35, 57, 15, 0, 67, 54, 58, 15, 67, 35, 12, 15, 23, 51, 12, 41, 23, 51, 96, 19, 72, 51, 121, 34, 91, 51, 96, 15, 91, 51, 98, 15, 70, 35, 86, 15, 70, 54, 82, 15, 70, 51, 39, 34, 78, 54, 12, 31, 78, 54, 86, 31, 74, 51, 6, 19, 74, 64, 57, 41, 0, 23, 64, 58, 7, 72, 54, 6, 7, 91, 51, 58, 7, 70, 51, 39, 87, 74, 51, 6, 19, 74, 54, 86, 19, 0, 23, 54, 6, 52, 23, 54, 86, 19, 72, 51, 88, 34, 91, 51, 86, 15, 13, 64, 96, 15, 70, 51, 57, 22, 70, 54, 73, 15, 70, 53, 50, 15, 70, 35, 12, 7, 78, 49, 50, 31, 78, 51, 57, 34, 74, 54, 50, 19]\n",
      "[0, 1, 2, 159, 1, 49, 50, 19, 8, 2, 159, 10, 49, 50, 22, 13, 2, 159, 13, 51, 50, 37, 17, 2, 159, 0, 1, 2, 159, 1, 51, 50, 31, 4, 49, 50, 19, 8, 2, 159, 13, 2, 159, 17, 51, 50, 19, 0, 1, 51, 50, 15, 72, 54, 50, 15, 10, 54, 50, 15, 13, 2, 159, 13, 32, 50, 37, 0, 1, 2, 159, 1, 2, 159, 1, 2, 159, 67, 54, 50, 15, 23, 51, 50, 15, 8, 2, 159, 8, 2, 159, 8, 51, 50, 19, 13, 2, 159, 13, 54, 50, 42, 17, 2, 159, 0, 1, 2, 159, 8, 51, 50, 15, 72, 64, 50, 19, 13, 2, 159, 13, 2, 159, 13, 51, 50, 15, 16, 51, 50, 22, 17, 51, 50, 52, 27, 51, 50, 15, 0, 1, 2, 159, 1, 51, 50, 22, 1, 64, 50, 42, 8, 2, 159, 10, 51, 50, 7, 13, 2, 159, 13, 2, 159, 13, 51, 50, 22, 70, 51, 50, 22, 16, 35, 50, 22, 78, 51, 50, 22, 17, 2, 159, 17, 64, 50, 31, 27, 51, 50, 22, 74, 51, 50, 22, 74, 49, 50, 22, 0, 1, 51, 50, 15, 67, 51, 33, 22, 4, 51, 50, 22, 23, 51, 50, 15, 23, 51, 33, 47, 8, 2, 159, 10, 54, 50, 22, 91, 51, 50, 15, 13, 2, 159, 16, 51, 6, 34, 17, 2, 159, 27, 51, 50, 34, 0, 1, 51, 50, 34, 8, 2, 159, 13, 2, 159, 13, 2, 159, 16, 51, 50, 22, 17, 2, 159, 90, 51, 50, 22, 27, 51, 6, 22, 74, 45, 50, 15, 74, 54, 50, 15, 0, 1, 51, 50, 22, 1, 2, 159, 1, 2, 159, 67, 51, 50, 22, 67, 45, 33, 22, 4, 51, 50, 77, 8, 2, 159, 23, 51, 50, 22, 23, 51, 33, 34, 8, 2, 159, 8, 51, 30, 22, 8, 51, 33, 22, 72, 51, 50, 22, 10, 54, 30, 34, 13, 2, 159]\n",
      "Epoch: 201 | Time: 1m 4s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
      "\t Val. Loss: 5.475 |  Val. PPL: 238.608\n",
      "=> Saving checkpoint\n",
      "Epoch: 202 | Time: 1m 3s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 5.577 |  Val. PPL: 264.398\n",
      "Epoch: 203 | Time: 1m 3s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 5.504 |  Val. PPL: 245.599\n",
      "Epoch: 204 | Time: 1m 3s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 5.537 |  Val. PPL: 253.875\n",
      "Epoch: 205 | Time: 1m 3s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 5.530 |  Val. PPL: 252.232\n",
      "Epoch: 206 | Time: 1m 3s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 5.518 |  Val. PPL: 249.216\n",
      "Epoch: 207 | Time: 1m 3s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 5.536 |  Val. PPL: 253.760\n",
      "Epoch: 208 | Time: 1m 4s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 5.595 |  Val. PPL: 269.111\n",
      "Epoch: 209 | Time: 1m 3s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 5.426 |  Val. PPL: 227.220\n",
      "Epoch: 210 | Time: 1m 4s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 5.608 |  Val. PPL: 272.594\n",
      "Epoch: 211 | Time: 1m 3s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 5.614 |  Val. PPL: 274.325\n",
      "Epoch: 212 | Time: 1m 3s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 5.590 |  Val. PPL: 267.728\n",
      "Epoch: 213 | Time: 1m 3s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 5.717 |  Val. PPL: 304.077\n",
      "Epoch: 214 | Time: 1m 3s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 5.677 |  Val. PPL: 292.167\n",
      "Epoch: 215 | Time: 1m 3s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 5.700 |  Val. PPL: 298.877\n",
      "Epoch: 216 | Time: 1m 3s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 5.685 |  Val. PPL: 294.446\n",
      "Epoch: 217 | Time: 1m 4s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 5.605 |  Val. PPL: 271.704\n",
      "Epoch: 218 | Time: 1m 3s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 5.636 |  Val. PPL: 280.350\n",
      "Epoch: 219 | Time: 1m 4s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 5.628 |  Val. PPL: 278.229\n",
      "Epoch: 220 | Time: 1m 3s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 5.683 |  Val. PPL: 293.838\n",
      "=> Saving checkpoint\n",
      "Epoch: 221 | Time: 1m 3s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 5.652 |  Val. PPL: 284.908\n",
      "=> Saving checkpoint\n",
      "Epoch: 222 | Time: 1m 3s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 5.793 |  Val. PPL: 328.050\n",
      "Epoch: 223 | Time: 1m 3s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 5.658 |  Val. PPL: 286.471\n",
      "Epoch: 224 | Time: 1m 3s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 5.783 |  Val. PPL: 324.829\n",
      "Epoch: 225 | Time: 1m 3s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 5.714 |  Val. PPL: 303.096\n",
      "[0, 1, 2, 203, 1, 38, 46, 7, 4, 38, 62, 15, 8, 25, 62, 66, 16, 18, 62, 15, 78, 38, 58, 7, 17, 79, 62, 85, 0, 1, 11, 46, 7, 4, 25, 62, 22, 8, 79, 62, 108, 0, 13, 11, 48, 47, 17, 25, 50, 77, 0, 8, 79, 62, 184]\n",
      "[0, 1, 43, 177, 23, 54, 57, 15, 8, 45, 12, 7, 72, 64, 61, 7, 10, 51, 86, 7, 91, 35, 96, 31, 70, 35, 96, 15, 78, 54, 84, 15, 17, 51, 98, 66, 0, 23, 54, 96, 7, 72, 51, 84, 31, 91, 54, 96, 19, 78, 54, 84, 15, 17, 51, 98, 15, 90, 51, 107, 15, 74, 35, 98, 15, 0, 1, 54, 84, 47, 72, 51, 96, 15, 91, 35, 98, 31, 70, 32, 39, 41, 78, 54, 98, 15, 17, 54, 57, 19, 74, 51, 86, 15, 0, 67, 54, 86, 31, 23, 51, 96, 15, 8, 64, 96, 31, 72, 64, 86, 36, 78, 54, 96, 15, 17, 35, 12, 15, 90, 51, 86, 15, 74, 51, 98, 15, 0, 67, 54, 86, 15, 4, 35, 96, 15, 23, 51, 96, 15, 8, 35, 86, 15, 72, 64, 96, 15, 10, 54, 96, 15, 91, 51, 96, 15, 70, 54, 12, 31, 78, 54, 96, 41, 78, 54, 86, 19, 74, 54, 96, 15, 0, 67, 35, 96, 7, 23, 54, 96, 19, 78, 54, 98, 15, 17, 53, 84, 15, 90, 54, 86, 19, 74, 35, 96, 108, 0, 78, 54, 96, 108, 0, 23, 54, 86, 31, 72, 53, 12, 15, 91, 35, 86, 31, 70, 64, 96, 22, 16, 54, 96, 15, 78, 54, 96, 15, 17, 49, 96, 52, 27, 53, 86, 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 159, 1, 49, 50, 41, 1, 54, 6, 34, 8, 2, 159, 72, 49, 50, 52, 72, 49, 30, 52, 13, 2, 159, 70, 32, 50, 15, 16, 32, 50, 22, 17, 2, 159, 17, 11, 30, 22, 90, 32, 50, 52, 0, 1, 2, 159, 1, 11, 6, 47, 8, 2, 159, 13, 2, 159, 70, 51, 6, 15, 16, 64, 50, 15, 78, 51, 50, 15, 17, 2, 159, 90, 32, 50, 52, 74, 51, 50, 19, 0, 1, 2, 159, 4, 11, 50, 22, 8, 2, 159, 8, 32, 6, 22, 72, 51, 50, 22, 10, 51, 50, 31, 13, 51, 50, 22, 70, 51, 50, 42, 78, 51, 50, 34, 17, 2, 159, 0, 1, 51, 50, 52, 8, 2, 159, 72, 51, 50, 135, 17, 2, 159, 0, 1, 2, 159, 8, 2, 159, 13, 51, 50, 31, 16, 51, 30, 31, 17, 2, 159, 27, 51, 50, 47, 0, 1, 2, 159, 67, 51, 50, 7, 4, 51, 50, 52, 8, 2, 159, 8, 2, 159, 13, 2, 159, 16, 51, 6, 41, 17, 2, 159, 17, 64, 50, 22, 90, 51, 50, 37, 0, 1, 2, 159, 1, 2, 159, 4, 51, 48, 15, 23, 35, 33, 22, 8, 2, 159, 8, 2, 159, 8, 51, 50, 7, 10, 54, 50, 22, 91, 51, 50, 22, 13, 64, 33, 31, 13, 2, 159, 16, 51, 50, 22, 78, 51, 50, 22, 17, 2, 159, 27, 51, 50, 7, 27, 51, 50, 22, 0, 1, 2, 159, 1, 2, 159, 1, 64, 33, 7, 4, 51, 50, 22, 8, 51, 50, 36, 8, 2, 159, 13, 54, 50, 15, 72, 54, 50, 15, 91, 64, 50, 22, 70, 51, 33, 22, 16, 51, 33, 22, 78, 32, 30, 34]\n",
      "Epoch: 226 | Time: 1m 3s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 5.792 |  Val. PPL: 327.605\n",
      "Epoch: 227 | Time: 1m 3s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 5.743 |  Val. PPL: 311.861\n",
      "Epoch: 228 | Time: 1m 3s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 5.802 |  Val. PPL: 330.865\n",
      "Epoch: 229 | Time: 1m 3s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 5.718 |  Val. PPL: 304.320\n",
      "Epoch: 230 | Time: 1m 3s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 5.714 |  Val. PPL: 303.011\n",
      "Epoch: 231 | Time: 1m 4s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 5.745 |  Val. PPL: 312.487\n",
      "Epoch: 232 | Time: 1m 3s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 5.779 |  Val. PPL: 323.440\n",
      "Epoch: 233 | Time: 1m 3s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 5.816 |  Val. PPL: 335.758\n",
      "Epoch: 234 | Time: 1m 3s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 5.762 |  Val. PPL: 318.043\n",
      "Epoch: 235 | Time: 1m 3s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 5.775 |  Val. PPL: 322.016\n",
      "Epoch: 236 | Time: 1m 3s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 5.819 |  Val. PPL: 336.556\n",
      "Epoch: 237 | Time: 1m 4s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 5.930 |  Val. PPL: 376.057\n",
      "Epoch: 238 | Time: 1m 3s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 5.782 |  Val. PPL: 324.447\n",
      "Epoch: 239 | Time: 1m 4s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 5.859 |  Val. PPL: 350.502\n",
      "Epoch: 240 | Time: 1m 3s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 5.809 |  Val. PPL: 333.182\n",
      "=> Saving checkpoint\n",
      "Epoch: 241 | Time: 1m 4s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 5.929 |  Val. PPL: 375.699\n",
      "=> Saving checkpoint\n",
      "Epoch: 242 | Time: 1m 3s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 5.864 |  Val. PPL: 352.270\n",
      "Epoch: 243 | Time: 1m 4s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 5.968 |  Val. PPL: 390.636\n",
      "Epoch: 244 | Time: 1m 3s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 5.901 |  Val. PPL: 365.441\n",
      "Epoch: 245 | Time: 1m 3s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 5.878 |  Val. PPL: 357.010\n",
      "Epoch: 246 | Time: 1m 3s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 5.904 |  Val. PPL: 366.569\n",
      "Epoch: 247 | Time: 1m 4s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 5.939 |  Val. PPL: 379.394\n",
      "Epoch: 248 | Time: 1m 3s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 5.935 |  Val. PPL: 378.007\n",
      "Epoch: 249 | Time: 1m 3s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 5.940 |  Val. PPL: 379.903\n",
      "Epoch: 250 | Time: 1m 3s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 5.872 |  Val. PPL: 354.815\n",
      "[0, 1, 2, 212, 4, 79, 58, 22, 8, 79, 62, 22, 10, 79, 58, 15, 13, 24, 73, 22, 16, 79, 57, 31, 17, 79, 58, 31, 0, 8, 25, 62, 15, 72, 24, 48, 15, 10, 32, 46, 15, 91, 24, 55, 15, 13, 5, 46, 41, 0, 1, 11, 46, 15, 67, 38, 46, 22, 4, 24, 100, 15, 8, 79, 46, 34, 0, 1, 25, 46, 7, 4, 38, 104, 15, 8, 79, 100, 31, 10, 79, 100, 42, 0, 67, 64, 100, 22, 4, 25, 100, 7, 8, 25, 75, 7, 10, 79, 100, 7, 13, 79, 100, 31, 16, 79, 100, 7, 17, 18, 100, 31, 27, 18, 104, 59, 0, 16, 18, 104, 15, 78, 11, 128, 15, 17, 38, 104, 7, 90, 11, 100, 15, 90, 79, 100, 112]\n",
      "[0, 1, 43, 177, 23, 54, 57, 15, 8, 35, 86, 7, 72, 64, 76, 7, 10, 51, 86, 7, 91, 18, 96, 31, 70, 18, 96, 15, 78, 54, 98, 15, 17, 49, 39, 7, 90, 64, 98, 7, 27, 51, 98, 7, 74, 32, 98, 7, 0, 67, 32, 98, 31, 23, 18, 121, 108, 13, 32, 107, 15, 70, 64, 98, 15, 16, 54, 84, 22, 78, 32, 98, 66, 0, 72, 54, 98, 31, 91, 54, 98, 31, 70, 32, 39, 31, 78, 53, 98, 52, 27, 54, 98, 15, 74, 64, 98, 31, 0, 67, 53, 98, 31, 23, 64, 96, 56, 27, 51, 84, 22, 74, 32, 98, 15, 0, 1, 54, 98, 15, 67, 64, 98, 15, 4, 35, 86, 22, 23, 51, 98, 19, 91, 32, 96, 7, 70, 32, 86, 31, 78, 51, 96, 52, 27, 11, 86, 15, 74, 35, 86, 52, 0, 67, 54, 86, 52, 72, 51, 98, 31, 91, 64, 96, 31, 70, 38, 96, 31, 78, 54, 86, 31, 90, 11, 61, 52, 74, 32, 86, 52]\n",
      "[0, 1, 2, 159, 1, 49, 50, 41, 1, 54, 6, 34, 8, 2, 159, 72, 49, 50, 52, 72, 49, 30, 52, 13, 2, 159, 70, 32, 6, 15, 16, 32, 50, 22, 17, 2, 159, 17, 11, 50, 22, 90, 32, 50, 52, 0, 1, 2, 159, 1, 11, 6, 47, 8, 2, 159, 13, 2, 159, 70, 51, 6, 15, 16, 64, 50, 15, 78, 51, 50, 15, 17, 2, 159, 90, 32, 50, 52, 74, 51, 50, 19, 0, 1, 2, 159, 4, 11, 50, 22, 8, 2, 159, 8, 32, 6, 22, 72, 49, 50, 31, 13, 2, 159, 13, 32, 50, 22, 70, 51, 50, 52, 17, 2, 159, 17, 51, 50, 22, 90, 32, 50, 31, 74, 51, 50, 15, 0, 1, 2, 159, 67, 51, 50, 22, 67, 51, 50, 15, 4, 51, 50, 22, 23, 49, 50, 22, 8, 51, 6, 15, 72, 54, 50, 31, 91, 54, 50, 15, 13, 2, 159, 70, 51, 50, 22, 16, 51, 50, 22, 78, 51, 50, 22, 17, 51, 50, 22, 17, 51, 50, 22, 90, 54, 50, 22, 27, 32, 50, 22, 74, 51, 50, 15, 0, 1, 2, 159, 1, 51, 33, 22, 67, 53, 50, 22, 67, 49, 50, 22, 4, 51, 50, 15, 23, 53, 50, 22, 8, 2, 159, 72, 54, 50, 15, 10, 54, 50, 15, 13, 51, 50, 15, 70, 54, 50, 22, 70, 54, 50, 22, 16, 63, 50, 22, 78, 53, 50, 22, 17, 2, 159, 90, 51, 50, 22, 27, 51, 50, 22, 74, 51, 50, 15, 74, 51, 50, 34, 0, 1, 51, 50, 15, 67, 54, 50, 31, 23, 53, 50, 15, 8, 2, 159, 8, 51, 50, 15, 72, 54, 50, 31, 13, 54, 50, 138, 70, 54, 50, 15, 17, 51, 50, 22, 27, 51, 6, 15, 74, 53, 50, 22, 74, 53, 50, 19, 0, 1, 2, 159, 1, 2, 159, 1, 2, 159, 67, 35, 50, 22, 67, 64, 50, 22, 67, 35, 50, 7, 23, 53, 33, 22, 23, 51, 55, 22, 72, 51, 50, 34, 72, 35, 50, 15, 10, 35, 30, 7, 13, 2, 159, 78, 51, 50, 15, 90, 54, 50, 22, 27, 63, 50, 15, 74, 51, 50, 15]\n",
      "Epoch: 251 | Time: 1m 3s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 5.991 |  Val. PPL: 399.917\n",
      "Epoch: 252 | Time: 1m 3s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 6.029 |  Val. PPL: 415.294\n",
      "Epoch: 253 | Time: 1m 3s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 5.953 |  Val. PPL: 384.927\n",
      "Epoch: 254 | Time: 1m 3s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 6.028 |  Val. PPL: 414.753\n",
      "Epoch: 255 | Time: 1m 3s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 5.992 |  Val. PPL: 400.379\n",
      "Epoch: 256 | Time: 1m 3s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 6.019 |  Val. PPL: 411.089\n",
      "Epoch: 257 | Time: 1m 4s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 6.055 |  Val. PPL: 426.174\n",
      "Epoch: 258 | Time: 1m 4s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 6.010 |  Val. PPL: 407.533\n",
      "Epoch: 259 | Time: 1m 3s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 6.061 |  Val. PPL: 428.617\n",
      "Epoch: 260 | Time: 1m 3s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 6.121 |  Val. PPL: 455.498\n",
      "=> Saving checkpoint\n",
      "Epoch: 261 | Time: 1m 3s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 6.130 |  Val. PPL: 459.596\n",
      "=> Saving checkpoint\n",
      "Epoch: 262 | Time: 1m 3s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 6.053 |  Val. PPL: 425.264\n",
      "Epoch: 263 | Time: 1m 4s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 6.080 |  Val. PPL: 437.171\n",
      "Epoch: 264 | Time: 1m 3s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 6.038 |  Val. PPL: 418.861\n",
      "Epoch: 265 | Time: 1m 3s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 6.015 |  Val. PPL: 409.387\n",
      "Epoch: 266 | Time: 1m 3s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 6.169 |  Val. PPL: 477.752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267 | Time: 1m 3s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 6.099 |  Val. PPL: 445.459\n",
      "Epoch: 268 | Time: 1m 4s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 6.107 |  Val. PPL: 448.842\n",
      "Epoch: 269 | Time: 1m 3s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 6.039 |  Val. PPL: 419.353\n",
      "Epoch: 270 | Time: 1m 4s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 6.174 |  Val. PPL: 480.118\n",
      "Epoch: 271 | Time: 1m 3s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 6.092 |  Val. PPL: 442.455\n",
      "Epoch: 272 | Time: 1m 3s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 6.120 |  Val. PPL: 455.078\n",
      "Epoch: 273 | Time: 1m 4s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 6.116 |  Val. PPL: 453.248\n",
      "Epoch: 274 | Time: 1m 3s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 6.189 |  Val. PPL: 487.484\n",
      "Epoch: 275 | Time: 1m 4s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 6.146 |  Val. PPL: 466.710\n",
      "[0, 1, 2, 175, 10, 25, 127, 22, 91, 11, 127, 22, 13, 5, 104, 15, 70, 5, 100, 15, 16, 25, 104, 7, 17, 25, 104, 31, 27, 11, 129, 31, 0, 1, 11, 127, 31, 4, 11, 129, 22, 8, 32, 131, 22, 72, 32, 129, 66, 10, 5, 127, 34, 13, 79, 129, 52, 70, 24, 132, 34, 0, 1, 11, 127, 22, 67, 24, 127, 66, 72, 24, 128, 22, 10, 24, 127, 22, 91, 24, 127, 15, 13, 24, 127, 52, 90, 24, 104, 22, 27, 25, 128, 15, 0, 1, 79, 100, 22, 67, 24, 100, 15, 4, 24, 127, 7, 8, 5, 128, 7, 72, 24, 127, 15, 10, 5, 104, 15]\n",
      "[0, 1, 43, 177, 23, 51, 58, 15, 8, 45, 12, 7, 72, 64, 86, 7, 10, 51, 86, 7, 91, 18, 96, 31, 70, 18, 96, 15, 78, 54, 96, 15, 17, 49, 82, 7, 90, 64, 84, 7, 27, 51, 98, 7, 74, 24, 39, 7, 0, 67, 25, 88, 31, 23, 45, 96, 108, 13, 32, 84, 15, 70, 64, 96, 15, 16, 54, 96, 22, 78, 32, 96, 66, 0, 72, 71, 98, 31, 91, 5, 88, 31, 70, 38, 39, 31, 78, 54, 98, 52, 27, 11, 98, 15, 74, 64, 84, 31, 0, 67, 35, 98, 31, 23, 54, 96, 56, 27, 51, 86, 22, 74, 32, 98, 15, 0, 1, 64, 107, 15, 67, 64, 39, 15, 4, 35, 96, 22, 23, 51, 96, 19, 91, 79, 98, 7, 70, 32, 84, 31, 78, 11, 96, 52, 27, 11, 86, 15, 74, 35, 86, 52, 0, 67, 54, 96, 52, 72, 11, 96, 31, 91, 64, 96, 31, 70, 38, 96, 31, 78, 64, 86, 31, 90, 11, 86, 52, 74, 32, 96, 52]\n",
      "[0, 1, 2, 159, 1, 49, 50, 41, 1, 54, 6, 34, 8, 2, 159, 72, 49, 50, 52, 72, 49, 30, 52, 13, 2, 159, 70, 32, 33, 15, 16, 35, 50, 22, 17, 2, 159, 17, 11, 50, 22, 90, 32, 50, 52, 0, 1, 2, 159, 1, 11, 50, 47, 8, 2, 159, 13, 2, 159, 70, 51, 6, 15, 16, 64, 50, 15, 78, 51, 50, 15, 17, 2, 159, 90, 32, 50, 52, 74, 51, 50, 19, 0, 1, 2, 159, 4, 11, 50, 22, 8, 2, 159, 8, 32, 6, 22, 72, 11, 6, 31, 13, 2, 159, 13, 32, 12, 22, 70, 11, 9, 52, 17, 2, 159, 17, 64, 12, 22, 90, 32, 6, 31, 74, 51, 50, 15, 0, 1, 2, 159, 67, 51, 50, 31, 8, 2, 159, 8, 32, 50, 22, 72, 51, 6, 31, 91, 51, 30, 31, 13, 2, 159, 16, 51, 50, 15, 78, 32, 50, 22, 17, 2, 159, 90, 32, 50, 19, 0, 1, 2, 159, 4, 54, 50, 15, 8, 2, 159, 8, 32, 50, 22, 72, 49, 6, 31, 13, 2, 159, 13, 32, 12, 15, 70, 32, 12, 52, 78, 53, 6, 15, 17, 2, 159, 90, 54, 6, 52, 0, 1, 2, 159, 1, 11, 6, 7, 67, 32, 6, 31, 8, 2, 159, 8, 32, 96, 15, 72, 32, 82, 52, 13, 2, 159, 13, 54, 12, 31, 16, 53, 50, 22, 17, 2, 159, 17, 51, 50, 22, 90, 32, 21, 19, 0, 1, 2, 159, 67, 53, 12, 15, 4, 51, 57, 22, 8, 51, 50, 22, 72, 35, 6, 22]\n",
      "Epoch: 276 | Time: 1m 3s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 6.166 |  Val. PPL: 476.091\n",
      "Epoch: 277 | Time: 1m 3s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 6.231 |  Val. PPL: 508.330\n",
      "Epoch: 278 | Time: 1m 4s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 6.242 |  Val. PPL: 514.092\n",
      "Epoch: 279 | Time: 1m 3s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 6.250 |  Val. PPL: 518.089\n",
      "Epoch: 280 | Time: 1m 4s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 6.213 |  Val. PPL: 499.419\n",
      "=> Saving checkpoint\n",
      "Epoch: 281 | Time: 1m 3s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 6.191 |  Val. PPL: 488.125\n",
      "=> Saving checkpoint\n",
      "Epoch: 282 | Time: 1m 3s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 6.313 |  Val. PPL: 551.890\n",
      "Epoch: 283 | Time: 1m 3s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 6.231 |  Val. PPL: 508.495\n",
      "Epoch: 284 | Time: 1m 4s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 6.349 |  Val. PPL: 571.706\n",
      "Epoch: 285 | Time: 1m 4s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 6.330 |  Val. PPL: 561.431\n",
      "Epoch: 286 | Time: 1m 4s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 6.335 |  Val. PPL: 563.838\n",
      "Epoch: 287 | Time: 1m 3s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 6.440 |  Val. PPL: 626.647\n",
      "Epoch: 288 | Time: 1m 3s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 6.281 |  Val. PPL: 534.504\n",
      "Epoch: 289 | Time: 1m 3s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 6.355 |  Val. PPL: 575.613\n",
      "Epoch: 290 | Time: 1m 3s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 6.312 |  Val. PPL: 551.000\n",
      "Epoch: 291 | Time: 1m 4s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 6.438 |  Val. PPL: 625.424\n",
      "Epoch: 292 | Time: 1m 4s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 6.257 |  Val. PPL: 521.852\n",
      "Epoch: 293 | Time: 1m 3s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 6.378 |  Val. PPL: 588.573\n",
      "Epoch: 294 | Time: 1m 3s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 6.423 |  Val. PPL: 616.129\n",
      "Epoch: 295 | Time: 1m 3s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 6.370 |  Val. PPL: 583.927\n",
      "Epoch: 296 | Time: 1m 3s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 6.419 |  Val. PPL: 613.214\n",
      "Epoch: 297 | Time: 1m 3s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 6.354 |  Val. PPL: 574.845\n",
      "Epoch: 298 | Time: 1m 3s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 6.432 |  Val. PPL: 621.574\n",
      "Epoch: 299 | Time: 1m 3s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 6.368 |  Val. PPL: 582.969\n",
      "Epoch: 300 | Time: 1m 3s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 6.454 |  Val. PPL: 635.043\n",
      "=> Saving checkpoint\n",
      "[0, 1, 2, 154, 8, 2, 154, 13, 2, 154, 78, 53, 75, 15, 17, 5, 104, 22, 90, 25, 100, 47, 0, 4, 5, 46, 31, 8, 25, 100, 22, 78, 64, 104, 22, 17, 53, 93, 22, 17, 64, 93, 22, 90, 64, 93, 15, 90, 11, 100, 7, 27, 51, 93, 22, 27, 25, 111, 22, 74, 38, 93, 22, 74, 25, 100, 15, 0, 1, 11, 104, 15, 1, 24, 100, 7, 4, 32, 122, 31, 4, 25, 75, 52, 78, 32, 93, 15, 17, 38, 104, 22, 90, 25, 30, 22, 27, 25, 55, 22, 74, 25, 30, 15, 0, 1, 5, 100, 15, 1, 5, 55, 15, 67, 25, 30, 31, 4, 25, 75, 22, 78, 32, 100, 15, 17, 32, 104, 15, 90, 11, 128, 41, 0, 1, 5, 55, 15, 4, 25, 30, 15, 23, 38, 100, 15, 8, 25, 100, 22, 8, 25, 46, 22, 72, 25, 111, 15, 13, 51, 100, 22, 70, 25, 100, 22, 78, 25, 29, 22, 17, 5, 55, 22, 90, 25, 100, 7]\n",
      "[0, 1, 43, 177, 8, 32, 46, 87, 16, 49, 86, 7, 78, 51, 84, 15, 17, 32, 88, 22, 27, 32, 84, 22, 0, 1, 64, 48, 34, 1, 32, 144, 15, 4, 51, 84, 22, 8, 64, 111, 103, 8, 51, 82, 22, 16, 49, 82, 7, 78, 54, 82, 15, 17, 51, 98, 7, 27, 53, 82, 15, 0, 1, 32, 100, 34, 1, 51, 120, 15, 4, 51, 82, 15, 8, 51, 122, 103, 16, 54, 20, 7, 78, 54, 96, 15, 17, 64, 84, 22, 27, 64, 98, 22, 0, 1, 64, 111, 41, 1, 64, 107, 15, 4, 51, 84, 22, 8, 64, 111, 60, 13, 54, 88, 41, 17, 64, 97, 41, 0, 1, 32, 48, 41, 1, 32, 82, 34, 8, 51, 50, 108, 16, 49, 86, 7, 78, 51, 84, 15, 17, 32, 39, 22, 27, 54, 84, 22, 0, 1, 32, 88, 15, 4, 51, 84, 22, 8, 32, 46, 87, 8, 51, 82, 22, 16, 49, 82, 7, 78, 54, 82, 15, 17, 51, 98, 7, 27, 53, 82, 15, 0, 1, 51, 140, 15, 4, 51, 82, 15, 8, 11, 48, 198, 16, 54, 61, 7, 78, 54, 96, 15, 17, 64, 84, 22, 27, 64, 96, 22, 0, 1, 64, 97, 15, 4, 51, 96, 22, 8, 64, 39, 7, 10, 38, 97, 15, 13, 51, 39, 7, 16, 54, 97, 15, 17, 51, 39, 7]\n",
      "[0, 1, 2, 159, 1, 49, 50, 41, 1, 54, 6, 34, 8, 2, 159, 72, 49, 6, 52, 72, 49, 30, 52, 13, 2, 159, 70, 32, 33, 15, 16, 35, 50, 22, 17, 2, 159, 17, 11, 50, 22, 90, 51, 50, 52, 0, 1, 2, 159, 1, 11, 50, 47, 8, 2, 159, 13, 2, 159, 70, 51, 6, 15, 16, 64, 50, 15, 78, 51, 50, 15, 17, 2, 159, 90, 32, 50, 52, 74, 51, 50, 19, 0, 1, 2, 159, 4, 51, 50, 22, 8, 2, 159, 8, 32, 6, 22, 72, 35, 50, 31, 13, 2, 159, 13, 32, 6, 22, 70, 51, 6, 52, 17, 2, 159, 17, 64, 6, 22, 90, 54, 50, 31, 74, 51, 50, 15, 0, 1, 2, 159, 67, 51, 50, 31, 8, 2, 159, 8, 54, 50, 22, 72, 51, 6, 15, 10, 51, 50, 31, 13, 2, 159, 16, 51, 50, 15, 78, 53, 6, 22, 17, 2, 159, 90, 32, 50, 19, 0, 1, 2, 159, 4, 35, 6, 15, 8, 2, 159, 8, 51, 50, 22, 72, 49, 6, 31, 13, 2, 159, 13, 51, 6, 15, 70, 51, 6, 52, 78, 53, 6, 15, 17, 2, 159, 90, 54, 50, 52, 0, 1, 2, 159, 1, 2, 159, 67, 54, 6, 15, 4, 35, 50, 15, 8, 2, 159, 8, 49, 6, 15, 72, 54, 50, 15, 72, 54, 6, 31, 91, 64, 33, 15, 13, 2, 159, 13, 51, 50, 41, 17, 51, 6, 41, 0, 1, 2, 159, 67, 63, 6, 15, 4, 45, 50, 22, 8, 51, 48, 22, 67, 35, 50, 15, 4, 49, 6, 52, 8, 54, 50, 47, 13, 54, 58, 19, 13, 54, 50, 22, 70, 51, 6, 42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301 | Time: 1m 3s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 6.451 |  Val. PPL: 633.057\n",
      "=> Saving checkpoint\n",
      "Epoch: 302 | Time: 1m 3s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 6.430 |  Val. PPL: 620.417\n",
      "Epoch: 303 | Time: 1m 3s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 6.480 |  Val. PPL: 651.843\n",
      "Epoch: 304 | Time: 1m 3s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 6.503 |  Val. PPL: 666.956\n",
      "Epoch: 305 | Time: 1m 3s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 6.531 |  Val. PPL: 686.056\n",
      "Epoch: 306 | Time: 1m 3s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 6.427 |  Val. PPL: 618.374\n",
      "Epoch: 307 | Time: 1m 4s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 6.522 |  Val. PPL: 679.839\n",
      "Epoch: 308 | Time: 1m 3s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 6.483 |  Val. PPL: 653.699\n",
      "Epoch: 309 | Time: 1m 3s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 6.495 |  Val. PPL: 661.502\n",
      "Epoch: 310 | Time: 1m 3s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 6.439 |  Val. PPL: 625.561\n",
      "Epoch: 311 | Time: 1m 3s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 6.475 |  Val. PPL: 648.741\n",
      "Epoch: 312 | Time: 1m 3s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 6.515 |  Val. PPL: 675.137\n",
      "Epoch: 313 | Time: 1m 4s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 6.491 |  Val. PPL: 658.909\n",
      "Epoch: 314 | Time: 1m 3s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.141\n",
      "\t Val. Loss: 6.610 |  Val. PPL: 742.658\n",
      "Epoch: 315 | Time: 1m 4s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 6.544 |  Val. PPL: 694.863\n",
      "Epoch: 316 | Time: 1m 3s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 6.626 |  Val. PPL: 754.440\n",
      "Epoch: 317 | Time: 1m 3s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 6.560 |  Val. PPL: 706.203\n",
      "Epoch: 318 | Time: 1m 4s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 6.527 |  Val. PPL: 683.574\n",
      "Epoch: 319 | Time: 1m 3s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 6.604 |  Val. PPL: 737.715\n",
      "Epoch: 320 | Time: 1m 3s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 6.557 |  Val. PPL: 704.472\n",
      "=> Saving checkpoint\n",
      "Epoch: 321 | Time: 1m 4s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 6.665 |  Val. PPL: 784.766\n",
      "=> Saving checkpoint\n",
      "Epoch: 322 | Time: 1m 4s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 6.503 |  Val. PPL: 667.439\n",
      "Epoch: 323 | Time: 1m 4s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 6.619 |  Val. PPL: 749.352\n",
      "Epoch: 324 | Time: 1m 3s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 6.586 |  Val. PPL: 724.541\n",
      "Epoch: 325 | Time: 1m 3s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 6.669 |  Val. PPL: 787.688\n",
      "[0, 1, 2, 177, 4, 18, 46, 22, 23, 14, 75, 22, 8, 5, 55, 22, 72, 14, 100, 7, 91, 14, 100, 15, 13, 14, 75, 22, 70, 18, 100, 7, 78, 14, 100, 15, 17, 25, 100, 42, 0, 4, 25, 100, 22, 23, 14, 104, 22, 8, 14, 128, 22, 72, 25, 128, 7, 91, 25, 100, 22, 13, 25, 100, 19, 78, 25, 128, 7, 90, 18, 104, 66, 0, 4, 18, 100, 22, 23, 5, 104, 15, 8, 25, 100, 22, 72, 5, 93, 41, 70, 25, 100, 7, 78, 14, 75, 22, 17, 5, 111, 7, 90, 5, 29, 47, 0, 4, 18, 75, 15, 23, 24, 100, 15, 8, 5, 29, 22, 72, 25, 100, 7, 70, 18, 100, 15, 78, 18, 104, 15, 17, 25, 100, 22, 90, 5, 104, 15]\n",
      "[0, 1, 43, 177, 23, 54, 12, 19, 8, 45, 12, 7, 72, 64, 84, 7, 10, 51, 84, 7, 91, 32, 98, 31, 70, 35, 96, 15, 78, 54, 84, 15, 17, 49, 39, 66, 0, 23, 54, 96, 7, 72, 51, 84, 31, 10, 53, 82, 15, 91, 54, 84, 15, 13, 54, 98, 52, 78, 54, 84, 15, 17, 35, 96, 15, 90, 51, 84, 66, 0, 23, 35, 57, 15, 8, 35, 98, 31, 72, 51, 86, 52, 91, 53, 96, 15, 70, 54, 86, 52, 78, 54, 98, 15, 17, 54, 84, 66, 74, 35, 96, 31, 0, 4, 53, 84, 31, 23, 35, 96, 52, 8, 54, 84, 15, 72, 64, 96, 52, 91, 54, 84, 19, 78, 54, 96, 41, 74, 35, 98, 7, 0, 67, 49, 96, 31, 23, 35, 86, 15, 8, 35, 96, 31, 72, 64, 84, 52, 13, 54, 96, 41, 16, 54, 96, 34, 0, 67, 54, 12, 31, 4, 54, 86, 31, 23, 54, 12, 31, 72, 54, 86, 31, 91, 54, 86, 52, 70, 35, 86, 31, 78, 54, 96, 31, 90, 54, 86, 19, 0, 67, 35, 86, 15, 23, 54, 96, 31, 72, 54, 86, 15, 10, 54, 96, 31, 91, 54, 86, 15, 13, 35, 12, 34, 78, 51, 86, 19]\n",
      "[0, 1, 2, 159, 1, 49, 50, 41, 1, 54, 6, 34, 8, 2, 159, 72, 49, 33, 52, 72, 49, 30, 52, 13, 2, 159, 70, 32, 33, 15, 16, 32, 33, 22, 17, 2, 159, 17, 11, 30, 22, 90, 32, 50, 52, 0, 1, 2, 159, 1, 11, 50, 47, 8, 2, 159, 13, 2, 159, 70, 51, 6, 15, 16, 64, 6, 15, 78, 51, 50, 15, 17, 2, 159, 90, 32, 50, 52, 74, 51, 50, 19, 0, 1, 2, 159, 4, 11, 50, 22, 8, 2, 159, 8, 32, 6, 22, 72, 11, 9, 31, 13, 2, 159, 13, 32, 6, 22, 70, 11, 9, 52, 17, 2, 159, 17, 64, 9, 22, 90, 32, 50, 31, 74, 51, 50, 15, 0, 1, 2, 159, 67, 51, 50, 31, 8, 2, 159, 8, 32, 50, 22, 72, 51, 6, 31, 91, 51, 30, 31, 13, 2, 159, 16, 51, 50, 15, 78, 32, 30, 22, 17, 2, 159, 90, 32, 30, 19, 0, 1, 2, 159, 4, 54, 50, 15, 8, 2, 159, 8, 32, 50, 22, 72, 49, 6, 31, 13, 2, 159, 13, 32, 6, 15, 70, 32, 6, 52, 78, 53, 6, 15, 17, 2, 159, 90, 54, 50, 52, 0, 1, 2, 159, 1, 11, 50, 7, 67, 32, 50, 31, 8, 2, 159, 8, 32, 57, 15, 72, 32, 12, 52, 13, 2, 159, 13, 54, 9, 31, 16, 32, 6, 15, 17, 2, 159, 17, 51, 50, 22, 90, 32, 6, 19, 0, 1, 2, 159, 67, 11, 9, 15, 4, 51, 50, 22, 8, 2, 159, 8, 51, 50, 15, 72, 32, 6, 31, 91, 64, 6, 42, 13, 2, 159, 17, 2, 159, 0, 1, 2, 159, 67, 11, 50, 22, 4, 11, 50, 22, 8, 2, 159, 8, 64, 33, 22, 72, 54, 50, 52, 91, 64, 50, 34, 13, 2, 159, 17, 2, 159, 27, 45, 55, 7, 74, 64, 122, 7, 0, 1, 2, 159, 67, 45, 100, 22, 67, 53, 111, 7, 4, 54, 55, 15, 23, 32, 30, 7, 8, 2, 159]\n",
      "Epoch: 326 | Time: 1m 4s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 6.709 |  Val. PPL: 820.050\n",
      "Epoch: 327 | Time: 1m 4s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 6.685 |  Val. PPL: 800.637\n",
      "Epoch: 328 | Time: 1m 4s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 6.702 |  Val. PPL: 814.028\n",
      "Epoch: 329 | Time: 1m 4s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 6.655 |  Val. PPL: 776.991\n",
      "Epoch: 330 | Time: 1m 3s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 6.654 |  Val. PPL: 775.688\n",
      "Epoch: 331 | Time: 1m 3s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 6.730 |  Val. PPL: 837.437\n",
      "Epoch: 332 | Time: 1m 4s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 6.700 |  Val. PPL: 812.138\n",
      "Epoch: 333 | Time: 1m 4s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 6.727 |  Val. PPL: 834.473\n",
      "Epoch: 334 | Time: 1m 3s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 6.726 |  Val. PPL: 834.033\n",
      "Epoch: 335 | Time: 1m 3s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 6.711 |  Val. PPL: 821.334\n",
      "Epoch: 336 | Time: 1m 3s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 6.755 |  Val. PPL: 858.221\n",
      "Epoch: 337 | Time: 1m 3s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 6.769 |  Val. PPL: 870.837\n",
      "Epoch: 338 | Time: 1m 4s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 6.785 |  Val. PPL: 884.785\n",
      "Epoch: 339 | Time: 1m 2s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 6.782 |  Val. PPL: 881.495\n",
      "Epoch: 340 | Time: 1m 3s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 6.813 |  Val. PPL: 909.596\n",
      "=> Saving checkpoint\n",
      "Epoch: 341 | Time: 1m 4s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 6.707 |  Val. PPL: 818.414\n",
      "=> Saving checkpoint\n",
      "Epoch: 342 | Time: 1m 4s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 6.740 |  Val. PPL: 845.380\n",
      "Epoch: 343 | Time: 1m 3s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 6.757 |  Val. PPL: 860.391\n",
      "Epoch: 344 | Time: 1m 4s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 6.758 |  Val. PPL: 860.738\n",
      "Epoch: 345 | Time: 1m 3s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 6.721 |  Val. PPL: 829.425\n",
      "Epoch: 346 | Time: 1m 3s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 6.714 |  Val. PPL: 824.192\n",
      "Epoch: 347 | Time: 1m 3s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 6.817 |  Val. PPL: 912.899\n",
      "Epoch: 348 | Time: 1m 3s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 6.856 |  Val. PPL: 949.348\n",
      "Epoch: 349 | Time: 1m 3s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 6.865 |  Val. PPL: 958.306\n",
      "Epoch: 350 | Time: 1m 3s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 6.847 |  Val. PPL: 941.220\n",
      "[0, 1, 2, 154, 8, 2, 129, 31, 72, 53, 127, 31, 91, 18, 122, 15, 70, 25, 100, 42, 17, 51, 100, 22, 90, 25, 104, 85, 0, 23, 25, 100, 15, 8, 18, 104, 22, 72, 25, 100, 22, 91, 18, 104, 15, 13, 153, 31, 78, 18, 127, 22, 90, 25, 100, 47, 0, 23, 18, 129, 15, 72, 25, 104, 15, 10, 25, 127, 15, 91, 18, 129, 15, 13, 18, 128, 22, 70, 5, 127, 15, 78, 5, 131, 15, 90, 25, 104, 15, 27, 18, 128, 7, 0, 1, 24, 104, 22, 67, 79, 104, 31, 23, 18, 127, 41, 78, 18, 127, 15, 17, 11, 127, 15, 90, 11, 131, 15, 27, 25, 128, 15, 74, 5, 127, 15, 0, 1, 18, 127, 15, 67, 18, 127, 41, 70, 5, 128, 15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 43, 177, 23, 54, 12, 15, 8, 45, 12, 7, 72, 64, 86, 7, 10, 51, 86, 7, 91, 18, 96, 31, 70, 35, 96, 15, 78, 54, 98, 15, 17, 49, 39, 7, 90, 64, 107, 7, 27, 35, 39, 7, 74, 24, 39, 7, 0, 67, 53, 98, 31, 23, 35, 98, 108, 13, 32, 84, 15, 70, 64, 96, 15, 16, 54, 86, 22, 78, 32, 96, 66, 0, 72, 35, 96, 31, 91, 54, 96, 31, 70, 38, 82, 31, 78, 53, 96, 52, 27, 11, 96, 15, 74, 64, 84, 31, 0, 67, 53, 96, 31, 23, 35, 96, 56, 27, 51, 86, 22, 74, 32, 96, 15, 0, 1, 64, 84, 15, 67, 64, 96, 15, 4, 35, 86, 22, 23, 54, 86, 19, 72, 71, 96, 7, 91, 32, 86, 31, 70, 51, 96, 52, 78, 54, 86, 15, 90, 35, 86, 52, 74, 35, 86, 31, 0, 67, 54, 57, 31, 23, 54, 12, 31, 72, 49, 12, 31, 91, 35, 86, 31, 70, 35, 12, 52, 70, 35, 96, 52, 78, 35, 96, 31, 90, 54, 86, 31, 74, 35, 86, 31, 0, 67, 35, 12, 31, 23, 35, 57, 52, 72, 54, 12, 52, 70, 35, 57, 19, 78, 54, 12, 34, 74, 35, 57, 19]\n",
      "[0, 1, 2, 159, 1, 49, 50, 41, 1, 54, 6, 34, 8, 2, 159, 72, 49, 33, 52, 72, 49, 30, 52, 13, 2, 159, 70, 32, 33, 15, 16, 32, 33, 22, 17, 2, 159, 17, 11, 48, 22, 90, 32, 50, 52, 0, 1, 2, 159, 1, 11, 50, 47, 8, 2, 159, 13, 2, 159, 70, 51, 6, 15, 16, 64, 6, 15, 78, 51, 50, 15, 17, 2, 159, 90, 32, 6, 52, 74, 51, 50, 19, 0, 1, 2, 159, 4, 11, 50, 22, 8, 2, 159, 8, 32, 6, 22, 72, 53, 6, 31, 13, 2, 159, 13, 32, 9, 22, 70, 51, 6, 52, 17, 2, 159, 17, 51, 6, 22, 90, 32, 6, 31, 74, 51, 50, 15, 0, 1, 2, 159, 67, 51, 50, 31, 8, 2, 159, 8, 51, 50, 22, 72, 51, 6, 31, 91, 51, 30, 31, 13, 2, 159, 16, 51, 30, 15, 78, 32, 30, 22, 17, 2, 159, 90, 32, 30, 19, 0, 1, 2, 159, 4, 54, 50, 15, 8, 2, 159, 8, 32, 6, 22, 72, 49, 6, 31, 13, 2, 159, 13, 32, 6, 15, 70, 51, 6, 52, 78, 53, 6, 15, 17, 2, 159, 90, 54, 6, 52, 0, 1, 2, 159, 1, 11, 50, 7, 67, 32, 6, 15, 67, 51, 50, 15, 67, 35, 50, 52, 8, 2, 159, 8, 2, 159, 8, 54, 6, 31, 72, 54, 30, 15, 10, 51, 50, 7, 13, 51, 6, 47, 17, 2, 159, 90, 51, 50, 15, 90, 54, 30, 22, 74, 51, 50, 22, 0, 1, 51, 50, 15, 67, 51, 33, 31, 4, 51, 30, 7, 8, 54, 33, 31, 72, 32, 30, 52, 13, 2, 159, 13, 2, 159, 16, 51, 55, 22]\n",
      "Epoch: 351 | Time: 1m 3s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 6.877 |  Val. PPL: 970.054\n",
      "Epoch: 352 | Time: 1m 4s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 6.826 |  Val. PPL: 921.605\n",
      "Epoch: 353 | Time: 1m 3s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 6.793 |  Val. PPL: 891.954\n",
      "Epoch: 354 | Time: 1m 4s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 6.862 |  Val. PPL: 955.639\n",
      "Epoch: 355 | Time: 1m 4s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 6.867 |  Val. PPL: 960.043\n",
      "Epoch: 356 | Time: 1m 4s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 6.899 |  Val. PPL: 990.884\n",
      "Epoch: 357 | Time: 1m 4s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 6.852 |  Val. PPL: 945.472\n",
      "Epoch: 358 | Time: 1m 3s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 6.899 |  Val. PPL: 991.141\n",
      "Epoch: 359 | Time: 1m 4s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 6.941 |  Val. PPL: 1033.993\n",
      "Epoch: 360 | Time: 1m 4s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 6.886 |  Val. PPL: 978.038\n",
      "=> Saving checkpoint\n",
      "Epoch: 361 | Time: 1m 3s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 6.935 |  Val. PPL: 1027.470\n",
      "=> Saving checkpoint\n",
      "Epoch: 362 | Time: 1m 3s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 6.887 |  Val. PPL: 979.546\n",
      "Epoch: 363 | Time: 1m 4s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 6.871 |  Val. PPL: 964.053\n",
      "Epoch: 364 | Time: 1m 4s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 6.932 |  Val. PPL: 1025.012\n",
      "Epoch: 365 | Time: 1m 4s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 6.927 |  Val. PPL: 1019.516\n",
      "Epoch: 366 | Time: 1m 4s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 6.868 |  Val. PPL: 960.847\n",
      "Epoch: 367 | Time: 1m 4s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 6.955 |  Val. PPL: 1048.605\n",
      "Epoch: 368 | Time: 1m 4s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 7.036 |  Val. PPL: 1136.511\n",
      "Epoch: 369 | Time: 1m 4s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 6.973 |  Val. PPL: 1067.028\n",
      "Epoch: 370 | Time: 1m 4s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 7.000 |  Val. PPL: 1096.657\n",
      "Epoch: 371 | Time: 1m 4s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 7.048 |  Val. PPL: 1150.346\n",
      "Epoch: 372 | Time: 1m 3s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 7.035 |  Val. PPL: 1135.493\n",
      "Epoch: 373 | Time: 1m 4s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 6.988 |  Val. PPL: 1083.383\n",
      "Epoch: 374 | Time: 1m 3s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 7.000 |  Val. PPL: 1096.403\n",
      "Epoch: 375 | Time: 1m 3s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 7.010 |  Val. PPL: 1107.539\n",
      "[0, 1, 2, 154, 23, 64, 131, 31, 72, 11, 128, 31, 91, 79, 55, 42, 27, 5, 55, 15, 74, 11, 75, 41, 0, 10, 11, 131, 15, 91, 79, 55, 42, 17, 64, 104, 22, 90, 25, 46, 15, 27, 18, 75, 47, 0, 10, 11, 104, 15, 91, 11, 100, 22, 13, 53, 100, 22, 70, 18, 100, 22, 16, 51, 100, 22, 78, 24, 100, 15, 17, 18, 46, 15, 90, 24, 100, 15, 27, 25, 55, 31, 0, 1, 32, 75, 15, 67, 38, 55, 22, 4, 38, 55, 15, 23, 5, 55, 31, 10, 25, 100, 15, 91, 11, 75, 7, 13, 24, 75, 15, 70, 38, 75, 15, 16, 64, 75, 22, 78, 38, 75, 31, 17, 18, 75, 15, 90, 38, 75, 15, 90, 18, 104, 59]\n",
      "[0, 1, 43, 117, 4, 123, 58, 34, 4, 123, 84, 34, 10, 45, 58, 31, 10, 45, 84, 31, 13, 45, 6, 31, 13, 45, 82, 31, 16, 63, 58, 31, 16, 63, 84, 31, 17, 71, 12, 31, 17, 71, 39, 31, 27, 49, 61, 34, 27, 49, 107, 34, 0, 4, 45, 58, 34, 4, 45, 84, 34, 10, 53, 58, 31, 10, 53, 84, 31, 13, 63, 6, 31, 13, 63, 82, 31, 16, 71, 58, 31, 16, 71, 84, 31, 17, 45, 86, 31, 17, 45, 88, 31, 27, 45, 12, 34, 27, 45, 39, 34, 0, 4, 63, 58, 34, 4, 63, 84, 34, 10, 45, 58, 31, 10, 45, 84, 31, 13, 63, 6, 31, 13, 63, 82, 31, 16, 63, 58, 31, 16, 63, 84, 31, 17, 71, 12, 31, 17, 71, 39, 31, 27, 53, 86, 34, 27, 53, 88, 34, 0, 4, 71, 58, 34, 4, 71, 84, 34, 10, 45, 58, 31, 10, 45, 84, 31, 13, 63, 6, 31, 13, 63, 82, 31, 16, 123, 58, 31, 16, 123, 84, 31, 17, 63, 86, 31, 17, 63, 107, 31, 90, 49, 12, 34, 90, 49, 39, 34]\n",
      "[0, 1, 2, 183, 1, 64, 96, 19, 4, 35, 50, 31, 23, 35, 6, 15, 8, 2, 183, 8, 54, 57, 19, 10, 35, 50, 31, 91, 123, 6, 15, 13, 2, 183, 13, 54, 57, 19, 16, 54, 50, 7, 78, 35, 9, 15, 17, 2, 183, 17, 54, 57, 7, 90, 32, 12, 7, 27, 32, 96, 31, 74, 35, 9, 15, 0, 1, 2, 183, 1, 54, 57, 41, 67, 32, 96, 136, 4, 64, 50, 31, 23, 35, 6, 15, 8, 2, 183, 8, 51, 50, 52, 10, 49, 50, 31, 91, 32, 9, 15, 13, 2, 183, 13, 54, 57, 52, 16, 38, 50, 7, 78, 64, 9, 15, 17, 2, 183, 17, 51, 57, 31, 90, 5, 12, 15, 27, 54, 50, 31, 74, 32, 9, 15, 0, 1, 2, 183, 1, 64, 57, 41, 1, 64, 98, 136, 4, 32, 50, 7, 23, 71, 9, 15, 8, 2, 183, 10, 51, 50, 31, 13, 53, 6, 7, 16, 35, 9, 15, 78, 35, 9, 15, 17, 53, 9, 15, 90, 54, 57, 31, 27, 64, 57, 15, 74, 64, 96, 15, 0, 1, 53, 50, 15, 67, 35, 9, 7, 4, 53, 6, 31, 23, 35, 57, 7, 8, 53, 50, 52, 10, 53, 6, 15, 91, 51, 21, 15, 13, 53, 96, 15, 13, 2, 183, 70, 35, 9, 15, 16, 35, 9, 7, 78, 35, 9, 15, 17, 35, 21, 15, 17, 35, 12, 41, 27, 35, 57, 7, 74, 35, 9, 31, 0, 1, 2, 183, 67, 35, 57, 41, 4, 35, 50, 15, 23, 35, 9, 15, 8, 35, 9, 15, 8, 54, 12, 31, 10, 53, 12, 7, 91, 53, 57, 7, 13, 53, 96, 37]\n",
      "Epoch: 376 | Time: 1m 3s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 7.110 |  Val. PPL: 1223.871\n",
      "Epoch: 377 | Time: 1m 3s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 7.050 |  Val. PPL: 1152.867\n",
      "Epoch: 378 | Time: 1m 3s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 7.076 |  Val. PPL: 1183.773\n",
      "Epoch: 379 | Time: 1m 3s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 7.096 |  Val. PPL: 1207.276\n",
      "Epoch: 380 | Time: 1m 3s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 7.036 |  Val. PPL: 1137.227\n",
      "=> Saving checkpoint\n",
      "Epoch: 381 | Time: 1m 3s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 6.968 |  Val. PPL: 1062.119\n",
      "=> Saving checkpoint\n",
      "Epoch: 382 | Time: 1m 4s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 7.164 |  Val. PPL: 1292.000\n",
      "Epoch: 383 | Time: 1m 3s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 7.055 |  Val. PPL: 1158.185\n",
      "Epoch: 384 | Time: 1m 3s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 7.015 |  Val. PPL: 1112.992\n",
      "Epoch: 385 | Time: 1m 4s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 7.061 |  Val. PPL: 1165.644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 386 | Time: 1m 4s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 7.039 |  Val. PPL: 1140.398\n",
      "Epoch: 387 | Time: 1m 3s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 7.082 |  Val. PPL: 1190.181\n",
      "Epoch: 388 | Time: 1m 4s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 7.033 |  Val. PPL: 1133.396\n",
      "Epoch: 389 | Time: 1m 3s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 6.966 |  Val. PPL: 1060.236\n",
      "Epoch: 390 | Time: 1m 3s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 7.052 |  Val. PPL: 1155.281\n",
      "Epoch: 391 | Time: 1m 4s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 7.071 |  Val. PPL: 1177.671\n",
      "Epoch: 392 | Time: 1m 4s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 7.126 |  Val. PPL: 1243.724\n",
      "Epoch: 393 | Time: 1m 4s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 7.147 |  Val. PPL: 1270.058\n",
      "Epoch: 394 | Time: 1m 3s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 7.117 |  Val. PPL: 1232.880\n",
      "Epoch: 395 | Time: 1m 4s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 7.071 |  Val. PPL: 1177.595\n",
      "Epoch: 396 | Time: 1m 4s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 7.081 |  Val. PPL: 1189.748\n",
      "Epoch: 397 | Time: 1m 3s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 7.030 |  Val. PPL: 1130.199\n",
      "Epoch: 398 | Time: 1m 4s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 7.118 |  Val. PPL: 1233.877\n",
      "Epoch: 399 | Time: 1m 3s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 7.072 |  Val. PPL: 1178.888\n",
      "Epoch: 400 | Time: 1m 3s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 7.185 |  Val. PPL: 1319.217\n",
      "=> Saving checkpoint\n",
      "[0, 1, 2, 154, 23, 64, 93, 31, 72, 11, 128, 31, 91, 79, 55, 42, 27, 5, 55, 15, 74, 11, 75, 41, 0, 10, 11, 93, 15, 91, 79, 55, 42, 17, 64, 104, 22, 90, 25, 46, 15, 27, 18, 75, 47, 0, 10, 11, 104, 15, 91, 11, 100, 22, 13, 53, 100, 22, 70, 11, 100, 22, 16, 51, 100, 22, 78, 24, 100, 15, 17, 32, 46, 15, 90, 24, 100, 15, 27, 25, 55, 31, 0, 1, 32, 46, 15, 67, 38, 55, 22, 4, 38, 55, 15, 23, 5, 55, 31, 10, 25, 100, 15, 91, 11, 75, 7, 13, 24, 26, 15, 70, 38, 75, 15, 16, 64, 75, 22, 78, 38, 75, 31, 17, 18, 75, 15, 90, 38, 75, 15, 90, 18, 104, 59]\n",
      "[0, 1, 43, 177, 23, 5, 57, 15, 8, 45, 12, 7, 72, 64, 61, 7, 10, 51, 86, 7, 91, 18, 96, 31, 70, 35, 96, 15, 78, 54, 98, 15, 17, 49, 39, 7, 90, 64, 107, 7, 27, 51, 88, 7, 74, 64, 88, 7, 0, 67, 25, 121, 31, 23, 18, 121, 108, 13, 32, 107, 15, 70, 64, 96, 15, 16, 54, 86, 22, 78, 32, 98, 66, 0, 72, 79, 121, 31, 91, 5, 88, 31, 70, 5, 121, 31, 78, 11, 88, 52, 27, 11, 98, 15, 74, 64, 107, 31, 0, 67, 11, 88, 31, 23, 24, 96, 56, 27, 51, 86, 22, 74, 32, 98, 15, 0, 1, 64, 107, 15, 67, 64, 98, 15, 4, 35, 84, 22, 23, 32, 98, 19, 91, 79, 98, 7, 70, 32, 84, 31, 78, 11, 96, 52, 27, 11, 61, 15, 74, 35, 86, 52, 0, 67, 54, 96, 52, 72, 11, 98, 31, 91, 64, 96, 31, 70, 38, 82, 31, 78, 64, 86, 31, 90, 11, 61, 52, 74, 32, 86, 52]\n",
      "[0, 1, 2, 183, 1, 64, 57, 19, 4, 63, 57, 31, 23, 35, 9, 15, 8, 2, 183, 8, 54, 57, 19, 10, 71, 50, 31, 91, 123, 9, 15, 13, 2, 183, 13, 54, 57, 19, 16, 54, 50, 7, 78, 71, 9, 15, 17, 2, 183, 17, 54, 57, 7, 90, 32, 6, 7, 27, 32, 50, 31, 74, 35, 9, 15, 0, 1, 2, 183, 1, 54, 57, 41, 67, 32, 96, 136, 4, 64, 50, 31, 23, 64, 9, 15, 8, 2, 183, 8, 51, 57, 52, 10, 49, 50, 31, 91, 32, 9, 15, 13, 2, 183, 13, 54, 57, 52, 16, 38, 50, 7, 78, 64, 9, 15, 17, 2, 183, 17, 51, 57, 31, 90, 51, 12, 15, 27, 54, 50, 31, 74, 32, 9, 15, 0, 1, 2, 183, 1, 64, 57, 41, 1, 64, 98, 136, 4, 51, 50, 7, 23, 71, 9, 15, 8, 2, 183, 8, 53, 57, 31, 10, 51, 50, 7, 91, 51, 9, 15, 13, 2, 183, 13, 53, 57, 31, 16, 64, 50, 7, 78, 35, 9, 15, 17, 2, 183, 17, 64, 57, 7, 90, 38, 96, 15, 27, 11, 50, 15, 74, 53, 9, 15, 0, 1, 2, 183, 1, 11, 121, 83, 1, 35, 57, 31, 4, 51, 50, 7, 23, 45, 9, 15, 8, 2, 183, 8, 54, 57, 52, 10, 51, 50, 7, 91, 35, 9, 15, 13, 2, 183, 13, 64, 57, 52, 16, 38, 33, 15, 78, 64, 9, 15, 17, 2, 183, 17, 38, 57, 52, 90, 32, 57, 15, 27, 53, 6, 7, 74, 64, 57, 15, 0, 1, 54, 50, 15, 1, 53, 6, 42, 8, 2, 183, 1, 64, 50, 15, 67, 35, 6, 15, 4, 35, 6, 41, 8, 2, 183, 8, 35, 6, 52, 8, 53, 50, 31, 10, 35, 50, 22, 91, 64, 57, 15, 13, 2, 183, 13, 51, 21, 31, 16, 51, 50, 7, 78, 35, 57, 15, 17, 2, 183, 17, 53, 50, 31, 17, 53, 50, 31, 17, 53, 30, 31, 90, 63, 6, 7, 27, 11, 30, 22, 74, 54, 57, 15, 0, 1, 54, 50, 15, 1, 65, 33, 52, 4, 53, 30, 34, 1, 51, 121, 108]\n",
      "Epoch: 401 | Time: 1m 3s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 7.131 |  Val. PPL: 1250.348\n",
      "=> Saving checkpoint\n",
      "Epoch: 402 | Time: 1m 4s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 7.159 |  Val. PPL: 1286.255\n",
      "Epoch: 403 | Time: 1m 3s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 7.214 |  Val. PPL: 1358.511\n",
      "Epoch: 404 | Time: 1m 3s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 7.061 |  Val. PPL: 1165.995\n",
      "Epoch: 405 | Time: 1m 4s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 7.209 |  Val. PPL: 1351.678\n",
      "Epoch: 406 | Time: 1m 3s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 7.112 |  Val. PPL: 1226.440\n",
      "Epoch: 407 | Time: 1m 4s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 7.110 |  Val. PPL: 1223.849\n",
      "Epoch: 408 | Time: 1m 3s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 7.178 |  Val. PPL: 1310.291\n",
      "Epoch: 409 | Time: 1m 3s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 7.271 |  Val. PPL: 1437.513\n",
      "Epoch: 410 | Time: 1m 4s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 7.286 |  Val. PPL: 1459.005\n",
      "Epoch: 411 | Time: 1m 3s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 7.265 |  Val. PPL: 1429.037\n",
      "Epoch: 412 | Time: 1m 4s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 7.161 |  Val. PPL: 1288.098\n",
      "Epoch: 413 | Time: 1m 4s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 7.175 |  Val. PPL: 1306.390\n",
      "Epoch: 414 | Time: 1m 3s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 7.166 |  Val. PPL: 1295.289\n",
      "Epoch: 415 | Time: 1m 4s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 7.235 |  Val. PPL: 1387.469\n",
      "Epoch: 416 | Time: 1m 3s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 7.174 |  Val. PPL: 1304.590\n",
      "Epoch: 417 | Time: 1m 4s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 7.177 |  Val. PPL: 1308.889\n",
      "Epoch: 418 | Time: 1m 4s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 7.190 |  Val. PPL: 1326.149\n",
      "Epoch: 419 | Time: 1m 4s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 7.308 |  Val. PPL: 1491.508\n",
      "Epoch: 420 | Time: 1m 4s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 7.238 |  Val. PPL: 1391.294\n",
      "=> Saving checkpoint\n",
      "Epoch: 421 | Time: 1m 4s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 7.282 |  Val. PPL: 1454.073\n",
      "=> Saving checkpoint\n",
      "Epoch: 422 | Time: 1m 4s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 7.241 |  Val. PPL: 1395.017\n",
      "Epoch: 423 | Time: 1m 4s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 7.235 |  Val. PPL: 1386.607\n",
      "Epoch: 424 | Time: 1m 3s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 7.267 |  Val. PPL: 1432.908\n",
      "Epoch: 425 | Time: 1m 4s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 7.284 |  Val. PPL: 1456.205\n",
      "[0, 1, 2, 154, 23, 64, 93, 31, 72, 11, 128, 31, 91, 79, 55, 42, 27, 5, 55, 15, 74, 11, 75, 41, 0, 10, 11, 104, 15, 91, 79, 55, 42, 17, 64, 104, 22, 90, 25, 46, 15, 27, 18, 75, 47, 0, 10, 11, 104, 15, 91, 11, 100, 22, 13, 53, 100, 22, 70, 11, 100, 22, 16, 51, 100, 22, 78, 24, 100, 15, 17, 32, 100, 15, 90, 24, 100, 15, 27, 25, 55, 31, 0, 1, 32, 104, 15, 67, 38, 128, 22, 4, 38, 122, 15, 23, 5, 122, 31, 10, 25, 100, 15, 91, 11, 93, 7, 13, 24, 93, 15, 70, 38, 104, 15, 16, 64, 131, 22, 78, 38, 93, 31, 17, 18, 93, 15, 90, 38, 104, 15, 90, 18, 127, 59]\n",
      "[0, 1, 43, 117, 8, 43, 117, 13, 43, 117, 78, 32, 58, 15, 17, 43, 117, 0, 1, 51, 12, 7, 4, 51, 86, 31, 8, 43, 117, 72, 51, 12, 31, 10, 51, 86, 31, 13, 51, 57, 42, 0, 1, 43, 117, 67, 51, 12, 7, 23, 51, 12, 31, 8, 43, 117, 8, 64, 57, 15, 72, 64, 58, 31, 91, 51, 57, 7, 13, 43, 117, 78, 51, 57, 36, 17, 43, 117, 0, 1, 43, 117, 1, 51, 12, 41, 8, 64, 57, 87, 23, 51, 86, 66, 13, 51, 61, 42, 27, 51, 86, 31, 0, 1, 43, 117, 67, 38, 12, 7, 23, 51, 86, 34, 72, 51, 96, 34, 13, 43, 117, 78, 51, 86, 52, 17, 51, 86, 52, 74, 51, 57, 34, 0, 1, 43, 117, 1, 43, 117, 67, 51, 96, 31, 4, 54, 86, 34, 23, 35, 12, 41, 8, 43, 176, 72, 51, 96, 85, 64, 86, 85, 10, 35, 86, 34, 13, 51, 12, 7, 13, 51, 12, 34, 78, 51, 86, 7, 17, 51, 86, 41, 0, 67, 35, 86, 60, 8, 43, 117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 183, 1, 64, 12, 19, 4, 63, 98, 31, 23, 35, 9, 15, 8, 2, 183, 8, 54, 57, 19, 10, 71, 30, 31, 91, 123, 9, 15, 13, 2, 183, 13, 54, 12, 19, 16, 54, 50, 7, 78, 35, 9, 15, 17, 2, 183, 17, 54, 57, 7, 90, 32, 12, 7, 27, 32, 55, 31, 74, 35, 6, 15, 0, 1, 2, 183, 1, 54, 57, 41, 67, 32, 96, 136, 4, 64, 30, 31, 23, 64, 9, 15, 8, 2, 183, 8, 51, 50, 52, 10, 49, 50, 31, 91, 32, 9, 15, 13, 2, 183, 13, 54, 57, 52, 16, 38, 30, 7, 78, 64, 9, 15, 17, 2, 183, 17, 51, 50, 31, 90, 51, 6, 15, 27, 54, 50, 31, 74, 32, 9, 15, 0, 1, 2, 183, 1, 64, 57, 41, 1, 64, 96, 136, 4, 32, 30, 7, 23, 71, 9, 15, 8, 2, 183, 8, 53, 57, 31, 10, 51, 30, 7, 91, 51, 9, 15, 13, 64, 50, 31, 16, 32, 9, 31, 78, 32, 9, 31, 17, 53, 50, 31, 90, 63, 6, 31, 27, 11, 96, 15, 74, 64, 9, 52, 0, 1, 64, 9, 108, 0, 1, 53, 50, 7, 67, 45, 6, 7, 4, 53, 57, 31, 23, 51, 50, 7, 72, 63, 6, 7, 10, 64, 57, 7, 91, 64, 9, 15, 13, 2, 183, 70, 35, 62, 15, 16, 35, 50, 15, 78, 35, 48, 7, 17, 64, 50, 41, 17, 64, 50, 41, 17, 35, 50, 52, 27, 35, 50, 52, 74, 35, 33, 15, 74, 35, 48, 31, 0, 1, 32, 6, 7, 1, 51, 50, 7, 67, 64, 6, 41, 4, 35, 62, 52, 4, 35, 50, 52, 8, 35, 33, 7, 23, 54, 50, 31, 8, 35, 50, 52, 8, 54, 33, 52, 10, 35, 33, 15, 91, 35, 50, 7]\n",
      "Epoch: 426 | Time: 1m 4s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 7.293 |  Val. PPL: 1469.865\n",
      "Epoch: 427 | Time: 1m 4s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 7.286 |  Val. PPL: 1460.180\n",
      "Epoch: 428 | Time: 1m 4s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 7.385 |  Val. PPL: 1611.866\n",
      "Epoch: 429 | Time: 1m 4s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 7.383 |  Val. PPL: 1608.193\n",
      "Epoch: 430 | Time: 1m 4s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 7.366 |  Val. PPL: 1580.849\n",
      "Epoch: 431 | Time: 1m 4s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.078\n",
      "\t Val. Loss: 7.276 |  Val. PPL: 1444.911\n",
      "Epoch: 432 | Time: 1m 3s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 7.275 |  Val. PPL: 1443.398\n",
      "Epoch: 433 | Time: 1m 3s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 7.330 |  Val. PPL: 1525.778\n",
      "Epoch: 434 | Time: 1m 4s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 7.276 |  Val. PPL: 1445.790\n",
      "Epoch: 435 | Time: 1m 3s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 7.311 |  Val. PPL: 1496.644\n",
      "Epoch: 436 | Time: 1m 4s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 7.304 |  Val. PPL: 1485.939\n",
      "Epoch: 437 | Time: 1m 3s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 7.370 |  Val. PPL: 1587.915\n",
      "Epoch: 438 | Time: 1m 4s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 7.371 |  Val. PPL: 1589.929\n",
      "Epoch: 439 | Time: 1m 3s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 7.436 |  Val. PPL: 1695.229\n",
      "Epoch: 440 | Time: 1m 4s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 7.435 |  Val. PPL: 1694.160\n",
      "=> Saving checkpoint\n",
      "Epoch: 441 | Time: 1m 3s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 7.331 |  Val. PPL: 1526.219\n",
      "=> Saving checkpoint\n",
      "Epoch: 442 | Time: 1m 3s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 7.405 |  Val. PPL: 1643.429\n",
      "Epoch: 443 | Time: 1m 3s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 7.388 |  Val. PPL: 1616.919\n",
      "Epoch: 444 | Time: 1m 4s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 7.433 |  Val. PPL: 1690.801\n",
      "Epoch: 445 | Time: 1m 3s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 7.481 |  Val. PPL: 1773.830\n",
      "Epoch: 446 | Time: 1m 3s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 7.372 |  Val. PPL: 1590.268\n",
      "Epoch: 447 | Time: 1m 3s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 7.378 |  Val. PPL: 1601.112\n",
      "Epoch: 448 | Time: 1m 3s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 7.427 |  Val. PPL: 1681.080\n",
      "Epoch: 449 | Time: 1m 3s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 7.518 |  Val. PPL: 1840.716\n",
      "Epoch: 450 | Time: 1m 3s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 7.381 |  Val. PPL: 1605.957\n",
      "[0, 1, 2, 154, 23, 64, 131, 31, 72, 11, 128, 31, 91, 79, 55, 42, 27, 5, 55, 15, 74, 11, 75, 41, 0, 10, 11, 131, 15, 91, 79, 55, 42, 17, 64, 104, 22, 90, 25, 46, 15, 27, 18, 75, 47, 0, 10, 11, 104, 15, 91, 11, 100, 22, 13, 53, 100, 22, 70, 11, 100, 22, 16, 51, 100, 22, 78, 24, 100, 15, 17, 32, 100, 15, 90, 24, 100, 15, 27, 25, 55, 31, 0, 1, 32, 127, 15, 67, 38, 128, 22, 4, 38, 122, 15, 23, 5, 128, 31, 10, 25, 100, 15, 91, 11, 104, 7, 13, 24, 93, 15, 70, 38, 104, 15, 16, 38, 104, 22, 78, 38, 104, 31, 17, 18, 93, 15, 90, 38, 104, 15, 90, 18, 127, 59]\n",
      "[0, 1, 43, 177, 23, 54, 12, 15, 8, 45, 12, 7, 72, 64, 86, 7, 10, 51, 86, 7, 91, 32, 96, 31, 70, 64, 96, 15, 78, 54, 98, 15, 17, 64, 39, 7, 90, 64, 98, 7, 27, 51, 98, 7, 74, 32, 98, 7, 0, 67, 51, 98, 31, 23, 35, 98, 108, 13, 32, 84, 15, 70, 64, 96, 15, 16, 54, 86, 22, 78, 32, 96, 66, 0, 72, 79, 121, 31, 91, 5, 88, 31, 70, 5, 39, 31, 78, 11, 98, 52, 27, 11, 98, 15, 74, 64, 84, 31, 0, 67, 11, 98, 31, 23, 24, 96, 56, 27, 51, 86, 22, 74, 32, 96, 15, 0, 1, 64, 84, 15, 67, 64, 96, 15, 4, 35, 86, 22, 23, 32, 96, 19, 91, 79, 96, 7, 70, 32, 86, 31, 78, 11, 12, 52, 27, 11, 58, 15, 74, 35, 57, 52, 0, 67, 54, 12, 52, 72, 11, 96, 31, 91, 64, 86, 31, 70, 38, 12, 31, 78, 64, 57, 31, 90, 11, 58, 52, 74, 32, 57, 52]\n",
      "[0, 1, 2, 183, 1, 64, 96, 19, 4, 63, 98, 31, 23, 35, 40, 15, 8, 2, 183, 8, 54, 96, 19, 10, 71, 39, 31, 91, 123, 21, 15, 13, 2, 183, 13, 54, 98, 19, 16, 54, 98, 7, 78, 71, 40, 15, 17, 2, 183, 17, 54, 96, 7, 90, 32, 82, 7, 27, 32, 39, 31, 74, 35, 9, 15, 0, 1, 2, 183, 1, 54, 57, 41, 67, 32, 98, 136, 4, 35, 57, 31, 23, 64, 9, 15, 8, 2, 183, 8, 51, 57, 52, 10, 49, 98, 31, 91, 32, 82, 15, 13, 2, 183, 13, 54, 96, 52, 16, 38, 98, 7, 78, 64, 20, 15, 17, 2, 183, 17, 51, 12, 31, 90, 51, 21, 15, 27, 54, 50, 31, 74, 32, 21, 15, 0, 1, 2, 183, 1, 64, 12, 41, 1, 64, 98, 136, 4, 32, 50, 7, 23, 71, 9, 15, 8, 2, 183, 8, 53, 57, 31, 10, 51, 50, 7, 91, 51, 21, 15, 13, 2, 183, 13, 53, 57, 31, 16, 64, 121, 68, 17, 35, 9, 15, 90, 53, 21, 15, 27, 54, 12, 15, 74, 35, 9, 15, 0, 1, 2, 183, 1, 2, 183, 1, 53, 50, 7, 4, 45, 6, 7, 23, 35, 57, 15, 8, 53, 50, 7, 8, 2, 183, 8, 53, 6, 15, 72, 54, 57, 108, 10, 35, 30, 7, 91, 35, 9, 15, 13, 2, 183, 13, 64, 12, 19, 16, 45, 50, 15, 78, 35, 9, 15, 17, 2, 183, 17, 38, 57, 52, 90, 32, 12, 15, 27, 53, 6, 7, 74, 64, 12, 15, 27, 64, 57, 15, 0, 1, 53, 57, 7, 4, 53, 50, 15, 23, 35, 6, 68]\n",
      "Epoch: 451 | Time: 1m 4s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 7.414 |  Val. PPL: 1658.796\n",
      "Epoch: 452 | Time: 1m 3s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 7.506 |  Val. PPL: 1819.360\n",
      "Epoch: 453 | Time: 1m 3s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 7.367 |  Val. PPL: 1582.428\n",
      "Epoch: 454 | Time: 1m 3s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 7.371 |  Val. PPL: 1588.879\n",
      "Epoch: 455 | Time: 1m 3s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 7.414 |  Val. PPL: 1659.095\n",
      "Epoch: 456 | Time: 1m 3s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 7.370 |  Val. PPL: 1587.778\n",
      "Epoch: 457 | Time: 1m 3s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 7.323 |  Val. PPL: 1514.325\n",
      "Epoch: 458 | Time: 1m 3s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 7.507 |  Val. PPL: 1820.039\n",
      "Epoch: 459 | Time: 1m 3s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 7.362 |  Val. PPL: 1574.685\n",
      "Epoch: 460 | Time: 1m 3s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 7.438 |  Val. PPL: 1698.779\n",
      "=> Saving checkpoint\n",
      "Epoch: 461 | Time: 1m 4s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 7.542 |  Val. PPL: 1884.999\n",
      "=> Saving checkpoint\n",
      "Epoch: 462 | Time: 1m 3s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 7.436 |  Val. PPL: 1696.603\n",
      "Epoch: 463 | Time: 1m 4s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 7.431 |  Val. PPL: 1688.137\n",
      "Epoch: 464 | Time: 1m 3s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 7.493 |  Val. PPL: 1795.380\n",
      "Epoch: 465 | Time: 1m 4s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 7.459 |  Val. PPL: 1736.263\n",
      "Epoch: 466 | Time: 1m 3s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 7.453 |  Val. PPL: 1724.414\n",
      "Epoch: 467 | Time: 1m 4s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.467 |  Val. PPL: 1749.923\n",
      "Epoch: 468 | Time: 1m 4s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.554 |  Val. PPL: 1908.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 469 | Time: 1m 4s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.495 |  Val. PPL: 1799.539\n",
      "Epoch: 470 | Time: 1m 3s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.476 |  Val. PPL: 1765.674\n",
      "Epoch: 471 | Time: 1m 4s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.473 |  Val. PPL: 1760.745\n",
      "Epoch: 472 | Time: 1m 3s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 7.488 |  Val. PPL: 1787.115\n",
      "Epoch: 473 | Time: 1m 3s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.550 |  Val. PPL: 1900.891\n",
      "Epoch: 474 | Time: 1m 3s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 7.390 |  Val. PPL: 1620.504\n",
      "Epoch: 475 | Time: 1m 4s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.488 |  Val. PPL: 1785.596\n",
      "[0, 1, 2, 154, 23, 64, 93, 31, 72, 11, 128, 31, 91, 79, 55, 42, 27, 5, 55, 15, 74, 11, 75, 41, 0, 10, 11, 131, 15, 91, 79, 55, 42, 17, 64, 104, 22, 90, 25, 46, 15, 27, 18, 75, 47, 0, 10, 11, 104, 15, 91, 11, 100, 22, 13, 53, 100, 22, 70, 11, 100, 22, 16, 51, 100, 22, 78, 24, 100, 15, 17, 32, 46, 15, 90, 24, 100, 15, 27, 25, 55, 31, 0, 1, 32, 100, 15, 67, 38, 55, 22, 4, 38, 55, 15, 23, 5, 55, 31, 10, 25, 100, 15, 91, 11, 75, 7, 13, 24, 29, 15, 70, 38, 75, 15, 16, 64, 104, 22, 78, 38, 104, 31, 17, 18, 93, 15, 90, 38, 104, 15, 90, 18, 131, 59]\n",
      "[0, 1, 43, 169, 4, 54, 57, 52, 8, 43, 169, 8, 54, 96, 31, 91, 64, 86, 31, 13, 43, 169, 16, 51, 96, 31, 17, 43, 169, 17, 11, 96, 42, 0, 1, 43, 169, 23, 32, 86, 7, 8, 43, 169, 72, 38, 86, 19, 13, 43, 169, 13, 51, 96, 52, 16, 64, 12, 52, 17, 43, 169, 90, 64, 86, 28, 0, 1, 43, 169, 4, 32, 12, 52, 8, 43, 169, 8, 64, 96, 31, 10, 32, 86, 52, 13, 43, 169, 13, 11, 96, 31, 16, 32, 82, 7, 17, 43, 169, 17, 11, 84, 31, 74, 11, 84, 31, 0, 1, 43, 169, 4, 11, 84, 7, 72, 11, 98, 31, 91, 32, 96, 52, 16, 64, 86, 31, 90, 32, 86, 47, 0, 23, 64, 12, 31, 72, 38, 57, 19, 13, 38, 12, 52, 16, 32, 58, 19, 90, 11, 12, 19, 0, 1, 38, 57, 19, 4, 64, 58, 31, 8, 54, 58, 19, 91, 32, 50, 19, 16, 64, 12, 19, 17, 38, 57, 80]\n",
      "[0, 1, 2, 183, 1, 38, 96, 19, 4, 63, 98, 31, 23, 35, 40, 15, 8, 2, 183, 8, 54, 96, 19, 10, 71, 39, 31, 91, 123, 21, 15, 13, 2, 183, 13, 54, 98, 19, 16, 54, 98, 7, 78, 35, 21, 15, 17, 2, 183, 17, 54, 98, 7, 90, 32, 82, 7, 27, 32, 98, 31, 74, 35, 21, 15, 0, 1, 2, 183, 1, 54, 86, 41, 67, 32, 98, 136, 4, 64, 39, 41, 8, 54, 98, 31, 10, 51, 39, 31, 10, 51, 39, 31, 13, 54, 96, 15, 13, 2, 183, 70, 35, 21, 15, 16, 35, 21, 15, 78, 54, 96, 15, 17, 2, 183, 17, 51, 96, 15, 90, 51, 82, 15, 27, 64, 82, 15, 74, 51, 39, 108, 0, 1, 2, 183, 67, 64, 57, 41, 8, 54, 98, 31, 91, 51, 57, 19, 13, 2, 183, 13, 2, 183, 70, 51, 82, 19, 17, 2, 183, 90, 32, 98, 34, 0, 1, 2, 183, 67, 64, 96, 19, 8, 2, 183, 72, 51, 39, 34, 10, 51, 98, 15, 91, 45, 96, 15, 13, 54, 57, 136]\n",
      "Epoch: 476 | Time: 1m 3s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 7.616 |  Val. PPL: 2030.614\n",
      "Epoch: 477 | Time: 1m 4s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 7.486 |  Val. PPL: 1782.172\n",
      "Epoch: 478 | Time: 1m 3s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 7.473 |  Val. PPL: 1759.973\n",
      "Epoch: 479 | Time: 1m 4s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 7.507 |  Val. PPL: 1819.957\n",
      "Epoch: 480 | Time: 1m 4s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 7.632 |  Val. PPL: 2063.867\n",
      "=> Saving checkpoint\n",
      "Epoch: 481 | Time: 1m 3s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 7.573 |  Val. PPL: 1945.607\n",
      "=> Saving checkpoint\n",
      "Epoch: 482 | Time: 1m 3s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 7.391 |  Val. PPL: 1620.783\n",
      "Epoch: 483 | Time: 1m 4s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 7.654 |  Val. PPL: 2108.026\n",
      "Epoch: 484 | Time: 1m 3s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 7.550 |  Val. PPL: 1901.075\n",
      "Epoch: 485 | Time: 1m 4s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 7.495 |  Val. PPL: 1799.368\n",
      "Epoch: 486 | Time: 1m 4s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 7.510 |  Val. PPL: 1826.970\n",
      "Epoch: 487 | Time: 1m 4s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 7.576 |  Val. PPL: 1951.722\n",
      "Epoch: 488 | Time: 1m 4s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 7.634 |  Val. PPL: 2068.005\n",
      "Epoch: 489 | Time: 1m 4s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 7.599 |  Val. PPL: 1995.243\n",
      "Epoch: 490 | Time: 1m 4s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 7.678 |  Val. PPL: 2160.315\n",
      "Epoch: 491 | Time: 1m 4s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 7.488 |  Val. PPL: 1786.062\n",
      "Epoch: 492 | Time: 1m 4s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 7.541 |  Val. PPL: 1883.812\n",
      "Epoch: 493 | Time: 1m 3s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 7.568 |  Val. PPL: 1935.697\n",
      "Epoch: 494 | Time: 1m 4s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 7.525 |  Val. PPL: 1853.021\n",
      "Epoch: 495 | Time: 1m 3s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 7.561 |  Val. PPL: 1920.944\n",
      "Epoch: 496 | Time: 1m 3s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 7.554 |  Val. PPL: 1908.600\n",
      "Epoch: 497 | Time: 1m 3s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 7.585 |  Val. PPL: 1968.586\n",
      "Epoch: 498 | Time: 1m 4s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 7.614 |  Val. PPL: 2027.256\n",
      "Epoch: 499 | Time: 1m 4s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 7.592 |  Val. PPL: 1982.030\n",
      "Epoch: 500 | Time: 1m 4s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 7.500 |  Val. PPL: 1807.719\n",
      "=> Saving checkpoint\n",
      "[0, 1, 2, 154, 23, 64, 131, 31, 72, 11, 128, 31, 91, 79, 55, 42, 27, 5, 55, 15, 74, 11, 75, 41, 0, 10, 11, 131, 15, 91, 79, 55, 42, 17, 64, 104, 22, 90, 25, 46, 15, 27, 18, 75, 47, 0, 10, 11, 104, 15, 91, 11, 100, 22, 13, 53, 100, 22, 70, 11, 100, 22, 16, 51, 128, 22, 78, 24, 100, 15, 17, 32, 46, 15, 90, 24, 100, 15, 27, 25, 55, 31, 0, 1, 32, 127, 15, 67, 38, 128, 22, 4, 38, 122, 15, 23, 5, 128, 31, 10, 25, 100, 15, 91, 11, 104, 7, 13, 24, 93, 15, 70, 38, 104, 15, 16, 64, 104, 22, 78, 38, 104, 31, 17, 18, 93, 15, 90, 38, 104, 15, 90, 18, 127, 59]\n",
      "[0, 1, 43, 176, 70, 51, 39, 102, 78, 35, 50, 15, 78, 51, 58, 15, 78, 35, 12, 15, 90, 53, 50, 15, 90, 51, 58, 15, 90, 35, 12, 15, 74, 49, 48, 22, 74, 49, 6, 22, 74, 35, 57, 15, 0, 67, 53, 50, 15, 67, 53, 58, 22, 67, 53, 12, 15, 23, 53, 58, 66, 23, 53, 12, 66, 23, 35, 96, 66, 72, 51, 96, 52, 70, 51, 98, 108, 78, 123, 48, 15, 78, 53, 6, 15, 78, 35, 57, 15, 90, 53, 48, 15, 90, 35, 6, 15, 90, 54, 57, 22, 74, 53, 46, 22, 74, 53, 50, 15, 74, 53, 58, 15, 0, 67, 49, 48, 22, 67, 35, 6, 22, 67, 35, 57, 15, 23, 54, 6, 34, 23, 54, 57, 34, 23, 35, 86, 34, 70, 51, 98, 108, 78, 51, 6, 15, 78, 51, 57, 15, 90, 54, 6, 22, 90, 54, 57, 22, 74, 63, 50, 22, 74, 49, 58, 15, 0, 67, 71, 6, 22, 67, 53, 57, 15, 23, 35, 6, 34, 23, 54, 57, 34, 23, 54, 86, 34, 91, 51, 6, 22, 70, 51, 58, 15, 70, 51, 84, 108, 78, 53, 46, 15, 78, 54, 57, 15, 90, 53, 48, 15, 90, 54, 12, 15, 74, 51, 58, 47, 0, 23, 35, 50, 15, 23, 51, 58, 15, 72, 51, 58, 15, 10, 54, 57, 7, 91, 53, 50, 7, 91, 54, 58, 7, 70, 51, 39, 87, 78, 54, 58, 15, 78, 54, 12, 15, 90, 54, 58, 15, 90, 51, 12, 15, 74, 35, 6, 22, 74, 35, 57, 15, 0, 67, 54, 58, 15, 67, 35, 12, 15, 23, 51, 12, 41, 23, 51, 96, 19, 72, 51, 121, 34, 91, 51, 96, 15, 91, 51, 84, 15, 70, 35, 86, 15, 70, 54, 82, 15, 70, 51, 98, 34, 78, 54, 12, 31, 78, 54, 86, 31, 74, 51, 6, 19, 74, 64, 57, 41, 0, 23, 64, 58, 7, 72, 54, 6, 7, 91, 51, 58, 7, 70, 51, 98, 87, 74, 51, 6, 19, 74, 54, 86, 19, 0, 23, 54, 6, 52, 23, 54, 86, 19, 72, 51, 88, 34, 91, 64, 86, 15, 13, 64, 96, 15, 70, 51, 57, 22, 70, 54, 73, 15, 70, 53, 50, 15, 70, 35, 12, 7, 70, 51, 39, 87, 78, 49, 50, 31, 78, 51, 57, 34, 74, 54, 50, 19, 74, 38, 58, 66]\n",
      "[0, 1, 2, 183, 1, 38, 96, 19, 4, 63, 98, 22, 23, 35, 40, 15, 8, 2, 183, 8, 54, 96, 19, 10, 71, 39, 31, 91, 123, 21, 15, 13, 2, 183, 13, 54, 98, 19, 16, 54, 96, 7, 78, 35, 21, 15, 17, 2, 183, 17, 54, 96, 7, 90, 51, 96, 7, 27, 32, 98, 15, 74, 35, 9, 15, 0, 1, 2, 183, 1, 54, 57, 41, 67, 32, 98, 136, 4, 64, 39, 41, 8, 54, 98, 31, 10, 54, 98, 15, 91, 51, 40, 15, 13, 54, 96, 15, 13, 2, 183, 70, 71, 21, 15, 16, 35, 21, 15, 78, 54, 96, 15, 17, 2, 183, 17, 51, 96, 15, 90, 51, 96, 15, 27, 35, 21, 15, 74, 35, 12, 15, 0, 1, 2, 183, 1, 2, 183, 1, 35, 9, 41, 1, 64, 98, 136, 4, 53, 57, 7, 23, 71, 12, 15, 8, 2, 183, 8, 53, 21, 31, 72, 51, 6, 7, 10, 71, 12, 15, 91, 51, 50, 15, 13, 53, 6, 15, 13, 2, 183, 13, 53, 6, 7, 16, 53, 21, 15, 78, 35, 12, 15, 17, 35, 57, 7, 90, 65, 21, 28, 17, 53, 50, 15, 27, 53, 6, 15, 74, 35, 21, 7, 74, 53, 12, 7, 0, 1, 11, 12, 7, 4, 54, 50, 15, 23, 35, 57, 7, 4, 54, 12, 47, 8, 53, 6, 7, 10, 54, 50, 7, 10, 53, 57, 34, 10, 51, 9, 7, 91, 54, 6, 7, 13, 2, 183, 13, 53, 50, 7, 16, 53, 6, 7, 78, 35, 9, 7, 17, 53, 50, 41, 17, 38, 57, 31, 27, 64, 21, 7, 27, 53, 50, 85]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 7.858 | Test PPL: 2585.684 |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 500\n",
    "S_EPOCH = 0\n",
    "CLIP = 1\n",
    "train_loss_log = []\n",
    "valid_loss_log = []\n",
    "best_valid_loss = float('inf')\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "#model = nn.DataParallel(model, device_ids=[0,1]).to(device)\n",
    "for epoch in range(S_EPOCH, N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    train_loss_log.append(train_loss)\n",
    "    valid_loss_log.append(valid_loss)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        checkpoint = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "        save_best_checkpoint(checkpoint,N_EPOCHS)\n",
    "    if (epoch+1) % 20 == 0 or (epoch) % 20 == 0:\n",
    "        save_final_checkpoint(checkpoint,epoch)\n",
    "    if (epoch+1) % 25 ==0:\n",
    "        if check_mode_collapse(model) > 1:\n",
    "            print(\"model is mode collapsing\")\n",
    "save_final_checkpoint(checkpoint,N_EPOCHS)\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8b1fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(folder + \"train_loss_log.pkl\", 'wb')\n",
    "pickle.dump(train_loss_log, output)\n",
    "output.close()\n",
    "gug.pkl\", 'wb')\n",
    "pickle.dump(valid_loss_log, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checkpoint = {'model_state_dict': model.state_dict(),\n",
    "#                   'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                   'valid_loss': valid_loss}\n",
    "# save_checkpoint(destination_folder + checkpoint,N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33caff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "005d41b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Transformer:\n\tMissing key(s) in state_dict: \"src2_word_embedding.weight\", \"src2_position_embedding.weight\", \"transformer.encoder2.layers.0.self_attn.in_proj_weight\", \"transformer.encoder2.layers.0.self_attn.in_proj_bias\", \"transformer.encoder2.layers.0.self_attn.out_proj.weight\", \"transformer.encoder2.layers.0.self_attn.out_proj.bias\", \"transformer.encoder2.layers.0.linear1.weight\", \"transformer.encoder2.layers.0.linear1.bias\", \"transformer.encoder2.layers.0.linear2.weight\", \"transformer.encoder2.layers.0.linear2.bias\", \"transformer.encoder2.layers.0.norm1.weight\", \"transformer.encoder2.layers.0.norm1.bias\", \"transformer.encoder2.layers.0.norm2.weight\", \"transformer.encoder2.layers.0.norm2.bias\", \"transformer.encoder2.layers.1.self_attn.in_proj_weight\", \"transformer.encoder2.layers.1.self_attn.in_proj_bias\", \"transformer.encoder2.layers.1.self_attn.out_proj.weight\", \"transformer.encoder2.layers.1.self_attn.out_proj.bias\", \"transformer.encoder2.layers.1.linear1.weight\", \"transformer.encoder2.layers.1.linear1.bias\", \"transformer.encoder2.layers.1.linear2.weight\", \"transformer.encoder2.layers.1.linear2.bias\", \"transformer.encoder2.layers.1.norm1.weight\", \"transformer.encoder2.layers.1.norm1.bias\", \"transformer.encoder2.layers.1.norm2.weight\", \"transformer.encoder2.layers.1.norm2.bias\", \"transformer.encoder2.layers.2.self_attn.in_proj_weight\", \"transformer.encoder2.layers.2.self_attn.in_proj_bias\", \"transformer.encoder2.layers.2.self_attn.out_proj.weight\", \"transformer.encoder2.layers.2.self_attn.out_proj.bias\", \"transformer.encoder2.layers.2.linear1.weight\", \"transformer.encoder2.layers.2.linear1.bias\", \"transformer.encoder2.layers.2.linear2.weight\", \"transformer.encoder2.layers.2.linear2.bias\", \"transformer.encoder2.layers.2.norm1.weight\", \"transformer.encoder2.layers.2.norm1.bias\", \"transformer.encoder2.layers.2.norm2.weight\", \"transformer.encoder2.layers.2.norm2.bias\", \"transformer.encoder2.norm.weight\", \"transformer.encoder2.norm.bias\", \"transformer.decoder.layers.0.norm4.weight\", \"transformer.decoder.layers.0.norm4.bias\", \"transformer.decoder.layers.1.norm4.weight\", \"transformer.decoder.layers.1.norm4.bias\", \"transformer.decoder.layers.2.norm4.weight\", \"transformer.decoder.layers.2.norm4.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29805/3485708772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/500_checkpoint.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_29805/379804629.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint, model, optimizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> Loading checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer:\n\tMissing key(s) in state_dict: \"src2_word_embedding.weight\", \"src2_position_embedding.weight\", \"transformer.encoder2.layers.0.self_attn.in_proj_weight\", \"transformer.encoder2.layers.0.self_attn.in_proj_bias\", \"transformer.encoder2.layers.0.self_attn.out_proj.weight\", \"transformer.encoder2.layers.0.self_attn.out_proj.bias\", \"transformer.encoder2.layers.0.linear1.weight\", \"transformer.encoder2.layers.0.linear1.bias\", \"transformer.encoder2.layers.0.linear2.weight\", \"transformer.encoder2.layers.0.linear2.bias\", \"transformer.encoder2.layers.0.norm1.weight\", \"transformer.encoder2.layers.0.norm1.bias\", \"transformer.encoder2.layers.0.norm2.weight\", \"transformer.encoder2.layers.0.norm2.bias\", \"transformer.encoder2.layers.1.self_attn.in_proj_weight\", \"transformer.encoder2.layers.1.self_attn.in_proj_bias\", \"transformer.encoder2.layers.1.self_attn.out_proj.weight\", \"transformer.encoder2.layers.1.self_attn.out_proj.bias\", \"transformer.encoder2.layers.1.linear1.weight\", \"transformer.encoder2.layers.1.linear1.bias\", \"transformer.encoder2.layers.1.linear2.weight\", \"transformer.encoder2.layers.1.linear2.bias\", \"transformer.encoder2.layers.1.norm1.weight\", \"transformer.encoder2.layers.1.norm1.bias\", \"transformer.encoder2.layers.1.norm2.weight\", \"transformer.encoder2.layers.1.norm2.bias\", \"transformer.encoder2.layers.2.self_attn.in_proj_weight\", \"transformer.encoder2.layers.2.self_attn.in_proj_bias\", \"transformer.encoder2.layers.2.self_attn.out_proj.weight\", \"transformer.encoder2.layers.2.self_attn.out_proj.bias\", \"transformer.encoder2.layers.2.linear1.weight\", \"transformer.encoder2.layers.2.linear1.bias\", \"transformer.encoder2.layers.2.linear2.weight\", \"transformer.encoder2.layers.2.linear2.bias\", \"transformer.encoder2.layers.2.norm1.weight\", \"transformer.encoder2.layers.2.norm1.bias\", \"transformer.encoder2.layers.2.norm2.weight\", \"transformer.encoder2.layers.2.norm2.bias\", \"transformer.encoder2.norm.weight\", \"transformer.encoder2.norm.bias\", \"transformer.decoder.layers.0.norm4.weight\", \"transformer.decoder.layers.0.norm4.bias\", \"transformer.decoder.layers.1.norm4.weight\", \"transformer.decoder.layers.1.norm4.bias\", \"transformer.decoder.layers.2.norm4.weight\", \"transformer.decoder.layers.2.norm4.bias\". "
     ]
    }
   ],
   "source": [
    "state = torch.load(destination_folder + '/500_checkpoint.pt', map_location=device)\n",
    "load_checkpoint(state, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "501467c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266468.95956058794\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "print(math.exp(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33d25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs = folder +  \"/generated_samples_temp\"\n",
    "Path(generated_outputs+\"/intro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/outro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/solo\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/predict\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intro = pd.read_csv(source_folder + '/test_torchtext.csv')\n",
    "test_intro = df_intro['intro'].values\n",
    "test_solo = df_intro['solo'].values\n",
    "test_outro = df_intro['outro'].values\n",
    "test_data=[]\n",
    "for i in range(len(test_intro)):\n",
    "    temp_dict = {}\n",
    "    temp_dict['intro'] = test_intro[i]\n",
    "    temp_dict['solo'] = test_solo[i]\n",
    "    temp_dict['outro'] = test_outro[i]\n",
    "    test_data.append(temp_dict)\n",
    "print(len(test_intro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caada925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(test_intro)):\n",
    "#     if len(test_intro) > 1200:\n",
    "#         continue\n",
    "    intro = test_intro[i]\n",
    "    solo = test_solo[i]\n",
    "    outro = test_outro[i]\n",
    "    #print(intro)\n",
    "    #print(outro)\n",
    "    list_intro = [int(x) for x in intro.split(' ')]\n",
    "    list_solo = [int(x) for x in solo.split(' ')]\n",
    "    list_outro = [int(x) for x in outro.split(' ')]\n",
    "    #print(list_sentence)\n",
    "    translated_sentence = translate_sentence(model, intro, outro, intro_field, outro_field, solo_field, device, max_length=1200)\n",
    "    #print(translated_sentence)\n",
    "    translated_sentence = [int(x) for x in translated_sentence if x != '<pad>' and x != '<sos>' and x != '<eos>' and x != '<unk>']\n",
    "    print(translated_sentence)\n",
    "    utils.write_midi(list_intro, word2event, generated_outputs + \"/intro/\" + \"/intro\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(list_solo, word2event, generated_outputs  + \"/solo/\" + \"/solo\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(list_outro, word2event, generated_outputs + \"/outro/\" + \"/outro\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(translated_sentence, word2event, generated_outputs + \"/predict/\" + \"/predict\" + str(i)  + \".mid\")\n",
    "    print(i)\n",
    "#     if i == 10:\n",
    "#         break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b85e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "for i in range(11):\n",
    "    intro = mido.MidiFile(generated_outputs + \"/intro/\" + '/intro' + str(i) + '.mid')\n",
    "    solo = mido.MidiFile(generated_outputs + \"/solo/\" +'/solo' + str(i) + '.mid')\n",
    "    outro = mido.MidiFile(generated_outputs + \"/outro/\" +'/outro' + str(i) + '.mid')\n",
    "    predict = mido.MidiFile(generated_outputs + \"/predict/\" +'/predict' + str(i) + '.mid')\n",
    "    total_intro_time = 0\n",
    "    total_solo_time = 0\n",
    "    total_predict_time = 0\n",
    "    for msg in intro.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_intro_time += msg.time\n",
    "    for msg in solo.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_solo_time += msg.time\n",
    "    for msg in predict.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_predict_time += msg.time\n",
    "            \n",
    "    original_outro_time = 0 + outro.tracks[1][1].time\n",
    "    \n",
    "    print(original_outro_time + total_solo_time + total_intro_time)\n",
    "    solo.tracks[1][1].time += total_intro_time\n",
    "    outro.tracks[1][1].time = original_outro_time + total_solo_time + total_intro_time\n",
    "    print(outro.tracks[1][1].time)\n",
    "    intro.tracks[1].name = \"intro\"\n",
    "    solo.tracks[1].name = \"solo\"\n",
    "    outro.tracks[1].name = \"outro\"\n",
    "    predict.tracks[1].name = \"predict\"\n",
    "    merged_mid = mido.MidiFile()\n",
    "    merged_mid.ticks_per_beat = intro.ticks_per_beat\n",
    "    merged_mid.tracks = intro.tracks + solo.tracks + outro.tracks\n",
    "    merged_mid.save(generated_outputs + '/merged' + str(i) + '.mid')\n",
    "    \n",
    "    \n",
    "    outro = mido.MidiFile(generated_outputs + \"/outro/\" +'/outro' + str(i) + '.mid')\n",
    "    \n",
    "    print(original_outro_time + total_predict_time + total_intro_time)\n",
    "    predict.tracks[1][1].time += total_intro_time\n",
    "    outro.tracks[1][1].time = original_outro_time + total_predict_time + total_intro_time\n",
    "    print(outro.tracks[1][1].time)\n",
    "    merged_mid = mido.MidiFile()\n",
    "    merged_mid.ticks_per_beat = intro.ticks_per_beat\n",
    "    merged_mid.tracks = intro.tracks + predict.tracks + outro.tracks\n",
    "    merged_mid.save(generated_outputs + '/merged_predict' + str(i) + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29e021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dissimilar_interpolation\n",
    "for i in range(0,len(test_intro)):\n",
    "#     if len(test_intro) > 1200:\n",
    "#         continue\n",
    "    intro = test_intro[i]\n",
    "    #solo = test_solo[i]\n",
    "    if i + 3 < (len(test_intro)):\n",
    "        outro = test_outro[i+3]\n",
    "    else:\n",
    "        outro = test_outro[i]\n",
    "    #print(intro)\n",
    "    #print(outro)\n",
    "    list_intro = [int(x) for x in intro.split(' ')]\n",
    "    #list_solo = [int(x) for x in solo.split(' ')]\n",
    "    list_outro = [int(x) for x in outro.split(' ')]\n",
    "    #print(list_sentence)\n",
    "    translated_sentence = translate_sentence(model, intro, outro, intro_field, outro_field, solo_field, device, max_length=1200)\n",
    "    #print(translated_sentence)\n",
    "    translated_sentence = [int(x) for x in translated_sentence if x != '<pad>' and x != '<sos>' and x != '<eos>' and x != '<unk>']\n",
    "    print(translated_sentence)\n",
    "    utils.write_midi(list_intro, word2event, dissimilar_interpolation + \"/intro/\" + \"/intro\" + str(i)  + \".mid\")\n",
    "    #utils.write_midi(list_solo, word2event, generated_outputs  + \"/solo/\" + \"/solo\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(list_outro, word2event, dissimilar_interpolation + \"/outro/\" + \"/outro\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(translated_sentence, word2event, dissimilar_interpolation + \"/predict/\" + \"/predict\" + str(i)  + \".mid\")\n",
    "    print(i)\n",
    "#     if i == 10:\n",
    "#         break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "for i in range(len(test_intro)):\n",
    "    intro = mido.MidiFile(dissimilar_interpolation + \"/intro/\" + '/intro' + str(i) + '.mid')\n",
    "    outro = mido.MidiFile(dissimilar_interpolation + \"/outro/\" +'/outro' + str(i) + '.mid')\n",
    "    predict = mido.MidiFile(dissimilar_interpolation + \"/predict/\" +'/predict' + str(i) + '.mid')\n",
    "    total_intro_time = 0\n",
    "    total_solo_time = 0\n",
    "    total_predict_time = 0\n",
    "    for msg in intro.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_intro_time += msg.time\n",
    "    for msg in predict.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_predict_time += msg.time\n",
    "            \n",
    "    original_outro_time = 0 + outro.tracks[1][1].time\n",
    "    \n",
    "    print(original_outro_time + total_predict_time + total_intro_time)\n",
    "    predict.tracks[1][1].time += total_intro_time\n",
    "    outro.tracks[1][1].time = original_outro_time + total_predict_time + total_intro_time\n",
    "    print(outro.tracks[1][1].time)\n",
    "    merged_mid = mido.MidiFile()\n",
    "    merged_mid.ticks_per_beat = intro.ticks_per_beat\n",
    "    merged_mid.tracks = intro.tracks + predict.tracks + outro.tracks\n",
    "    merged_mid.save(dissimilar_interpolation + '/merged_predict' + str(i) + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, prev_node, wid, logp, length):\n",
    "        self.prev_node = prev_node\n",
    "        self.wid = wid\n",
    "        self.logp = logp\n",
    "        self.length = length\n",
    "\n",
    "    def eval(self):\n",
    "        return self.logp / float(self.length - 1 + 1e-6)\n",
    "# }}}\n",
    "import copy\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2930460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_beam(model, sentence, german, english, device, max_length=1200,beam_width=2,max_dec_steps=25000):\n",
    "    \n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    eos_token = english.vocab.stoi[\"<eos>\"]\n",
    "    sos_token = english.vocab.stoi[\"<sos>\"]\n",
    "    \n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    n_best_list = []\n",
    "    \n",
    "     \n",
    "    #trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "    #first token as input\n",
    "    trg_tensor = torch.LongTensor(outputs).to(device)\n",
    "    \n",
    "    end_nodes = []\n",
    "\n",
    "    #starting node\n",
    "    node = BeamSearchNode(prev_node=None, wid=trg_tensor, logp=0, length=1)\n",
    "\n",
    "    nodes = []\n",
    "\n",
    "    heappush(nodes, (-node.eval(), id(node), node))\n",
    "    n_dec_steps = 0\n",
    "\n",
    "    while True:\n",
    "        # Give up when decoding takes too long\n",
    "        if n_dec_steps > max_dec_steps:\n",
    "            break\n",
    "        \n",
    "        # Fetch the best node\n",
    "        #print([n[2].wid for n in nodes])\n",
    "        score, _, n = heappop(nodes)\n",
    "        decoder_input = n.wid\n",
    "        \n",
    "        if n.wid.item() == eos_token and n.prev_node is not None:\n",
    "            end_nodes.append((score, id(n), n))\n",
    "            # If we reached maximum # of sentences required\n",
    "            if len(end_nodes) >= beam_width:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "   \n",
    "        sequence = [n.wid.item()]\n",
    "        a = n\n",
    "        while a.prev_node is not None:\n",
    "            a = a.prev_node\n",
    "            sequence.append(a.wid.item())\n",
    "        sequence = sequence[::-1] # reverse\n",
    "        \n",
    "        #print(sequence)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, torch.LongTensor(sequence).unsqueeze(1).to(device))\n",
    "        \n",
    "        # Get top-k from this decoded result\n",
    "        topk_log_prob, topk_indexes = torch.topk(output, beam_width)\n",
    "        #print(topk_indexes)\n",
    "        #print(topk_log_prob)\n",
    "        # Then, register new top-k nodes\n",
    "        for new_k in range(beam_width):\n",
    "            decoded_t = topk_indexes[0][0][new_k].view(1) # (1)\n",
    "            logp = topk_log_prob[0][0][new_k].item() # float log probability val\n",
    "\n",
    "            node = BeamSearchNode(prev_node=n,\n",
    "                                  wid=decoded_t,\n",
    "                                  logp=n.logp+logp,\n",
    "                                  length=n.length+1)\n",
    "            heappush(nodes, (-node.eval(), id(node), node))\n",
    "        n_dec_steps += beam_width\n",
    "        #print(n_dec_steps)\n",
    "    # if there are no end_nodes, retrieve best nodes (they are probably truncated)\n",
    "    if len(end_nodes) == 0:\n",
    "        end_nodes = [heappop(nodes) for _ in range(beam_width)]\n",
    "\n",
    "    # Construct sequences from end_nodes\n",
    "    n_best_seq_list = []\n",
    "    for score, _id, n in sorted(end_nodes, key=lambda x: x[0]):\n",
    "        sequence = [n.wid.item()]\n",
    "        # back trace from end node\n",
    "        while n.prev_node is not None:\n",
    "            n = n.prev_node\n",
    "            sequence.append(n.wid.item())\n",
    "        sequence = sequence[::-1] # reverse\n",
    "\n",
    "        n_best_seq_list.append(sequence)\n",
    "\n",
    "\n",
    "    # return n_best_seq_list\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in n_best_seq_list[0]]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(vocab, path):\n",
    "    output = open(path, 'wb')\n",
    "    pickle.dump(vocab, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(intro_field.vocab, vocab + '/intro_vocab.pkl')\n",
    "save_vocab(solo_field.vocab, vocab + '/solo_vocab.pkl')\n",
    "save_vocab(outro_field.vocab, vocab + '/outro_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_translate_sentence(model, sentence, german, english, device, max_length=1200):\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    #tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    #tokens.insert(0, german.init_token)\n",
    "    #tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    #text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(sentence).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e97881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    print(len(data))\n",
    "    for example in data:\n",
    "        #print( vars(example))\n",
    "        src = vars(example)[\"intro\"]\n",
    "        trg = vars(example)[\"solo\"]\n",
    "        \n",
    "        src = [int(x) for x in src]\n",
    "        trg = [int(x) for x in trg]\n",
    "        \n",
    "        if len(trg) > 1200 or len(src) > 1200:\n",
    "            continue\n",
    "        \n",
    "        prediction = bleu_translate_sentence(model, src, german, english, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append(trg)\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on entire test data takes a while\n",
    "score = bleu(test[1:10], model, intro_field, solo_field, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
