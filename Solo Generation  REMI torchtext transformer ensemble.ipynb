{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022889aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator,ReversibleField\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import remi_utils as utils\n",
    "import twoencodertransformer as kk\n",
    "import pickle\n",
    "source_folder = \"solo_generation_dataset_fixed_augmented\"\n",
    "folder = \"dynamic_augmented_models/ensemble\"\n",
    "destination_folder = folder + \"/solo_generation_weights\"\n",
    "weights_intro = \"fixed_augmented_models/intro/solo_generation_weights\"\n",
    "weights_outro = \"fixed_augmented_models/outro/solo_generation_weights\"\n",
    "generated_outputs = folder +  \"/generated_samples\"\n",
    "dissimilar_interpolation = folder + \"/interpolation\"\n",
    "vocab = folder + \"/vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2e5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(destination_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs).mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation).mkdir(parents=True, exist_ok=True)\n",
    "Path(vocab).mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/intro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/outro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/solo\").mkdir(parents=True, exist_ok=True)\n",
    "Path(generated_outputs+\"/predict\").mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation+\"/intro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation+\"/outro\").mkdir(parents=True, exist_ok=True)\n",
    "Path(dissimilar_interpolation+\"/predict\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "event2word, word2event = pickle.load(open('dictionary_fixed.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193a1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:1\" \n",
    "else:  \n",
    "    dev = \"cpu\" \n",
    "print(dev)\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc28e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields\n",
    "\n",
    "intro_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "intro_piano_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "outro_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "outro_piano_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "solo_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "solo_piano_field = Field(tokenize=None, lower=True, include_lengths=True, batch_first=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "fields = [('intro', intro_field), ('intro_piano', intro_piano_field), \\\n",
    "          ('outro', outro_field), ('outro_piano', outro_piano_field), \\\n",
    "          ('solo', solo_field), ('solo_piano', solo_piano_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='train_torchtext.csv', validation='val_torchtext.csv', test='test_torchtext.csv',\n",
    "                                           format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "# Iterators\n",
    "BATCH_SIZE = 1\n",
    "train_iter = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.intro),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.intro),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "test_iter = BucketIterator(test, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.intro),\n",
    "                            device=device, sort=False, sort_within_batch=True)\n",
    "\n",
    "# Vocabulary\n",
    "\n",
    "intro_field.build_vocab(train, min_freq=1)\n",
    "intro_piano_field.build_vocab(train, min_freq=1)\n",
    "outro_field.build_vocab(train, min_freq=1)\n",
    "outro_piano_field.build_vocab(train, min_freq=1)\n",
    "solo_field.build_vocab(train, min_freq=1)\n",
    "solo_piano_field.build_vocab(train, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300385ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([414, 1])\n",
      "torch.Size([160, 1])\n",
      "torch.Size([429, 1])\n",
      "torch.Size([151, 1])\n",
      "torch.Size([325, 1])\n",
      "torch.Size([90, 1])\n",
      "torch.Size([141, 1])\n",
      "torch.Size([233, 1])\n",
      "torch.Size([237, 1])\n",
      "torch.Size([212, 1])\n",
      "torch.Size([194, 1])\n",
      "torch.Size([205, 1])\n",
      "torch.Size([241, 1])\n",
      "torch.Size([301, 1])\n",
      "torch.Size([366, 1])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([179, 1])\n",
      "torch.Size([153, 1])\n",
      "torch.Size([362, 1])\n",
      "torch.Size([265, 1])\n",
      "torch.Size([351, 1])\n",
      "torch.Size([217, 1])\n",
      "torch.Size([87, 1])\n",
      "torch.Size([175, 1])\n",
      "torch.Size([98, 1])\n",
      "torch.Size([156, 1])\n",
      "torch.Size([179, 1])\n",
      "torch.Size([253, 1])\n",
      "torch.Size([319, 1])\n",
      "torch.Size([285, 1])\n",
      "torch.Size([353, 1])\n",
      "torch.Size([257, 1])\n",
      "torch.Size([170, 1])\n",
      "torch.Size([522, 1])\n",
      "torch.Size([69, 1])\n",
      "torch.Size([294, 1])\n",
      "torch.Size([353, 1])\n",
      "torch.Size([275, 1])\n",
      "torch.Size([150, 1])\n",
      "torch.Size([314, 1])\n",
      "torch.Size([272, 1])\n",
      "torch.Size([433, 1])\n",
      "torch.Size([444, 1])\n",
      "torch.Size([260, 1])\n",
      "torch.Size([185, 1])\n",
      "torch.Size([335, 1])\n",
      "torch.Size([173, 1])\n",
      "torch.Size([211, 1])\n",
      "torch.Size([82, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([358, 1])\n",
      "torch.Size([115, 1])\n",
      "torch.Size([279, 1])\n",
      "torch.Size([133, 1])\n",
      "torch.Size([301, 1])\n",
      "torch.Size([303, 1])\n",
      "torch.Size([153, 1])\n",
      "torch.Size([166, 1])\n",
      "torch.Size([240, 1])\n",
      "torch.Size([148, 1])\n",
      "torch.Size([237, 1])\n",
      "torch.Size([225, 1])\n",
      "torch.Size([509, 1])\n",
      "torch.Size([151, 1])\n",
      "torch.Size([266, 1])\n",
      "torch.Size([210, 1])\n",
      "torch.Size([269, 1])\n",
      "torch.Size([195, 1])\n",
      "torch.Size([270, 1])\n",
      "torch.Size([331, 1])\n",
      "torch.Size([71, 1])\n",
      "torch.Size([209, 1])\n",
      "torch.Size([213, 1])\n",
      "torch.Size([250, 1])\n",
      "torch.Size([233, 1])\n",
      "torch.Size([139, 1])\n",
      "torch.Size([253, 1])\n",
      "torch.Size([96, 1])\n",
      "torch.Size([182, 1])\n",
      "torch.Size([177, 1])\n",
      "torch.Size([133, 1])\n",
      "torch.Size([365, 1])\n",
      "torch.Size([198, 1])\n",
      "torch.Size([170, 1])\n",
      "torch.Size([489, 1])\n",
      "torch.Size([159, 1])\n",
      "torch.Size([326, 1])\n",
      "torch.Size([156, 1])\n",
      "torch.Size([265, 1])\n",
      "torch.Size([281, 1])\n",
      "torch.Size([321, 1])\n",
      "torch.Size([439, 1])\n",
      "torch.Size([186, 1])\n",
      "torch.Size([289, 1])\n",
      "torch.Size([173, 1])\n",
      "torch.Size([619, 1])\n",
      "torch.Size([73, 1])\n",
      "torch.Size([210, 1])\n",
      "torch.Size([139, 1])\n",
      "torch.Size([491, 1])\n",
      "torch.Size([204, 1])\n",
      "torch.Size([148, 1])\n",
      "torch.Size([473, 1])\n",
      "torch.Size([204, 1])\n",
      "torch.Size([114, 1])\n",
      "torch.Size([133, 1])\n",
      "torch.Size([281, 1])\n",
      "torch.Size([105, 1])\n",
      "torch.Size([217, 1])\n",
      "torch.Size([314, 1])\n",
      "torch.Size([177, 1])\n",
      "torch.Size([224, 1])\n"
     ]
    }
   ],
   "source": [
    "for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "     (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "     (solo, solo_len),(solo_piano, solo_piano_len)), _ in (test_iter):\n",
    "    print(solo.transpose(1,0).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82eec1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.enabled=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2906224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2794d9da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/seq2seq_transformer/seq2seq_transformer.py\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
    "        )\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e4ad3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_vocab_size = len(intro_field.vocab)\n",
    "trg_vocab_size = len(solo_field.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 1200\n",
    "forward_expansion = 4\n",
    "src_pad_idx = 1 #english.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# model = Transformer(\n",
    "#     embedding_size,\n",
    "#     src_vocab_size,\n",
    "#     trg_vocab_size,\n",
    "#     src_pad_idx,\n",
    "#     num_heads,\n",
    "#     num_encoder_layers,\n",
    "#     num_decoder_layers,\n",
    "#     forward_expansion,\n",
    "#     dropout,\n",
    "#     max_len,\n",
    "#     device,\n",
    "# )\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3910013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "# model.apply(init_weights)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "\n",
    "def save_best_checkpoint(state, nth,filename=\"_checkpoint.pt\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "#     torch.save(state, destination_folder + str(nth)+filename)\n",
    "    torch.save(state, destination_folder + '/metrics.pt')\n",
    "\n",
    "def save_final_checkpoint(state, nth,filename=\"_checkpoint.pt\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, destination_folder + \"/\" + str(nth)+filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43049c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoi input str get int\n",
    "# intro_field.vocab.stoi\n",
    "# itos input into get token/str\n",
    "# intro_field.vocab.itos[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d99045b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b08d03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #for _, (src, _,trg,_) in enumerate(iterator):\n",
    "    for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "     (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "     (solo, solo_len),(solo_piano, solo_piano_len)), _ in (iterator):\n",
    "        src, trg = intro.transpose(1,0), solo.transpose(1,0)\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:-1, :])\n",
    "        \n",
    "#         print(output.size())\n",
    "#         print(trg.size())\n",
    "        \n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        trg = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.cpu().detach().item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #for _, (src, _,trg,_) in enumerate(iterator):\n",
    "        for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "         (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "         (solo, solo_len),(solo_piano, solo_piano_len)), _ in (iterator):\n",
    "            src, trg = intro.transpose(1,0), solo.transpose(1,0)\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            src2 = outro.transpose(1,0)\n",
    "            src2 = src2.to(device)\n",
    "            output = model(src,src2, trg[:-1, :]) #turn off teacher forcing\n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.cpu().detach().item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_outro(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #for _, (src, _,trg,_) in enumerate(iterator):\n",
    "        for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "         (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "         (solo, solo_len),(solo_piano, solo_piano_len)), _ in (iterator):\n",
    "            src, trg = outro.transpose(1,0), solo.transpose(1,0)\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg[:-1, :]) #turn off teacher forcing\n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.cpu().detach().item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_intro(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #for _, (src, _,trg,_) in enumerate(iterator):\n",
    "        for ((intro, intro_len), (intro_piano, intro_piano_len),\\\n",
    "         (outro, outro_len),(outro_piano, outro_piano_len),\\\n",
    "         (solo, solo_len),(solo_piano, solo_piano_len)), _ in (iterator):\n",
    "            src, trg = intro.transpose(1,0), solo.transpose(1,0)\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg[:-1, :]) #turn off teacher forcing\n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.cpu().detach().item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71aa22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, german, english, device, max_length=1200):\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    # print(outputs)\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9410c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "df_intro = pd.read_csv(source_folder + '/val_torchtext.csv')\n",
    "val_intro = df_intro['intro'].values\n",
    "val_solo = df_intro['solo'].values\n",
    "val_outro = df_intro['outro'].values\n",
    "val_data=[]\n",
    "for i in range(len(val_intro)):\n",
    "    temp_dict = {}\n",
    "    temp_dict['intro'] = val_intro[i]\n",
    "    temp_dict['solo'] = val_solo[i]\n",
    "    temp_dict['outro'] = val_outro[i]\n",
    "    val_data.append(temp_dict)\n",
    "print(len(val_intro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6724636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mode_collapse(model):\n",
    "    count = 0\n",
    "    translations = []\n",
    "    for i in range(5):\n",
    "        if len(val_intro) > 1200:\n",
    "            continue\n",
    "        intro = val_intro[i]\n",
    "        solo = val_solo[i]\n",
    "        outro = val_outro[i]\n",
    "        #print(intro)\n",
    "        list_intro = [int(x) for x in intro.split(' ')]\n",
    "        list_solo = [int(x) for x in solo.split(' ')]\n",
    "        list_outro = [int(x) for x in outro.split(' ')]\n",
    "        translated_sentence = translate_sentence(model, intro, intro_field, solo_field, device, max_length=1200)\n",
    "        \n",
    "        translated_sentence = [int(x) for x in translated_sentence if x != '<pad>' and x != '<sos>' and x != '<eos>' and x != '<unk>']\n",
    "        print(translated_sentence)\n",
    "        translations.append(translated_sentence)\n",
    "        if i > 0:\n",
    "            if translations[i-1] == translations[i]:\n",
    "                count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce0fa416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = {'model_state_dict': model.state_dict(),\n",
    "#                   'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                   'valid_loss': valid_loss}\n",
    "# save_checkpoint(destination_folder + checkpoint,N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33caff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_transformer = Transformer(\n",
    "    embedding_size,\n",
    "    len(intro_field.vocab),\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "outro_transformer = Transformer(\n",
    "    embedding_size,\n",
    "    len(outro_field.vocab),\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(outro_transformer.parameters(), lr=2e-4)\n",
    "optimizer = optim.Adam(intro_transformer.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "005d41b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_state_dict\n",
      "optimizer_state_dict\n",
      "valid_loss\n",
      "=> Loading checkpoint\n",
      "model_state_dict\n",
      "optimizer_state_dict\n",
      "valid_loss\n",
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "intro_transformer.apply(init_weights)\n",
    "\n",
    "state = torch.load(weights_intro + '/500_checkpoint.pt', map_location=device)\n",
    "for key in state:\n",
    "    print(key)\n",
    "load_checkpoint(state, intro_transformer, optimizer)\n",
    "\n",
    "outro_transformer.apply(init_weights)\n",
    "\n",
    "state1 = torch.load(weights_outro + '/500_checkpoint.pt', map_location=device)\n",
    "for key in state1:\n",
    "    print(key)\n",
    "load_checkpoint(state1, outro_transformer, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05aa022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsemble(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        \n",
    "    def forward(self, x1, x2, x3):\n",
    "        intro = self.modelA(x1,x3)\n",
    "        outro = self.modelB(x2,x3)\n",
    "        best_guess1=intro.argmax(2)[-1, :].item()\n",
    "        best_guess2=outro.argmax(2)[-1, :].item()\n",
    "        \n",
    "        if intro[0][0][best_guess1] >= outro[0][0][best_guess2]:\n",
    "            return intro\n",
    "        else:\n",
    "            return outro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17632ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910.4126908617222\n"
     ]
    }
   ],
   "source": [
    "ensemble = MyEnsemble(intro_transformer, outro_transformer)\n",
    "test_loss=evaluate(ensemble, test_iter, criterion)\n",
    "print(math.exp(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58b44e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78785909155.74995\n"
     ]
    }
   ],
   "source": [
    "print(math.exp(25.09))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e947ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317.618425251075\n",
      "3932.4725722951425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss = evaluate_intro(intro_transformer, test_iter, criterion)\n",
    "print(math.exp(test_loss))\n",
    "test_loss1 = evaluate_outro(outro_transformer, test_iter, criterion)\n",
    "print(math.exp(test_loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1f6ba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "df_intro = pd.read_csv(source_folder + '/test_torchtext.csv')\n",
    "test_intro = df_intro['intro'].values\n",
    "test_solo = df_intro['solo'].values\n",
    "test_outro = df_intro['outro'].values\n",
    "test_data=[]\n",
    "for i in range(len(test_intro)):\n",
    "    temp_dict = {}\n",
    "    temp_dict['intro'] = test_intro[i]\n",
    "    temp_dict['solo'] = test_solo[i]\n",
    "    temp_dict['outro'] = test_outro[i]\n",
    "    test_data.append(temp_dict)\n",
    "print(len(test_intro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c28b1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_ensemble(model, model2, sentence, sentence2, intro, outro, solo, device, max_length=1200):\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    # print(tokens)\n",
    "    tokens2 = [token.lower() for token in sentence2.split(' ')]\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, intro.init_token)\n",
    "    tokens.append(intro.eos_token)\n",
    "\n",
    "    tokens2.insert(0, outro.init_token)\n",
    "    tokens2.append(outro.eos_token)\n",
    "    \n",
    "    # Go through each german token and convert to an index\n",
    "    print(tokens)\n",
    "    text_to_indices = [intro.vocab.stoi[token] for token in tokens]\n",
    "    print(text_to_indices)\n",
    "    text_to_indices2 = [outro.vocab.stoi[token] for token in tokens2]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "    sentence2_tensor = torch.LongTensor(text_to_indices2).unsqueeze(1).to(device)\n",
    "    \n",
    "    outputs = [solo.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            \n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "            best_guess1 = output.argmax(2)[-1, :].item()\n",
    "            \n",
    "            output2 = model2(sentence2_tensor, trg_tensor)\n",
    "            best_guess2 = output2.argmax(2)[-1, :].item()\n",
    "            if output[0][0][best_guess1] >= output2[0][0][best_guess2]:\n",
    "                best_guess = best_guess1\n",
    "            else:\n",
    "                best_guess = best_guess2\n",
    "            outputs.append(best_guess)\n",
    "            \n",
    "        if best_guess == solo.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    # print(outputs)\n",
    "    translated_sentence = [solo.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caada925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 122 48 73 43 14 8 2 122 8 73 34 7 15 81 41 37 17 2 122 23 2 122 0 1 2 122 4 9 39 14 48 12 41 14 8 2 122 8 81 30 7 15 73 41 14 52 81 30 7 17 2 122 36 81 47 7 23 2 122 23 81 30 10 33 81 30 14 53 73 30 14 0 1 2 122 1 81 30 10 54 73 47 10 4 12 39 14 48 12 30 14 8 2 122 8 81 41 10 15 73 41 14 52 73 39 14 17 2 122 36 81 13 24 23 2 122 23 5 39 16 33 81 39 14 53 73 13 16 0 1 2 122 54 81 46 7 8 2 122 8 73 13 16 15 73 13 14 52 81 46 7 17 2 122 36 81 47 27 23 2 122 32 72 46 14 33 70 46 14 53 73 46 14 0 1 2 122 1 81 46 16 4 81 47 10 48 73 20 7 8 2 122 8 81 47 168\n",
      "['<sos>', '0', '1', '2', '122', '48', '73', '43', '14', '8', '2', '122', '8', '73', '34', '7', '15', '81', '41', '37', '17', '2', '122', '23', '2', '122', '0', '1', '2', '122', '4', '9', '39', '14', '48', '12', '41', '14', '8', '2', '122', '8', '81', '30', '7', '15', '73', '41', '14', '52', '81', '30', '7', '17', '2', '122', '36', '81', '47', '7', '23', '2', '122', '23', '81', '30', '10', '33', '81', '30', '14', '53', '73', '30', '14', '0', '1', '2', '122', '1', '81', '30', '10', '54', '73', '47', '10', '4', '12', '39', '14', '48', '12', '30', '14', '8', '2', '122', '8', '81', '41', '10', '15', '73', '41', '14', '52', '73', '39', '14', '17', '2', '122', '36', '81', '13', '24', '23', '2', '122', '23', '5', '39', '16', '33', '81', '39', '14', '53', '73', '13', '16', '0', '1', '2', '122', '54', '81', '46', '7', '8', '2', '122', '8', '73', '13', '16', '15', '73', '13', '14', '52', '81', '46', '7', '17', '2', '122', '36', '81', '47', '27', '23', '2', '122', '32', '72', '46', '14', '33', '70', '46', '14', '53', '73', '46', '14', '0', '1', '2', '122', '1', '81', '46', '16', '4', '81', '47', '10', '48', '73', '20', '7', '8', '2', '122', '8', '81', '47', '168', '<eos>']\n",
      "[2, 6, 10, 21, 132, 16, 19, 54, 8, 15, 21, 132, 15, 19, 43, 5, 32, 22, 37, 117, 20, 21, 132, 28, 21, 132, 6, 10, 21, 132, 33, 14, 36, 8, 16, 11, 37, 8, 15, 21, 132, 15, 22, 39, 5, 32, 19, 37, 8, 17, 22, 39, 5, 20, 21, 132, 24, 22, 49, 5, 28, 21, 132, 28, 22, 39, 4, 46, 22, 39, 8, 30, 19, 39, 8, 6, 10, 21, 132, 10, 22, 39, 4, 18, 19, 49, 4, 33, 11, 36, 8, 16, 11, 39, 8, 15, 21, 132, 15, 22, 37, 4, 32, 19, 37, 8, 17, 19, 36, 8, 20, 21, 132, 24, 22, 40, 50, 28, 21, 132, 28, 13, 36, 7, 46, 22, 36, 8, 30, 19, 40, 7, 6, 10, 21, 132, 18, 22, 42, 5, 15, 21, 132, 15, 19, 40, 7, 32, 19, 40, 8, 17, 22, 42, 5, 20, 21, 132, 24, 22, 49, 65, 28, 21, 132, 29, 61, 42, 8, 46, 51, 42, 8, 30, 19, 42, 8, 6, 10, 21, 132, 10, 22, 42, 7, 33, 22, 49, 4, 16, 19, 55, 5, 15, 21, 132, 15, 22, 49, 149, 3]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    intro = test_intro[i]\n",
    "    solo = test_solo[i]\n",
    "    outro = test_outro[i]\n",
    "    #print(intro)\n",
    "    list_intro = [int(x) for x in intro.split(' ')]\n",
    "    list_solo = [int(x) for x in solo.split(' ')]\n",
    "    list_outro = [int(x) for x in outro.split(' ')]\n",
    "    #print(list_sentence)\n",
    "    translated_sentence = translate_sentence_ensemble(intro_transformer, outro_transformer, intro, outro, intro_field, outro_field, solo_field, device, max_length=1200)\n",
    "    #print(translated_sentence)\n",
    "    translated_sentence = [int(x) for x in translated_sentence if x != '<pad>' and x != '<sos>' and x != '<eos>' and x != '<unk>']\n",
    "    #print(translated_sentence)\n",
    "    utils.write_midi(list_intro, word2event, generated_outputs + \"/intro/\" + \"/intro\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(list_solo, word2event, generated_outputs  + \"/solo/\" + \"/solo\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(list_outro, word2event, generated_outputs + \"/outro/\" + \"/outro\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(translated_sentence, word2event, generated_outputs + \"/predict/\" + \"/predict\" + str(i)  + \".mid\")\n",
    "    #print(i)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a43b85e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "4621\n",
      "4621\n",
      "40200\n",
      "40200\n",
      "36600\n",
      "36600\n",
      "27720\n",
      "27720\n",
      "23520\n",
      "23520\n",
      "53340\n",
      "53340\n",
      "67080\n",
      "67080\n",
      "25800\n",
      "25800\n",
      "22860\n",
      "22860\n",
      "20400\n",
      "20400\n",
      "25920\n",
      "25920\n",
      "30900\n",
      "30900\n",
      "25440\n",
      "25440\n",
      "31680\n",
      "31680\n",
      "29280\n",
      "29280\n",
      "30720\n",
      "30720\n",
      "30180\n",
      "30180\n",
      "61500\n",
      "61500\n",
      "56700\n",
      "56700\n",
      "31380\n",
      "31380\n",
      "32880\n",
      "32880\n",
      "32940\n",
      "32940\n",
      "26640\n",
      "26640\n",
      "29280\n",
      "29280\n",
      "30780\n",
      "30780\n",
      "22260\n",
      "22260\n",
      "26100\n",
      "26100\n",
      "28560\n",
      "28560\n",
      "27240\n",
      "27240\n",
      "60660\n",
      "60660\n",
      "68220\n",
      "68220\n",
      "23460\n",
      "23460\n",
      "18420\n",
      "18420\n",
      "30300\n",
      "30300\n",
      "30180\n",
      "30180\n",
      "30840\n",
      "30840\n",
      "24660\n",
      "24660\n",
      "73260\n",
      "73260\n",
      "52740\n",
      "52740\n",
      "42360\n",
      "42360\n",
      "37140\n",
      "37140\n",
      "23100\n",
      "23100\n",
      "28440\n",
      "28440\n",
      "32760\n",
      "32760\n",
      "33600\n",
      "33600\n",
      "30060\n",
      "30060\n",
      "30540\n",
      "30540\n",
      "30720\n",
      "30720\n",
      "42180\n",
      "42180\n",
      "31200\n",
      "31200\n",
      "30000\n",
      "30000\n",
      "38160\n",
      "38160\n",
      "43020\n",
      "43020\n",
      "35400\n",
      "35400\n",
      "35940\n",
      "35940\n",
      "35400\n",
      "35400\n",
      "30240\n",
      "30240\n",
      "15840\n",
      "15840\n",
      "20400\n",
      "20400\n",
      "28740\n",
      "28740\n",
      "30660\n",
      "30660\n",
      "28800\n",
      "28800\n",
      "31020\n",
      "31020\n",
      "15840\n",
      "15840\n",
      "20280\n",
      "20280\n",
      "30960\n",
      "30960\n",
      "32880\n",
      "32880\n",
      "67560\n",
      "67560\n",
      "46560\n",
      "46560\n",
      "16560\n",
      "16560\n",
      "24660\n",
      "24660\n",
      "28740\n",
      "28740\n",
      "30780\n",
      "30780\n",
      "61920\n",
      "61920\n",
      "52320\n",
      "52320\n",
      "30000\n",
      "30000\n",
      "24000\n",
      "24000\n",
      "32400\n",
      "32400\n",
      "35520\n",
      "35520\n",
      "27720\n",
      "27720\n",
      "30660\n",
      "30660\n",
      "25920\n",
      "25920\n",
      "28080\n",
      "28080\n",
      "34800\n",
      "34800\n",
      "28980\n",
      "28980\n",
      "19380\n",
      "19380\n",
      "22260\n",
      "22260\n",
      "21840\n",
      "21840\n",
      "24840\n",
      "24840\n",
      "16920\n",
      "16920\n",
      "22920\n",
      "22920\n",
      "30780\n",
      "30780\n",
      "31920\n",
      "31920\n",
      "27900\n",
      "27900\n",
      "29340\n",
      "29340\n",
      "31080\n",
      "31080\n",
      "28860\n",
      "28860\n",
      "22020\n",
      "22020\n",
      "27660\n",
      "27660\n",
      "23220\n",
      "23220\n",
      "27060\n",
      "27060\n",
      "29040\n",
      "29040\n",
      "32100\n",
      "32100\n",
      "27240\n",
      "27240\n",
      "27120\n",
      "27120\n",
      "30780\n",
      "30780\n",
      "30120\n",
      "30120\n",
      "30840\n",
      "30840\n",
      "28800\n",
      "28800\n",
      "23700\n",
      "23700\n",
      "26700\n",
      "26700\n",
      "25020\n",
      "25020\n",
      "27660\n",
      "27660\n",
      "23220\n",
      "23220\n",
      "27060\n",
      "27060\n",
      "26820\n",
      "26820\n",
      "28740\n",
      "28740\n",
      "30660\n",
      "30660\n",
      "29700\n",
      "29700\n",
      "20100\n",
      "20100\n",
      "28080\n",
      "28080\n",
      "23940\n",
      "23940\n",
      "27720\n",
      "27720\n",
      "29340\n",
      "29340\n",
      "25620\n",
      "25620\n",
      "21540\n",
      "21540\n",
      "25920\n",
      "25920\n",
      "16560\n",
      "16560\n",
      "12840\n",
      "12840\n",
      "31440\n",
      "31440\n",
      "31200\n",
      "31200\n",
      "36060\n",
      "36060\n",
      "30840\n",
      "30840\n",
      "33360\n",
      "33360\n",
      "28980\n",
      "28980\n",
      "33120\n",
      "33120\n",
      "31440\n",
      "31440\n",
      "29940\n",
      "29940\n",
      "26760\n",
      "26760\n",
      "34080\n",
      "34080\n",
      "32100\n",
      "32100\n",
      "16740\n",
      "16740\n",
      "19980\n",
      "19980\n",
      "16380\n",
      "16380\n",
      "24660\n",
      "24660\n",
      "35280\n",
      "35280\n",
      "32940\n",
      "32940\n",
      "16080\n",
      "16080\n",
      "22260\n",
      "22260\n",
      "60840\n",
      "60840\n",
      "47940\n",
      "47940\n",
      "33480\n",
      "33480\n",
      "28200\n",
      "28200\n",
      "33300\n",
      "33300\n",
      "36900\n",
      "36900\n",
      "32880\n",
      "32880\n",
      "31080\n",
      "31080\n",
      "47160\n",
      "47160\n",
      "41280\n",
      "41280\n",
      "18540\n",
      "18540\n",
      "17700\n",
      "17700\n",
      "16500\n",
      "16500\n",
      "23760\n",
      "23760\n",
      "30300\n",
      "30300\n",
      "25080\n",
      "25080\n",
      "27960\n",
      "27960\n",
      "25440\n",
      "25440\n",
      "16380\n",
      "16380\n",
      "16920\n",
      "16920\n",
      "62580\n",
      "62580\n",
      "59880\n",
      "59880\n",
      "16080\n",
      "16080\n",
      "23280\n",
      "23280\n",
      "31740\n",
      "31740\n",
      "31320\n",
      "31320\n",
      "39780\n",
      "39780\n",
      "36780\n",
      "36780\n",
      "26040\n",
      "26040\n",
      "27120\n",
      "27120\n",
      "30480\n",
      "30480\n",
      "30900\n",
      "30900\n",
      "36720\n",
      "36720\n",
      "36600\n",
      "36600\n",
      "30420\n",
      "30420\n",
      "30300\n",
      "30300\n",
      "31320\n",
      "31320\n",
      "29640\n",
      "29640\n",
      "45120\n",
      "45120\n",
      "39360\n",
      "39360\n",
      "27600\n",
      "27600\n",
      "25680\n",
      "25680\n",
      "33360\n",
      "33360\n",
      "27360\n",
      "27360\n",
      "15960\n",
      "15960\n",
      "22920\n",
      "22920\n",
      "17820\n",
      "17820\n",
      "25920\n",
      "25920\n",
      "29580\n",
      "29580\n",
      "29100\n",
      "29100\n",
      "30300\n",
      "30300\n",
      "31440\n",
      "31440\n",
      "31740\n",
      "31740\n",
      "31020\n",
      "31020\n",
      "62640\n",
      "62640\n",
      "54300\n",
      "54300\n",
      "50100\n",
      "50100\n",
      "41880\n",
      "41880\n",
      "32040\n",
      "32040\n",
      "38640\n",
      "38640\n",
      "32400\n",
      "32400\n",
      "25740\n",
      "25740\n",
      "45000\n",
      "45000\n",
      "47400\n",
      "47400\n",
      "36480\n",
      "36480\n",
      "36480\n",
      "36480\n",
      "61440\n",
      "61440\n",
      "62640\n",
      "62640\n",
      "36600\n",
      "36600\n",
      "26220\n",
      "26220\n",
      "16260\n",
      "16260\n",
      "21540\n",
      "21540\n",
      "23280\n",
      "23280\n",
      "30720\n",
      "30720\n"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "for i in range(len(test_intro)):\n",
    "    intro = mido.MidiFile(generated_outputs + \"/intro/\" + '/intro' + str(i) + '.mid')\n",
    "    solo = mido.MidiFile(generated_outputs + \"/solo/\" +'/solo' + str(i) + '.mid')\n",
    "    outro = mido.MidiFile(generated_outputs + \"/outro/\" +'/outro' + str(i) + '.mid')\n",
    "    predict = mido.MidiFile(generated_outputs + \"/predict/\" +'/predict' + str(i) + '.mid')\n",
    "    total_intro_time = 0\n",
    "    total_solo_time = 0\n",
    "    total_predict_time = 0\n",
    "    for msg in intro.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_intro_time += msg.time\n",
    "    for msg in solo.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_solo_time += msg.time\n",
    "    for msg in predict.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_predict_time += msg.time\n",
    "            \n",
    "    original_outro_time = 0 + outro.tracks[1][1].time\n",
    "    \n",
    "    print(original_outro_time + total_solo_time + total_intro_time)\n",
    "    solo.tracks[1][1].time += total_intro_time\n",
    "    outro.tracks[1][1].time = original_outro_time + total_solo_time + total_intro_time\n",
    "    print(outro.tracks[1][1].time)\n",
    "    intro.tracks[1].name = \"intro\"\n",
    "    solo.tracks[1].name = \"solo\"\n",
    "    outro.tracks[1].name = \"outro\"\n",
    "    predict.tracks[1].name = \"predict\"\n",
    "    merged_mid = mido.MidiFile()\n",
    "    merged_mid.ticks_per_beat = intro.ticks_per_beat\n",
    "    merged_mid.tracks = intro.tracks + solo.tracks + outro.tracks\n",
    "    merged_mid.save(generated_outputs + '/merged' + str(i) + '.mid')\n",
    "    \n",
    "    \n",
    "    outro = mido.MidiFile(generated_outputs + \"/outro/\" +'/outro' + str(i) + '.mid')\n",
    "    \n",
    "    print(original_outro_time + total_predict_time + total_intro_time)\n",
    "    predict.tracks[1][1].time += total_intro_time\n",
    "    outro.tracks[1][1].time = original_outro_time + total_predict_time + total_intro_time\n",
    "    print(outro.tracks[1][1].time)\n",
    "    merged_mid = mido.MidiFile()\n",
    "    merged_mid.ticks_per_beat = intro.ticks_per_beat\n",
    "    merged_mid.tracks = intro.tracks + predict.tracks + outro.tracks\n",
    "    merged_mid.save(generated_outputs + '/merged_predict' + str(i) + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b52fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '0', '1', '2', '122', '48', '73', '43', '14', '8', '2', '122', '8', '73', '34', '7', '15', '81', '41', '37', '17', '2', '122', '23', '2', '122', '0', '1', '2', '122', '4', '9', '39', '14', '48', '12', '41', '14', '8', '2', '122', '8', '81', '30', '7', '15', '73', '41', '14', '52', '81', '30', '7', '17', '2', '122', '36', '81', '47', '7', '23', '2', '122', '23', '81', '30', '10', '33', '81', '30', '14', '53', '73', '30', '14', '0', '1', '2', '122', '1', '81', '30', '10', '54', '73', '47', '10', '4', '12', '39', '14', '48', '12', '30', '14', '8', '2', '122', '8', '81', '41', '10', '15', '73', '41', '14', '52', '73', '39', '14', '17', '2', '122', '36', '81', '13', '24', '23', '2', '122', '23', '5', '39', '16', '33', '81', '39', '14', '53', '73', '13', '16', '0', '1', '2', '122', '54', '81', '46', '7', '8', '2', '122', '8', '73', '13', '16', '15', '73', '13', '14', '52', '81', '46', '7', '17', '2', '122', '36', '81', '47', '27', '23', '2', '122', '32', '72', '46', '14', '33', '70', '46', '14', '53', '73', '46', '14', '0', '1', '2', '122', '1', '81', '46', '16', '4', '81', '47', '10', '48', '73', '20', '7', '8', '2', '122', '8', '81', '47', '168', '<eos>']\n",
      "[2, 6, 10, 21, 132, 16, 19, 54, 8, 15, 21, 132, 15, 19, 43, 5, 32, 22, 37, 117, 20, 21, 132, 28, 21, 132, 6, 10, 21, 132, 33, 14, 36, 8, 16, 11, 37, 8, 15, 21, 132, 15, 22, 39, 5, 32, 19, 37, 8, 17, 22, 39, 5, 20, 21, 132, 24, 22, 49, 5, 28, 21, 132, 28, 22, 39, 4, 46, 22, 39, 8, 30, 19, 39, 8, 6, 10, 21, 132, 10, 22, 39, 4, 18, 19, 49, 4, 33, 11, 36, 8, 16, 11, 39, 8, 15, 21, 132, 15, 22, 37, 4, 32, 19, 37, 8, 17, 19, 36, 8, 20, 21, 132, 24, 22, 40, 50, 28, 21, 132, 28, 13, 36, 7, 46, 22, 36, 8, 30, 19, 40, 7, 6, 10, 21, 132, 18, 22, 42, 5, 15, 21, 132, 15, 19, 40, 7, 32, 19, 40, 8, 17, 22, 42, 5, 20, 21, 132, 24, 22, 49, 65, 28, 21, 132, 29, 61, 42, 8, 46, 51, 42, 8, 30, 19, 42, 8, 6, 10, 21, 132, 10, 22, 42, 7, 33, 22, 49, 4, 16, 19, 55, 5, 15, 21, 132, 15, 22, 49, 149, 3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3854/3152214274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlist_outro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#print(list_sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtranslated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintro_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutro_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintro_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutro_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolo_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m#print(translated_sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtranslated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslated_sentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<pad>'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<sos>'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<eos>'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3854/2928682283.py\u001b[0m in \u001b[0;36mtranslate_sentence_ensemble\u001b[0;34m(model, model2, sentence, sentence2, intro, outro, solo, device, max_length)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mbest_guess1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence2_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dissimilar_interpolation\n",
    "for i in range(0,len(test_intro)):\n",
    "#     if len(test_intro) > 1200:\n",
    "#         continue\n",
    "    intro = test_intro[i]\n",
    "    #solo = test_solo[i]\n",
    "    if i + 3 < (len(test_intro)):\n",
    "        outro = test_outro[i+3]\n",
    "    else:\n",
    "        outro = test_outro[i]\n",
    "    #print(intro)\n",
    "    #print(outro)\n",
    "    list_intro = [int(x) for x in intro.split(' ')]\n",
    "    #list_solo = [int(x) for x in solo.split(' ')]\n",
    "    list_outro = [int(x) for x in outro.split(' ')]\n",
    "    #print(list_sentence)\n",
    "    translated_sentence = translate_sentence_ensemble(intro_transformer, outro_transformer, intro, outro, intro_field, outro_field, solo_field, device, max_length=1200)\n",
    "    #print(translated_sentence)\n",
    "    translated_sentence = [int(x) for x in translated_sentence if x != '<pad>' and x != '<sos>' and x != '<eos>' and x != '<unk>']\n",
    "    print(translated_sentence)\n",
    "    utils.write_midi(list_intro, word2event, dissimilar_interpolation + \"/intro/\" + \"/intro\" + str(i)  + \".mid\")\n",
    "    #utils.write_midi(list_solo, word2event, generated_outputs  + \"/solo/\" + \"/solo\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(list_outro, word2event, dissimilar_interpolation + \"/outro/\" + \"/outro\" + str(i)  + \".mid\")\n",
    "    utils.write_midi(translated_sentence, word2event, dissimilar_interpolation + \"/predict/\" + \"/predict\" + str(i)  + \".mid\")\n",
    "    print(i)\n",
    "#     if i == 10:\n",
    "#         break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cb917c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fixed_models/ensemble/interpolation/intro//intro0.mid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23450/3870658820.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_intro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mintro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMidiFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdissimilar_interpolation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/intro/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/intro'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.mid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMidiFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdissimilar_interpolation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/outro/\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/outro'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.mid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMidiFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdissimilar_interpolation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/predict/\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/predict'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.mid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/mido/midifiles/midifiles.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, file, type, ticks_per_beat, charset, debug, clip, tracks)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fixed_models/ensemble/interpolation/intro//intro0.mid'"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "for i in range(len(test_intro)):\n",
    "    intro = mido.MidiFile(dissimilar_interpolation + \"/intro/\" + '/intro' + str(i) + '.mid')\n",
    "    outro = mido.MidiFile(dissimilar_interpolation + \"/outro/\" +'/outro' + str(i) + '.mid')\n",
    "    predict = mido.MidiFile(dissimilar_interpolation + \"/predict/\" +'/predict' + str(i) + '.mid')\n",
    "    total_intro_time = 0\n",
    "    total_solo_time = 0\n",
    "    total_predict_time = 0\n",
    "    for msg in intro.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_intro_time += msg.time\n",
    "    for msg in predict.tracks[1]:\n",
    "        if msg.type == \"note_on\":\n",
    "            total_predict_time += msg.time\n",
    "            \n",
    "    original_outro_time = 0 + outro.tracks[1][1].time\n",
    "    \n",
    "    print(original_outro_time + total_predict_time + total_intro_time)\n",
    "    predict.tracks[1][1].time += total_intro_time\n",
    "    outro.tracks[1][1].time = original_outro_time + total_predict_time + total_intro_time\n",
    "    print(outro.tracks[1][1].time)\n",
    "    merged_mid = mido.MidiFile()\n",
    "    merged_mid.ticks_per_beat = intro.ticks_per_beat\n",
    "    merged_mid.tracks = intro.tracks + predict.tracks + outro.tracks\n",
    "    merged_mid.save(dissimilar_interpolation + '/merged_predict' + str(i) + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, prev_node, wid, logp, length):\n",
    "        self.prev_node = prev_node\n",
    "        self.wid = wid\n",
    "        self.logp = logp\n",
    "        self.length = length\n",
    "\n",
    "    def eval(self):\n",
    "        return self.logp / float(self.length - 1 + 1e-6)\n",
    "# }}}\n",
    "import copy\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2930460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_beam(model, sentence, german, english, device, max_length=1200,beam_width=2,max_dec_steps=25000):\n",
    "    \n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    eos_token = english.vocab.stoi[\"<eos>\"]\n",
    "    sos_token = english.vocab.stoi[\"<sos>\"]\n",
    "    \n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    n_best_list = []\n",
    "    \n",
    "     \n",
    "    #trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "    #first token as input\n",
    "    trg_tensor = torch.LongTensor(outputs).to(device)\n",
    "    \n",
    "    end_nodes = []\n",
    "\n",
    "    #starting node\n",
    "    node = BeamSearchNode(prev_node=None, wid=trg_tensor, logp=0, length=1)\n",
    "\n",
    "    nodes = []\n",
    "\n",
    "    heappush(nodes, (-node.eval(), id(node), node))\n",
    "    n_dec_steps = 0\n",
    "\n",
    "    while True:\n",
    "        # Give up when decoding takes too long\n",
    "        if n_dec_steps > max_dec_steps:\n",
    "            break\n",
    "        \n",
    "        # Fetch the best node\n",
    "        #print([n[2].wid for n in nodes])\n",
    "        score, _, n = heappop(nodes)\n",
    "        decoder_input = n.wid\n",
    "        \n",
    "        if n.wid.item() == eos_token and n.prev_node is not None:\n",
    "            end_nodes.append((score, id(n), n))\n",
    "            # If we reached maximum # of sentences required\n",
    "            if len(end_nodes) >= beam_width:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "   \n",
    "        sequence = [n.wid.item()]\n",
    "        a = n\n",
    "        while a.prev_node is not None:\n",
    "            a = a.prev_node\n",
    "            sequence.append(a.wid.item())\n",
    "        sequence = sequence[::-1] # reverse\n",
    "        \n",
    "        #print(sequence)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, torch.LongTensor(sequence).unsqueeze(1).to(device))\n",
    "        \n",
    "        # Get top-k from this decoded result\n",
    "        topk_log_prob, topk_indexes = torch.topk(output, beam_width)\n",
    "        #print(topk_indexes)\n",
    "        #print(topk_log_prob)\n",
    "        # Then, register new top-k nodes\n",
    "        for new_k in range(beam_width):\n",
    "            decoded_t = topk_indexes[0][0][new_k].view(1) # (1)\n",
    "            logp = topk_log_prob[0][0][new_k].item() # float log probability val\n",
    "\n",
    "            node = BeamSearchNode(prev_node=n,\n",
    "                                  wid=decoded_t,\n",
    "                                  logp=n.logp+logp,\n",
    "                                  length=n.length+1)\n",
    "            heappush(nodes, (-node.eval(), id(node), node))\n",
    "        n_dec_steps += beam_width\n",
    "        #print(n_dec_steps)\n",
    "    # if there are no end_nodes, retrieve best nodes (they are probably truncated)\n",
    "    if len(end_nodes) == 0:\n",
    "        end_nodes = [heappop(nodes) for _ in range(beam_width)]\n",
    "\n",
    "    # Construct sequences from end_nodes\n",
    "    n_best_seq_list = []\n",
    "    for score, _id, n in sorted(end_nodes, key=lambda x: x[0]):\n",
    "        sequence = [n.wid.item()]\n",
    "        # back trace from end node\n",
    "        while n.prev_node is not None:\n",
    "            n = n.prev_node\n",
    "            sequence.append(n.wid.item())\n",
    "        sequence = sequence[::-1] # reverse\n",
    "\n",
    "        n_best_seq_list.append(sequence)\n",
    "\n",
    "\n",
    "    # return n_best_seq_list\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in n_best_seq_list[0]]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(vocab, path):\n",
    "    output = open(path, 'wb')\n",
    "    pickle.dump(vocab, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_folder = \"vocab/\"\n",
    "save_vocab(intro_field.vocab, vocab + '/intro_vocab.pkl')\n",
    "save_vocab(solo_field.vocab, vocab  + '/solo_vocab.pkl')\n",
    "save_vocab(outro_field.vocab, vocab + '/outro_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_folder = \"vocab_intro/\"\n",
    "intro_field.vocab = load_vocab(vocab_folder + 'intro_vocab.pkl')\n",
    "solo_field.vocab = load_vocab(vocab_folder + 'solo_vocab.pkl')\n",
    "outro_field.vocab = load_vocab(vocab_folder + 'outro_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c746e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_folder = \"vocab/\"\n",
    "with open()\n",
    "intro_field.vocab = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bfaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9963d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam = [2, 59, 119, 13, 212, 59, 212, 59, 59, 75, 59, 59, 13, 119, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 119, 59, 59, 59, 59, 166, 59, 59, 59, 13, 212, 59, 59, 59, 158, 59, 59, 59, 212, 59, 59, 59, 212, 212, 13, 59, 59, 59, 59, 212, 59, 212, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 13, 59, 59, 59, 59, 59, 14, 59, 59, 212, 59, 212, 212, 59, 59, 59, 68, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 13, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 212, 212, 212, 59, 59, 59, 59, 59, 59, 68, 59, 59, 212, 59, 59, 13, 59, 59, 59, 59, 59, 59, 97, 59, 59, 59, 59, 212, 59, 59, 166, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 212, 59, 59, 59, 59, 59, 59, 59, 158, 59, 59, 59, 59, 212, 59, 59, 59, 13, 59, 59, 59, 59, 158, 59, 59, 13, 59, 13, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 13, 212, 13, 59, 59, 59, 59, 212, 59, 212, 59, 59, 59, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 59, 13, 59, 59, 59, 158, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 212, 59, 59, 59, 59, 59, 59, 212, 158, 59, 59, 59, 212, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 13, 212, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 212, 59, 59, 158, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 13, 13, 59, 59, 13, 59, 212, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 13, 212, 59, 59, 59, 59, 59, 212, 59, 212, 59, 59, 212, 59, 59, 59, 212, 59, 212, 212, 59, 59, 59, 59, 59, 59, 13, 59, 166, 59, 212, 212, 59, 59, 59, 59, 59, 212, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 86, 59, 212, 212, 212, 59, 59, 59, 59, 212, 59, 86, 59, 59, 59, 212, 212, 212, 59, 59, 59, 59, 59, 59, 59, 13, 59, 59, 59, 13, 59, 59, 59, 59, 59, 59, 59, 59, 212, 212, 59, 59, 59, 59, 59, 212, 13, 59, 59, 59, 212, 59, 212, 59, 59, 166, 59, 86, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 166, 212, 59, 59, 59, 59, 59, 86, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 212, 13, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 13, 13, 59, 59, 212, 212, 158, 59, 59, 13, 212, 59, 59, 212, 59, 59, 59, 59, 212, 59, 59, 212, 59, 59, 212, 59, 158, 212, 59, 212, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 212, 59, 59, 212, 59, 212, 212, 59, 212, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 13, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 158, 59, 59, 59, 212, 59, 212, 86, 59, 59, 59, 158, 212, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 212, 212, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 212, 59, 13, 59, 59, 212, 59, 59, 59, 13, 59, 59, 59, 59, 59, 59, 13, 212, 59, 59, 59, 59, 59, 68, 59, 13, 59, 59, 13, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 14, 59, 59, 59, 59, 59, 59, 13, 86, 59, 59, 59, 212, 59, 86, 59, 59, 59, 59, 59, 59, 59, 212, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 212, 212, 59, 59, 59, 13, 59, 59, 59, 59, 68, 59, 59, 59, 212, 13, 59, 59, 59, 212, 59, 59, 212, 59, 59, 212, 59, 59, 212, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 13, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 59, 59, 59, 13, 59, 59, 59, 212, 212, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 212, 212, 212, 13, 59, 166, 59, 212, 59, 59, 59, 13, 59, 59, 59, 59, 59, 59, 59, 59, 166, 212, 212, 59, 59, 212, 59, 212, 59, 59, 13, 59, 59, 59, 59, 13, 59, 59, 14, 13, 59, 59, 86, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 158, 59, 59, 59, 59, 212, 59, 59, 158, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 59, 166, 59, 59, 59, 59, 59, 59, 59, 59, 13, 13, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 68, 59, 59, 59, 59, 59, 212, 212, 59, 59, 59, 59, 59, 59, 212, 59, 212, 59, 59, 59, 212, 59, 59, 13, 59, 59, 166, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 158, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 158, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 212, 212, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 13, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 13, 59, 59, 59, 59, 59, 212, 59, 59, 212, 59, 59, 59, 59, 59, 212, 59, 59, 166, 59, 59, 59, 59, 13, 59, 59, 212, 212, 59, 59, 212, 59, 59, 59, 59, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 59, 59, 59, 59, 212, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 59, 212, 59, 59, 59, 59]\n",
    "\n",
    "translated_sentence1 = [solo_field.vocab.itos[idx] for idx in beam]\n",
    "translated_sentence = [int(x) for x in translated_sentence1 if x != '<pad>' and x != '<sos>' and x != '<eos>' and x != '<unk>']    \n",
    "utils.write_midi(translated_sentence, word2event, generated_outputs + \"/predict1.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabdaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b76867",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_intro)):\n",
    "    if len(test_intro) > 1200:\n",
    "        continue\n",
    "    list_sentence = [int(x) for x in sentence.split(' ')]\n",
    "    remi = [word2event[x] for x in list_sentence]\n",
    "    print(remi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_translate_sentence(model, sentence, german, english, device, max_length=1200):\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    #tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    #tokens.insert(0, german.init_token)\n",
    "    #tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    #text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(sentence).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e97881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    print(len(data))\n",
    "    for example in data:\n",
    "        #print( vars(example))\n",
    "        src = vars(example)[\"intro\"]\n",
    "        trg = vars(example)[\"solo\"]\n",
    "        \n",
    "        src = [int(x) for x in src]\n",
    "        trg = [int(x) for x in trg]\n",
    "        \n",
    "        if len(trg) > 1200 or len(src) > 1200:\n",
    "            continue\n",
    "        \n",
    "        prediction = bleu_translate_sentence(model, src, german, english, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append(trg)\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on entire test data takes a while\n",
    "score = bleu(test[1:10], model, intro_field, solo_field, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
